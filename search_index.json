[["index.html", "Analysing Quantitative Data with R Preface", " Analysing Quantitative Data with R Dr Flo Reiche &amp; Piotr Bogdanski Department of Politics and International Studies University of Warwick Last Updated 22 June, 2024 Preface Welcome to Analysing Quantitative Data with R! The purpose of this module is to introduce you to key components of the statistical programming language R and to demonstrate some of the most commonly used methods than can be applied to preprocess and analyse quantitative data. The module is divided into four parts. Please work through each part in your own time, and ensure to complete the exercises at the end of each chapter. If you have any queries in the meantime, please contact me under F.Reiche@warwick.ac.uk And now, I hope you enjoy the module! "],["introduction.html", "1 Introduction 1.1 Content", " 1 Introduction 1.1 Content This section provides a basic overview of the R programming language and instructions on how to set up R and RStudio on your computer. What is R and why should I learn it? R is an open-source statistical programming language used for data processing, analysis and visualization. It was released in 1993 by University of Auckland and is based on the older S programming language. Since then, it has grown to be one of the most popular languages for quantitative data analysis across multiple disciplines. There are many reasons why learning R is worth your time: 1. It’s open-source - as opposed to other popular statistical packages such as SPSS, Stata or SAS, R is an open-source tool. This means that using R and all its public libraries is completely free of charge and requires no special licences. It is maintained by the user community, and thus all the improvements directly coincide with the requirements of the end-users, rather than profit-driven considerations often guiding the introduction of new functionalities to proprietary statistical packages. 2. It can be widely applied - since it’s developed by users, there are tools in R for doing almost everything, from simple data manipulation and visualization to automated web data collection, natural language processing, survey data analysis, epidemiology, computational biology, social network analysis, cognitive modelling, geospatial analysis, deep learning and many more. 3. It is widely applied - in 2019, R was the 5th most popular tool for data science mentioned by job advertisements on indeed.com. It is widely used in a variety of industries, including tech, consulting, think-tanks and public institutions. 4. It has an abundance of resources available on-line - due to its increase in popularity in recent years, there are multiple R courses available on-line, which help you master the usage of particular techniques and libraries through practical exercises. The considerable size of the community that emerged surrounding the language throughout the recent decades means that almost all of the problems you will face on different levels of advancement in R programming are likely to have been encountered by others in the past. As a result, Google is a tremendous resource for troubleshooting, debugging and deepening your understanding of the ins and outs of R programming. Examples of useful MOOC course websites include DataCamp, Coursera and Edex. 5. It is a programming language for analysing data. While this may sound like a cliché, the ubiquity of data is transforming almost every area of life nowadays. Therefore data literacy and understanding of key programming concepts are extremely useful in itself, as you never know when you may need these in the future. Majority of the concepts covered in this course are to a large extent transferable to other commonly used languages and data analysis tools. Having a high level of technical and data analysis skills is one of the most desirable skills according to employers - a fact which is largely overlooked by many prospective job seekers: The course structure This course is divided into four parts. The first one will introduce you to R, covering the basic concepts related to different data structures, simple tools for analysis and core programming concepts. The second part extends this to the application of specialist R libraries used for data manipulation and visualization. Part 3 covers key statistical analysis techniques, such as correlation and linear regression and discusses how they can be applied in R. Finally, part 4 is devoted to advanced topics. Each chapter consists of the content part, discussing new R tools and techniques with examples, followed by a summary, which includes the list of the functions used in the chapter along with short explanations and finally a set of practical _exrcises with solutions. All the material presented in the chapter - the R Script, the Markdown file, the exercises and the data - are available to be downloaded at the top of each chapter page. Throughout each chapter’s content, R code is always presented in “chunks” followed by the output they produce, as you can see in the image below. Note that you can always use the button in the top right corner to copy the content of the chunk into the clipboard and paste into your R session. In the remainder of this chapter, we will go through the process of setting up all the necessary tools on your personal computer. Setup To get started with R, you’ll need to download two things: the R language and the R Studio IDE (Integrated Development Environment). You can get the first one from CRAN by following the appropriate installation link for your operating system at the top of the page. Once you install R, go to RStudio website and download and install R Studio. Now that you are done with the installation, you are ready to start using R - you can do that by clicking the RStudio shortcut. After opening the application, there are a couple of things that are useful to set up. First, open the “Tools” tab at the top of the window and select “Global Options”. There, under the “General tab” uncheck “restore .RData into workspace at startup” and set “Save workspace to .RData on exit” to “Never”. This is to ensure your R session always starts clean, without the objects loaded from the previous time you have used it. Then, in the “Code” section, check the option “Soft-wrap R Source file”. Finally in the “Appearance” section select editor theme of choice, as well as the appropriate font size and zoom. Click Apply and then OK once all this is done. Layout The R Studio editor consists of R main panes - the “Source”, “Console”, “Environment/History/…”, and “Files/Plots/Packages/…”. You can adjust their location and hide them under the pane button, located on the right side of the navigation bar. Console pane The console is where all the output generated by R (except for plots) goes. You can also enter R Studio function calls in there to produce output. Give it a try and type print(\"Hello world!\") into the command line. print(&quot;Hello world!&quot;) [1] &quot;Hello world!&quot; You can also try entering simple mathematical calculations such as 2+2 and see that R works perfectly as a calculator. Note that you should rarely enter longer pieces of code into the console. To ensure reproducibility of your work, the code should always be executed from saved R Scripts. Source pane The source pane contains the R Scripts - these are essentially text files in which you save longer pieces of R code that you can execute sequentially. You can open a new R-Script by clicking on the icon in the top left corner or by pressing Ctrl + Shift + N in Windows (or Command + Shift + N in Macbooks). You can then save the R script by clicking the floppy disk icon at the top of your R script or by pressing Ctrl + S (Command + S). Please save your file and name it hello.R. Make sure to create a separate folder such as r_course in which you will store all the files related to the lessons. You can then type the same print(\"Hello world!\") function call as you did into the console and execute it by selecting the line and pressing Ctrl + Enter (Command + Enter) or pressing the small icon at the top right corner. Generally, you can execute longer pieces of code by first selecting the entire chunk using your cursor and then pressing the appropriate execution keyboard shortcut or clicking on the execution button. You can also comment on your code by using the # hashtag. Everything following # will not be interpreted by R as code, therefore you can write anything you want after it until you begin a new line by pressing Enter. Generally, it’s crucial to comment your code, as when you come back to some of your scripts in half a year, you are likely to not remember why and how you did everything. So rather than keeping print(\"Hello world!\") in your first R script, type: print(&quot;Hello world!&quot;) #this function prints it input to the console A short note on reproducibility It is vital that all the essential code that you work on both during this course and in all future cases of R usage is stored in R scripts rather than executed directly from the console. An ideal R project would consist of the file containing the data you have used in the format it was collected (in case of primary data) or provided (in case of secondary data) and an R script(s) used for the analysis with appropriate comments made at each step, including data preprocessing, visualization and any statistical analyses you run as part of the project. This way, anyone who wants to reproduce your analysis (including yourself in a year) can simply open the script, “press play” and get the same results again, as well as trace back all the steps and decisions you made to arrive at these results. This is crucial in academia, as it allows your supervisor/reviewer to ensure methodological correctness of your work, which is an important step in tackling the replication crisis haunting many academic disciplines. It is also very important in a business setting, as it allows your co-workers to understand your work, find possible errors in it or re-use it when necessary, rather than looking at it as a black box producing an output of potentially questionable quality. Essential code here refers to anything needed to reproduce your analysis. Some examples of non-essential operations you may still execute from your console include getting an overview of the data (for example printing the variable names, number of records, variable types or producing a simple plot that you are not going to include in your final work) as well as looking at function documentation described below. However, for this course, it’s recommended that you keep all the code you use in your R scripts. This will allow you to come back to each lesson and review all the tools that you have learned throughout the course. Every section will provide you with an R script with all the code that it uses in order at the top of the page. It will also include the R markdown file (.Rmd) with the same content. Rmarkdown is a special format used to produce documents in formats such as HTML, pdf or Docx mixing text and R code - you will learn about it in the R Markdown section at the end of this course. Environment pane The third tab provides a list of R Environments. While an important idea in R, they are irrelevant for this course. For more advanced readers, Hadley Wickham’s Advanced R provides a good overview of the concept. During this course, you will only work with R’s Global Environment. This is where all the objects you assign in R will be stored and displayed. For example, before you have printed the output of 2 + 2 to the R console. However, you can also store the same output in an object of (almost) arbitrary name, say x, using the &lt;- assignment operator. The output that would normally be stored in the console will instead be passed into the object on its left. You can then type x in your script and execute it or type x into the console and press enter to see the value stored in x, as seen below. x &lt;- 2 + 2 x [1] 4 Note that x appeared in the environment pane under the Values section. Similarly, we can create a function, by typing: foo &lt;- function() 2 + 2 Again, function foo will appear in your environment with an appropriate label. You will learn more about assignment and functions in R Programming Basics and Key Programming Concepts. Here, the most important takeout point is that the “Environment” tab allows you to see all the objects you have defined in R, along with some of their key properties. A short note on keeping your working environment clean While working on an R project, it’s important to always start with a clean environment. Especially when you start receiving an error that you cannot trace back easily, the best first step is often to clean your run session and run your script run by line carefully examining the output. To clean the working environment, you should simply restart your R Studio session, either by selecting the “Restart R” option from the “Session” tab at the top of the window or by pressing Ctrl + Shift + F10 (Command + Shift + F10). Note that this will cause you to lose everything that is stored in your environment. This is yet another reason to put all your code in a script - as a result, even when you restart the session, you should be able to reproduce everything that was previously present in your Global Environment. Files/plot/help … pane The last pane contains a file browser, a plot viewer and documentation. You will learn more about plotting data in the chapters on Exploratory analysis and Data visualization. More importantly, this section is also where you can view the help coming from R documentation. Every function in R has an offline documentation file associated with it that you can access while using R Studio. To do it simply type help(functionname) or a question mark ? followed by the function name. Give it a try by typing ?sum into the R console and pressing enter - you should be able to see the help for the sum function. The documentation is extremely useful when working with R. While its content may seem a bit technical at first, as you learn more about R, a lot of things should become clearer, making it easier to learn and understand new functionalities. Furthermore, it usually provides reproducible examples of R function’s usage (you need to scroll to the very bottom of the documentation to find it), which allow you to understand in what context and how can the function be used. For example, in the sum function documentation, we can see: ## Pass several numbers to sum, and it also adds the elements. sum(1, 2, 3, 4, 5) Give it a try in your hello.R R script - you should see the sum function behaving exactly as expected. A short note on finding help when working with R While the built-in documentation is useful, sometimes you may run into trouble that’s difficult to work out on your own. The simplest solution is to concisely describe your problem (or copy the error message you are receiving) and Google it. Usually, the first result that will show will come from StackOverflow. StackOverflow is a Q &amp; A website focused on programming in all languages, and has an extensive section solely on R (almost 350 000 questions at the time of this course creation). For example, you can try typing \"a\" + 5 into the console. The resulting error message Error in \"a\" + 5 : non-numeric argument to binary operator is rather complicated, and might put many first-time users off. However, a quick Google search will lead you to this Stack Overflow post, which explains that A binary operation is a calculation that takes two values (operands) and produces another value (…) When you see that error message, it means that you are (or the function you’re calling is) trying to perform a binary operation with something that isn’t a number. Note that it’s good to make sure that your question has not been answered previously (which is the case 99 out of 100 times) before posting a new one. Should you decide to post your question, make sure to follow Stack Overflow guidelines, as well as to make your example reproducible, as described in this post. "],["basics.html", "2 Basics of R Programming 2.1 Content 2.2 Summary 2.3 Exercises", " 2 Basics of R Programming Chapter script Chapter markdown Chapter exercises 2.1 Content Arithmetic operations and assignment R allows you to perform every basic mathematical operation, so it can simply be used as a calculator. Below, you can see several examples. 5 + 3 #addition [1] 8 5 - 3 #subtraction [1] 2 5 / 3 #division [1] 1.666667 5 / 0 [1] Inf 5 * 3 #multiplication [1] 15 5 ^ 3 #exponentiation [1] 125 sqrt(4) #square root [1] 2 (5 + 5) / (3 + 1) #parenthesis [1] 2.5 9 %% 2 #modulo [1] 1 9 %/% 2 #integer division [1] 4 log(2) #natural logarithim [1] 0.6931472 exp(2) #exponent [1] 7.389056 All the operations performed above generate some output, which is printed to the R console. Most of the time, however, it is useful to store the outputs of the operations we perform, so that they can be accessed repeatedly. To do that, we can store values in variables, which can be viewed and manipulated by referencing their names. Values are assigned to variables using the &lt;-operator. While = may seem equivalent, for more advanced reasons &lt;- is used whenever assigning a value. If you’re interested in the technical details behind it, you can check out this Stack Overflow post. x &lt;- 5 my_number &lt;- 4.2 result &lt;- 5 + 6 * 7 - 8 You can name a variable anything you like, however it cannot: start with a number have any whitespaces1 include mathemtical operators in it’s name, such as *, /, ^, %, etc. - it’s best to avoid any special characters except for “” and “.” to separate words if necessary, for example my_variable or my.variable. While you can see the dot being used in variable names by many users, technically using is a better practice, as the dot has an additional special meaning attached to it. It is also useful to avoid naming variables using names that are already defined in R to be something else - however, you will learn the avoid this as you progress in the course. Keep in mind, that while variable names are arbitrary, it’s good to keep them concise and informative, especially if you have to present your code to someone or to come back to your own work after a long period of time. Assigning the values does not produce any output. To access the values assigned to a variable, you have to call its name directly in the script or the console. You can use this to perform mathematical operations on the objects as well. x &lt;- 5 y &lt;- 7 x [1] 5 y [1] 7 x + y [1] 12 z &lt;- x * y z [1] 35 Finally, you can always assign a new value to the same name. However this implies that the old value will be discarded. It can be useful when you know that you won’t need to access the value again. For example, it’s a common practice to modify a variable already defined such that x &lt;- x + 5. This simply means “add 5 to x and store it in x”. x &lt;- 5 x [1] 5 x &lt;- 7 x [1] 7 x &lt;- x + 2 Vectors While the above operations are useful, the true power of R comes from the so-called vectorization of mathematical (and other) operations. A vector in R terminology is just a fancy word for an ordered sequence of numbers or a “column” in an Excel sheet. Vectors are also commonly referred to as variables - in the context of this course, these terms will be used interchangably. Vectors are created by the combine function c(). A function is simply an expression followed by parentheses which takes some values as input and generates other values as output. Vectors are assigned to names the same way as numbers. In fact, a number in R is equivalent to a vector of length 1. Below you can see some examples of vectors being created. x &lt;- c(1, 4, 5, 6) x [1] 1 4 5 6 z &lt;- c(5 / 2, 7 * 8, 2 + 1) z [1] 2.5 56.0 3.0 v &lt;- c(x, z) v [1] 1.0 4.0 5.0 6.0 2.5 56.0 3.0 R also offers a shorthand for creating sequences of integers between two values, for example: 1:10 [1] 1 2 3 4 5 6 7 8 9 10 x &lt;- 15:5 x [1] 15 14 13 12 11 10 9 8 7 6 5 It is also possible to create multiple repetitions of a given vector by using the rep() function, which takes a vector as its first argument and the number of required repetitions as the second argument. rep(5, 3) #repreat the number 5 3 times [1] 5 5 5 a &lt;- c(5, 6, 7) b &lt;- rep(a, 4) #repeat vector a 4 times and assign it to b b [1] 5 6 7 5 6 7 5 6 7 5 6 7 The length of a vector refers to the number of elements it contains. It can be examined using the length() function: a &lt;- 1:5 length(a) [1] 5 Vectorization of an operation refers to the fact that an operation are performed on vectors element-wise. This is true for majority of R operations. For example, adding the vectors [1 2 3] and [5 6 7] will produce the vector [6 8 10]. If an operation is performed between a vector and a single number (or a scalar, using more specific terminology), it is applied to each pair of the elements, such that [1 1 1] multiplied by 2 would yield [2 2 2]. Below you can see some examples of vectors in action. x &lt;- c(1, 2, 3, 2) y &lt;- c(5, 6, 7, 4) x + y [1] 6 8 10 6 x - y [1] -4 -4 -4 -2 x ^ y [1] 1 64 2187 16 z &lt;- 5 * x - 2 * y z [1] -5 -2 1 2 In case of performing operations on a vector and a scalar (one number), the operation is applied to each element of the vector. For example: x &lt;- c(1, 2, 3, 2) x * 3 [1] 3 6 9 6 While it was mentioned earlier that vectors can be thought of as ordered sequences of numbers, they can also contain text. Such vectors are callled “character vectors” and are constructed similarily to numeric vectors. Each text has to be enquoted in “” to denote that it should not be interpreted as a variable name or a function. a &lt;- &quot;Hello world&quot; a [1] &quot;Hello world&quot; words &lt;- c(&quot;This&quot;, &quot;Is&quot;, &quot;A&quot;, &quot;Sequence&quot;, &quot;Of&quot;, &quot;Words&quot;) words [1] &quot;This&quot; &quot;Is&quot; &quot;A&quot; &quot;Sequence&quot; &quot;Of&quot; &quot;Words&quot; Mathematical operations such as addition or division cannot be performed on character vectors for obvious reasons and will produce an error if you attempt them. However, you can combine words and numbers into one vector - however, they will all be treated as if they were text, so the number 5 in the example below gets converted to character “5”. mixed &lt;- c(5, &quot;something&quot;, &quot;something else&quot;) mixed [1] &quot;5&quot; &quot;something&quot; &quot;something else&quot; The paste function is a useful for dealing with character vectors - it can be thought of as an equivalent of addition operation for text. name &lt;- &quot;John&quot; surname &lt;- &quot;Doe&quot; paste(name, surname) [1] &quot;John Doe&quot; names &lt;- c(&quot;John&quot;,&quot;Jane&quot;,&quot;Thomas&quot;) surnames &lt;- c(&quot;Doe&quot;,&quot;Smith&quot;,&quot;Kowalsky&quot;) paste(names, surnames) [1] &quot;John Doe&quot; &quot;Jane Smith&quot; &quot;Thomas Kowalsky&quot; Coercion Another important topic when dealing with vectors is coerecion. This refers to forcing one vector type to become another using the as functions. For example, we use as.character to force an R object into a character vector, or as.numeric to force it into a numeric vector: numbers_char &lt;- c(&quot;5&quot;,&quot;6&quot;,&quot;7&quot;) numbers_char [1] &quot;5&quot; &quot;6&quot; &quot;7&quot; numbers &lt;- as.numeric(numbers_char) numbers [1] 5 6 7 numbers &lt;- c(10, 123, 12) as.character(numbers) [1] &quot;10&quot; &quot;123&quot; &quot;12&quot; Note that this will not always work, as in many cases elements of one vector type cannot be interpreted as another. For example: nums &lt;- c(&quot;1&quot;,&quot;2&quot;,&quot;three&quot;) as.numeric(nums) Warning: NAs introduced by coercion [1] 1 2 NA In this case, R still returns the output, however the third element of the nums vector is turned into an NA value. NA is shorthand for Not Available - it’s a constant that R uses to deal with missing values. This is indicated by the warning printed by R to the console. Missing values will be covered in more detail in the next chapter. Logical Values and Operators Another crucial type of operations in R are logical operations, also known as boolean. They are used to evaluate the truth value of logical statements such as variable “A is equal to variable B” or variable A is a numeric vector. Whenever the queried statement is True, they return TRUE and FALSE otherwise. Below you can see some simple examples using the equality operator == - the double equality means that we are checking whether two values are equal, rather than assigning one to another. a &lt;- 5 a == 5 [1] TRUE a == 3 [1] FALSE a - 2 == 3 [1] TRUE &quot;John&quot; == &quot;James&quot; [1] FALSE The ! operator is used for negation, so !TRUE results in FALSE and vice versa. Accordingly, != is used to denote ‘not equals to’. !TRUE [1] FALSE !FALSE [1] TRUE 5 != 6 [1] TRUE &quot;John&quot; != &quot;James&quot; [1] TRUE Logical operations can also be used to compare values, by using a &lt; b for “a is less than b”, a &lt;= b for “a is less or equal to b” and vice versa. x &lt;- 5 y &lt;- 3 x &gt; y [1] TRUE x - 2 &lt; y [1] FALSE x - 2 &lt;= y [1] TRUE Finally, the &amp; (logical “and”) and | (logical “or”) operators are designed to combine TRUE/FALSE values. So, if you put &amp; between two logical values, it will yield TRUE if and only if both values are TRUE. | on the other hand will return TRUE if any of the values is TRUE TRUE &amp; TRUE [1] TRUE TRUE &amp; FALSE [1] FALSE TRUE | FALSE [1] TRUE FALSE | FALSE [1] FALSE (5 + 1 == 6) | (2 + 2 == 5) [1] TRUE (5 + 1 == 6) &amp; (2 * 2 == 10) [1] FALSE The logical values are also often used to verify whether we are dealing with a certain R type - for example to check whether a value is a character or numeric. This is achieved by using the is functions, such as is.numeric or is.character. numbers &lt;- c(5, 6, 7) is.vector(numbers) [1] TRUE is.numeric(numbers) [1] TRUE is.character(numbers) [1] FALSE words &lt;- c(&quot;Word&quot;,&quot;Word&quot;) is.numeric(words) [1] FALSE is.character(words) [1] TRUE As with numbers and characters, the logical values also form their special types of vectors and can be used to perform element-wise operations. a &lt;- c(TRUE, FALSE, FALSE) b &lt;- c(TRUE, TRUE, TRUE) a &amp; b [1] TRUE FALSE FALSE They can also be used to find whether each value in a numeric or character vector is equal to another. x &lt;- c(5, 6, 7, 8) x == 5 [1] TRUE FALSE FALSE FALSE y &lt;- c(&quot;John&quot;, &quot;James&quot;, &quot;Thomas&quot;) z &lt;- c(&quot;John&quot;,&quot;James&quot;,&quot;Robert&quot;) z == y [1] TRUE TRUE FALSE The boolean vectors can be also thought of as a special case of numeric vectors consisting only of 0s and 1s, where 0 corresponds with FALSE and 1 with TRUE value. This can be easily seen in the example below: TRUE + TRUE + TRUE [1] 3 Indexing While a very large volume of data can be stored in one vector, we often may want to access only a specific element of it, or a fraction of the elements. An index of a vector, is simply an integer corresponding to the position of a value in a the vector. So, a vector with N values has integers ranging from 1 to N. For example, in vector c(5, 10, 3, 2), the index of 5 is 1, the index of 10 is 2, the index of 3 is 3, etc. Indexing is an operation of accessing a vector’s elemet at a given index, using the square brackets []. For example, a[5] means “get the fifth element from vector a”. a &lt;- c(5.2, 4.5, 6.2, 8.9, 10.2, 4.8, 0.1) a[5] [1] 10.2 Indexing can also be used to replace values at a given position in a vector. In the example below, we replace the first element of a with the number 1000. a[1] &lt;- 1000 a [1] 1000.0 4.5 6.2 8.9 10.2 4.8 0.1 Indexing can also be done using another vector of numeric values. For example we may want to get the first, second and fifth elements of a given vector, or a sequence of elements between 1 and 4. a &lt;- c(5.2, 4.5, 6.2, 8.9, 10.2, 4.8, 0.1) a[c(1, 3, 5)] [1] 5.2 6.2 10.2 #equivalent to: b &lt;- c(1, 3, 5) a[b] [1] 5.2 6.2 10.2 #can also be done for a sequence a[1:5] [1] 5.2 4.5 6.2 8.9 10.2 Indexing is even more powerful in conjunction with logical operations. This is because, a logical vector can be used to index any vector - such indexing operations returns all the values of the indexed vector where the corresponding indexing logical vector is TRUE. This may sound confusing at first, but is actually quite straightforward, as seen below: x &lt;- c(4.2, 5.6, 7.2, 1.1) index &lt;- c(FALSE, TRUE, TRUE, FALSE) #only second and third elements are TRUE x[index] #returns only second and third elements of the x vector [1] 5.6 7.2 For example, imagine vector gdp vector that holds the GDP per capita values for a list of countries and country vector that holds the corresponding country names. Logical indexing may be very useful, if we want to get names of countries with GDP per capita above or below a certain value: gdp &lt;- c(69687, 67037, 65111, 52367, 41030, 32946, 29961) countries &lt;- c(&quot;Qatar&quot;, &quot;Iceland&quot;, &quot;USA&quot;, &quot;Germany&quot;, &quot;United Kingdom&quot;, &quot;Italy&quot;, &quot;Spain&quot;) countries[gdp &gt; 40000] [1] &quot;Qatar&quot; &quot;Iceland&quot; &quot;USA&quot; &quot;Germany&quot; &quot;United Kingdom&quot; We can also use this with multiple critertia, for example index countries with GDP higher than 40000 USD and the UN Human Development Index higher than 0.9. hdi &lt;- c(0.848, 0.938, 0.920, 0.939, 0.920, 0.883, 0.893) countries[gdp &gt; 40000 &amp; hdi &gt; 0.9] [1] &quot;Iceland&quot; &quot;USA&quot; &quot;Germany&quot; &quot;United Kingdom&quot; Sorting On many occasions, it’s useful to sort a vector to see it’s highest or lowest values. This can be achieved by using the sort function. numbers &lt;- c(8, 4, 5, 10, 2, 123) sort(numbers) [1] 2 4 5 8 10 123 By default, R sorts vectors in an increasing order (in case of character vectors, this translates to A-Z sorting). However, the sort function has an additional argument, decreasing, that can be used to specify whether the sorting should be done in the decreasing order. The argument is a default argument, i.e. takes a certain value unless specified otherwise by the user. This is common in R and a lot of functions allow customizing the way they work by specifying additional arguments, which have a default value to avoid the effort of specifying them every time a certain function is used. Such default arguments can easily be recognized in R documentation. In case of sort, the Usage section reads sort(x, decreasing = FALSE, ...). This means, that the function takes x (the vector to be sorted) as its main argument, and decreasing, which defaults to FALSE. The argument decreasing is also logical - can only take TRUE or FALSE values - this is a common argument type if a certain operation can be performed in two different ways of with an additional element that may not always be desired. sort(numbers, decreasing = TRUE) [1] 123 10 8 5 4 2 While sorting a vector may be useful in certain circumstances, a lot of the time we may actually need to sort the values by another vector. For example, let’s assume that we have a vector of names and corresponding ages, and we want to see the names ordered by the age. names &lt;- c(&quot;Thomas&quot;,&quot;Peter&quot;,&quot;Ahmed&quot;,&quot;Emily&quot;,&quot;Helena&quot;) age &lt;- c(50, 10, 20, 15, 40) This can be achieved using the order function, which returns indices of the vector needed to re-arrange it into sorted order. order(age) [1] 2 4 3 5 1 In this case, the age of 10 (index 2) should go to the first place, 15 (index 4) to the second position, 20 (index 3) to the third, etc. Note that the two following operations are equivalent: sort(age) [1] 10 15 20 40 50 age[order(age)] [1] 10 15 20 40 50 The first one tells R to simply sort the values of age, whereas the second to index age by the indices of age in a sorted order. To get the names sorted by age, we can use: names[order(age)] [1] &quot;Peter&quot; &quot;Emily&quot; &quot;Ahmed&quot; &quot;Helena&quot; &quot;Thomas&quot; names[order(age, decreasing = TRUE)] #decreasing order [1] &quot;Thomas&quot; &quot;Helena&quot; &quot;Ahmed&quot; &quot;Emily&quot; &quot;Peter&quot; Finally, the rank function returns the sample ranks of a given vector, i.e. their relative position in a sorted list. Note that this is different from order. rank returns the position corresponding to each value in a sorted order, whereas order returns indices of the original vector needed to put it in a sorted order. age [1] 50 10 20 15 40 rank(age) [1] 5 1 3 2 4 order(age) [1] 2 4 3 5 1 So in our example, the first value of the vector returned by rank(age) is 5, since the first value of the age vector is 50, which would be last in the numeric order. The first value of the vector returned by order(age) is 2 - this is because, the 2nd element of age (i.e. the value of 10) should go to the first position for the vector to me correctly ordered. Finally, logical indices can be converted into numerical values using the which function. It takes a logical vector as input and returns the indices at which the value of the vector is TRUE. You can see an example below: numbers &lt;- 1:10 numbers &gt; 5 [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE which(numbers &gt; 5) [1] 6 7 8 9 10 This function is helpful in some certain situations, however it’s a bad practice to apply it in cases when logical indexing is sufficient, for example: numbers[numbers &gt; 5] [1] 6 7 8 9 10 is sufficient, and there’s no need to use: numbers[which(numbers &gt; 5)] [1] 6 7 8 9 10 One of the situations in which the use of which function can be preferred to simple logical indexing is when our vector contains missing values (discussed in the next chapter. For example, the first expression will return NA. numbers &lt;- c(2, 4, 1, 10, 20, NA) numbers[numbers &gt; 5] [1] 10 20 NA This is because, running logical comparisons such as numbers &gt; 5 always returns missing values, along the TRUE and FALSE logical values. This should make sense, since NA is not comparable to any number. numbers &gt; 5 [1] FALSE FALSE FALSE TRUE TRUE NA which skips the NA values, only returning the indices of values that are TRUE. which(numbers &gt; 5) [1] 4 5 As a result, we can perform indexing on variable with missing values using which: numbers[which(numbers &gt; 5)] [1] 10 20 Two cousins of the which function are which.max and which.min, which return the index of the highest and lowest value in a vector. So, coming back to the ages example, we can retrieve the name of the person with highest and lowest age using respectively: names[which.max(age)] [1] &quot;Thomas&quot; names[which.min(age)] [1] &quot;Peter&quot; 2.2 Summary Vectors are simply ordered sequences of elements that represent some concept we analyze - for example, we can have a vector of country names and a vector of corresponding GDP per capita amounts. They are also known as variables. Vectorization simply means, that if we add (or perform any other operation) two vectors toegether/an numeric element to a vector, it will be done element-wise. Functions in R are objects which take some values as inputs and produce other values as outputs. Logical values are the result of comparing vectors using logical operators (such as ==, &lt;, &gt;). They can be either TRUE if the logical statment is true (e.g. 2 + 2 == 4) or FALSE otherwise (e.g. 7 &gt; 10). Indexing means retrieving vector elements by their numeric index (i.e. their position in the vector). It can also be done using logical values - in this case, all elements corresponding to TRUE are retrieved. Sorting refers to putting elements of a vector into numeric or alphabetial order. It can be done either directly on a vector using sort() or in reference to another vector using order(). rank() allows to retrieve the relative rank of an item (i.e. position which it would have if the vector was sorted). Functions list function package description as.character() base coerce a vector to character as.numeric() base coerce a vector to numeric c() base Combine values/vectors into a vector exp() base exponent is.character() base check if vector is character is.numeric() base check if vector is numeric is.vector() base check if an object is a vector length() base get number of elements in a vector or list log() base log (default base = e) order() base get indexes that will sort a vector paste() base NA rank() base rank the values of a vector rep() base repeat a value or a vector N times sort() base sort the values. Specify decreasing = FALSE to sort in decreasing order sqrt() base square root which() base return indexes of TRUE entries of a logical vector which.max() base return index of the largest value in a vector which.min() base return index of the smallest value in a vector 2.3 Exercises The code below creates three vectors corresponding with individual’s name, birth year and birth month. year &lt;- c(1976, 1974, 1973, 1991, 1972, 1954, 1985, 1980, 1994, 1970, 1988, 1951, 1957, 1966, 1968, 1963, 1999, 1977, 1984, 1998) month &lt;- c(&quot;February&quot;, &quot;February&quot;, &quot;April&quot;, &quot;August&quot;, &quot;September&quot;, &quot;November&quot;, &quot;October&quot;, &quot;December&quot;, &quot;May&quot;, &quot;March&quot;, &quot;June&quot;, &quot;November&quot;, &quot;October&quot;, &quot;May&quot;, &quot;July&quot;, &quot;August&quot;, &quot;March&quot;, &quot;July&quot;, &quot;October&quot;, &quot;January&quot;) name &lt;- c(&quot;Thomas&quot;, &quot;Natalie&quot;, &quot;James&quot;, &quot;Gina&quot;, &quot;Cate&quot;, &quot;Rob&quot;, &quot;Frank&quot;, &quot;Tyle&quot;, &quot;Marshall&quot;, &quot;Ted&quot;, &quot;Emily&quot;, &quot;Brandon&quot;, &quot;Yasmin&quot;, &quot;Tina&quot;, &quot;Phillip&quot;, &quot;Natasha&quot;, &quot;Joan&quot;, &quot;Jack&quot;, &quot;Alice&quot;, &quot;Barney&quot;) Create a vector named birthdays which contains names, birth months and birth years of each person. For example, the first one should look like \"Thomas, February 1976. Filter out the people who were born in October after the year 1980. Store their names in vector named oct1980 Given the vector x &lt;- c(5, 40, 15, 10, 11). What would output would you expect from the following functions? sort(x) order(x) rank(x) Use R to verify your answers. Vector country contains the names of 6 countries and the following 4 vectors contain the countries’ correspondig Expected Years of Schooling, eys, Mean Years of Schooling mys, Life Expectancy at Birth lexp and Per capita Gross National Income gni. country &lt;- c(&quot;Argentina&quot;, &quot;Georgia&quot;, &quot;Mexico&quot;, &quot;Philippines&quot;, &quot;Turkey&quot;, &quot;Ukraine&quot;) eys &lt;- c(17.6, 15.4, 14.3, 12.7, 16.4, 15.1) mys &lt;- c(10.6, 12.8, 8.6, 9.4, 7.7, 11.3) lexp &lt;- c(76.5, 73.6, 75, 71.1, 77.4, 72) gni &lt;- c(17611, 9570, 17628, 9540, 24905, 7994) The United Nations Human Development Index (HDI) is given by the following formula: HDI = \\(\\sqrt[3]{\\text{Life Expectancy Index} * \\text{Education Index} * \\text{Income Index}}\\), where Life Expectancy Index = \\(\\frac{\\text{Life Expectancy}-20}{85-20}\\) Education Index = \\(\\frac{\\text{Mean Years of Schooling Index} + \\text{Expected Years of Schooling Index}}{2}\\) Mean Years of Schooling Index = \\(\\frac{\\text{Mean Years of Schooling}}{15}\\) Expected Years of Schooling Index = \\(\\frac{\\text{Expected Years of Schooling}}{18}\\) Income Index = \\(\\frac{ln(GNIpc) - ln(100)}{ln(75,000) - ln(100)}\\) Write R code to answer the following questions: Calculate the HDI for each of the countries. Store country names with HDI lower than 0.75 in vector coutry_lhdi. Print the names of the countries. Store country names with HDI lower than 0.8 and GNI higher than $10000 in vector country_hlgh Print names of the countries with HDI at least as high as HDI of Turkey (excluding Turkey). Print names of the countries in which the Expected Years of Schooling index is higher than the Life Expectancy Index. The data below contains the records of UK General Election turnout between 1964 and 2019. turnout &lt;- c(0.771, 0.758, 0.72, 0.788, 0.728, 0.76, 0.727, 0.753, 0.777, 0.714, 0.594, 0.614, 0.651, 0.661, 0.687, 0.673) year &lt;- c(1964, 1966, 1970, 1974, 1974, 1979, 1983, 1987, 1992, 1997, 2001, 2005, 2010, 2015, 2017, 2019) party &lt;- c(&quot;Labour&quot;, &quot;Labour&quot;, &quot;Conservative&quot;, &quot;Labour&quot;, &quot;Labour&quot;, &quot;Labour&quot;, &quot;Conservative&quot;, &quot;Conservative&quot;, &quot;Conservative&quot;, &quot;Conservative&quot;, &quot;Labour&quot;, &quot;Labour&quot;, &quot;Labour&quot;, &quot;Conservative&quot;, &quot;Conservative&quot;, &quot;Conservative&quot;) Write code to answer the following questions: Which years had the turnout higher than 70/%? Which parties won in elections with turnout below 0.65? Obtain the years in which the turnout was the lowest and the highest and store them in vector year_minmax. Store the names of the parties which won in the 3 elections with the highest turnout in vector top3 The solutions for the exercises will be available here on 2020-11-12. Chapter solutions Technically, you can create a variable with whitespaces or numbers in its name using the reverse quotation marks `, for example 5 my variable. However this is not a common practice and makes your code difficult to read, so this should be avoided.↩︎ "],["datastructures.html", "3 Data structures 3.1 Content 3.2 Summary 3.3 Exercises", " 3 Data structures Chapter script Chapter markdown Chapter exercises Chapter data 3.1 Content The the previous chapter you have become familiar with the most common data structure in R programming - a vector. In this section, you will be introduced to some more advanced data structures that are often used in R, in particular the data.frame, which is the most common way of storing and manipulating data in R. Generally, the best way to examine any R object is using the str() function, which returns contents of the object along with its class. For example you can check how it works for simple vectors numbers &lt;- c(5, 3, 8) str(numbers) num [1:3] 5 3 8 num indicates that it is an integer vector and [1:3] tells us that its index ranges from 1 to 3. words &lt;- c(&quot;five&quot;,&quot;three&quot;,&quot;eight&quot;) str(words) chr [1:3] &quot;five&quot; &quot;three&quot; &quot;eight&quot; Data Frames In the previous chapter’s exercises you’ve manipulated data related to some basic development indicators of several countries. When we’re dealing with multiple variables represented by multiple vectors, it’s often very useful to store them toegether as one entity - a data.frame. Data frames can simply be thought of as tables, where each of the columns is a vector with its unique name. In this case, we can store the information about countries in a data frame called dev_data. country &lt;- c(&quot;Argentina&quot;, &quot;Georgia&quot;, &quot;Mexico&quot;, &quot;Philippines&quot;, &quot;Turkey&quot;, &quot;Ukraine&quot;) eys &lt;- c(17.6, 15.4, 14.3, 12.7, 16.4, 15.1) mys &lt;- c(10.6, 12.8, 8.6, 9.4, 7.7, 11.3) lexp &lt;- c(76.5, 73.6, 75, 71.1, 77.4, 72) gni &lt;- c(17611, 9570, 17628, 9540, 24905, 7994) dev_data &lt;- data.frame(country, eys, mys, lexp, gni) We can use the head function to see the first 5 rows of the data (in the toy example we have above it might seem unnecessary, but it is useful to get an overview of all the variables when the data consists of potentially thousands of rows). head(dev_data) country eys mys lexp gni 1 Argentina 17.6 10.6 76.5 17611 2 Georgia 15.4 12.8 73.6 9570 3 Mexico 14.3 8.6 75.0 17628 4 Philippines 12.7 9.4 71.1 9540 5 Turkey 16.4 7.7 77.4 24905 6 Ukraine 15.1 11.3 72.0 7994 The str() function is also very useful to get an overview of the variables included in a dataframe: str(dev_data) &#39;data.frame&#39;: 6 obs. of 5 variables: $ country: chr &quot;Argentina&quot; &quot;Georgia&quot; &quot;Mexico&quot; &quot;Philippines&quot; ... $ eys : num 17.6 15.4 14.3 12.7 16.4 15.1 $ mys : num 10.6 12.8 8.6 9.4 7.7 11.3 $ lexp : num 76.5 73.6 75 71.1 77.4 72 $ gni : num 17611 9570 17628 9540 24905 ... To access a column stored in a dataframe, you can use the $ operator. dev_data$gni [1] 17611 9570 17628 9540 24905 7994 Similarily, we can use the same operator to create a new column: dev_data$log_gni &lt;- log(dev_data$gni) dev_data$log_gni [1] 9.776279 9.166388 9.777244 9.163249 10.122824 8.986447 As in the case of vectors, data frames can be indexed to retrieve values stored at specific positions. Since data frame is a table, each position in a dataframe is associated with two indices - one for rows, the other for columns - the first index references the row and the second the column. For example, the code below retrieves the value from the second row from the third column of dev_data. dev_data[2, 3] [1] 12.8 Note that this is identical to: dev_data$mys[2] [1] 12.8 This is because the mys is the third column in dev_data. By leaving one of the indices empty, we can also retrieve entire row/column of a data frame: dev_data[1, ] #get first row country eys mys lexp gni log_gni 1 Argentina 17.6 10.6 76.5 17611 9.776279 dev_data[, 2] #get second column [1] 17.6 15.4 14.3 12.7 16.4 15.1 Data frames can also be indexed with integer vectors. Such indexing will always return a smaller data frame. For example, to retrieve rows 1 and 5 from columns 2 and 3, we can do: dev_data[c(1,5), c(2,3)] eys mys 1 17.6 10.6 5 16.4 7.7 Similarily, character vectors referencing the column names can be used to subset a dataframe. To achieve similar result to the one above, one could also type: dev_data[c(1,5), c(&quot;eys&quot;,&quot;mys&quot;)] eys mys 1 17.6 10.6 5 16.4 7.7 We can also use logical indexing to subset dataframes. Recall from the previous chapter, that we can check which values of a given vector satisfy a certain condition by: dev_data$gni &gt; 10000 [1] TRUE FALSE TRUE FALSE TRUE FALSE We can then use the output generated by the above code to index the dev_data data frame and obtain the rows with gni per capita larger than 10000: dev_data[dev_data$gni &gt; 10000, ] country eys mys lexp gni log_gni 1 Argentina 17.6 10.6 76.5 17611 9.776279 3 Mexico 14.3 8.6 75.0 17628 9.777244 5 Turkey 16.4 7.7 77.4 24905 10.122824 There are many useful functions that work in combination with data frames. Below, there are several examples: is.data.frame(dev_data) #check if an object is of class `data.frame` [1] TRUE nrow(dev_data) #number of rows [1] 6 ncol(dev_data) #number of columns [1] 6 colnames(dev_data) #column names [1] &quot;country&quot; &quot;eys&quot; &quot;mys&quot; &quot;lexp&quot; &quot;gni&quot; &quot;log_gni&quot; rownames(dev_data) #row names [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; The with command allows to evaluate column names in the context of a given data frame. This means, that we do not have to reference the data frame name whenever we use one of its columns. Suppose we wanted to calculate the UN’s Education Index as in the previous section’s exercises and assign it’s values to a new column in dev_data, dev_data$edu_ind. This could be done by: dev_data$edu_ind &lt;- (dev_data$mys / 15 + dev_data$eys / 18)/2 However, in many circumstances this will require you to reference the name of the data frame you are using multiple times, often making the code long and unreadable. To avoid it, it’s often useful to do: dev_data$edu_ind &lt;- with(dev_data, (mys / 15 + eys / 18)/2) The with function takes name of the dataframe as its first argument and the operation you want to perform as the second argument. Similarily, to subset a dataframe by multiple variables, the subset() command can be used: dev_data[dev_data$eys &gt; 15 &amp; dev_data$lexp &gt; 75, ] country eys mys lexp gni log_gni edu_ind 1 Argentina 17.6 10.6 76.5 17611 9.776279 0.8422222 5 Turkey 16.4 7.7 77.4 24905 10.122824 0.7122222 subset(dev_data, eys &gt; 15 &amp; lexp &gt; 75) country eys mys lexp gni log_gni edu_ind 1 Argentina 17.6 10.6 76.5 17611 9.776279 0.8422222 5 Turkey 16.4 7.7 77.4 24905 10.122824 0.7122222 Factors When looking at the str(dev_data) you could’ve noticed that the country variable is a vector type that we haven’t encountered earlier - a factor. Factors are a specific type of vectors used to store values that take a prespecified set of values, called factor levels. For example, suppose we have two character vectors storing names of students and their year. We can use factor() to create a factor vector from a character vector. This can be done for any other type of vector as well. name &lt;- c(&quot;Thomas&quot;,&quot;James&quot;,&quot;Kate&quot;,&quot;Nina&quot;,&quot;Robert&quot;,&quot;Andrew&quot;,&quot;John&quot;) year_ch &lt;- c(&quot;Freshman&quot;,&quot;Freshman&quot;,&quot;Junior&quot;,&quot;Sophmore&quot;,&quot;Freshman&quot;,&quot;Senior&quot;,&quot;Junior&quot;) year_ch [1] &quot;Freshman&quot; &quot;Freshman&quot; &quot;Junior&quot; &quot;Sophmore&quot; &quot;Freshman&quot; &quot;Senior&quot; &quot;Junior&quot; year &lt;- factor(year_ch) year [1] Freshman Freshman Junior Sophmore Freshman Senior Junior Levels: Freshman Junior Senior Sophmore We can view the unique levels of the factor using the levels() function: levels(year) [1] &quot;Freshman&quot; &quot;Junior&quot; &quot;Senior&quot; &quot;Sophmore&quot; A crucial difference between factor and character vectors is that the former have an underlying integer representation. That means, that there’s a natural ordering to their levels, which is alphabetic by default. We can see that using the coercion function as.numeric on the year factor. year [1] Freshman Freshman Junior Sophmore Freshman Senior Junior Levels: Freshman Junior Senior Sophmore as.numeric(year) [1] 1 1 2 4 1 3 2 Note that the ordering of the values corresponds with the ordering obtained by the levels() function. This matters in some circumstances (such as when using factor variables in regression models, discussed in the Linear Regression section of the course). It’s a good practice to explicitly pass the factor levels to the factor() constructor. For example, in our case, “Sophmore” comes as the last value of the factor, even though it would make more sense for it to be second. Explicit creation of the factor levels can be seen below: year_ch &lt;- c(&quot;Freshman&quot;,&quot;Freshman&quot;,&quot;Junior&quot;, &quot;Sophmore&quot;,&quot;Freshman&quot;,&quot;Senior&quot;,&quot;Junior&quot;) year &lt;- factor(year_ch, levels = c(&quot;Freshman&quot;,&quot;Sophmore&quot;,&quot;Junior&quot;,&quot;Senior&quot;)) We can now see that the ordering of the levels is different, and so is the underlying numeric representation of the factor: levels(year) [1] &quot;Freshman&quot; &quot;Sophmore&quot; &quot;Junior&quot; &quot;Senior&quot; as.numeric(year) [1] 1 1 3 2 1 4 3 Note that we cannot change the value of a factor vector to any other than the pre-specified levels: year[1] &lt;- &quot;Graduate&quot; Warning in `[&lt;-.factor`(`*tmp*`, 1, value = &quot;Graduate&quot;): invalid factor level, NA generated The error message returned by R means that the value we were trying to assign to the factor is not one of the predefined levels (i.e. “Freshman”,“Junior”, “Senior” and “Sophmore”) and thus NA missing value was generated. However, if we know that a level that has no values attached to it will be created in the future, NAs can be avoided by explicitly creating an unused levels when constructing the factor vector. year_ch &lt;- c(&quot;Freshman&quot;,&quot;Freshman&quot;,&quot;Junior&quot;, &quot;Sophmore&quot;,&quot;Freshman&quot;,&quot;Senior&quot;,&quot;Junior&quot;) year &lt;- factor(year_ch, levels = c(&quot;Freshman&quot;,&quot;Sophmore&quot;,&quot;Junior&quot;,&quot;Senior&quot;, &quot;Graduate&quot;)) Here, we have created the variable with 5 levels: Freshman, Sophmore, Junior, Senior, Graduate, even though only 4 of them are actual values of the factor. As a result, we can assign a value with the “Graduate” value without producing NAs. The relevance of having empty factor levels will become apparent in the next part of the book when discussing Cross-Tabulation. year[1] &lt;- &quot;Graduate&quot; year [1] Graduate Freshman Junior Sophmore Freshman Senior Junior Levels: Freshman Sophmore Junior Senior Graduate We can also rename the levels of an existing factor, by using the levels&lt;- command. This can be done either to specific levels of a factor… year &lt;- factor(year_ch, levels = c(&quot;Freshman&quot;,&quot;Sophmore&quot;,&quot;Junior&quot;,&quot;Senior&quot;)) levels(year)[1] &lt;- &quot;Fresher&quot; year [1] Fresher Fresher Junior Sophmore Fresher Senior Junior Levels: Fresher Sophmore Junior Senior …or to all the levels: levels(year) &lt;- c(&quot;First&quot;,&quot;Second&quot;,&quot;Third&quot;,&quot;Final&quot;) year [1] First First Third Second First Final Third Levels: First Second Third Final This way, all the values of the character are changed very quickly. Finally, there’s some confusion about the difference between factor() and as.factor() functions. In many contexts, these can be used equivalently, since both create a factor vector from a numeric or a character vector. However, some important differences include: factor() allows to explicitly pass vector levels at construction, whether as.factor() assigns them by default The behaviour of the two functions is different when passed factors with empty levels. For example, let’s create the year factor as earlier and only keep the first three values. In this case, the Sophmore and Senior levels are unused. year_char &lt;- c(&quot;Freshman&quot;,&quot;Freshman&quot;,&quot;Junior&quot;, &quot;Sophmore&quot;,&quot;Freshman&quot;,&quot;Senior&quot;,&quot;Junior&quot;) year &lt;- factor(year_char, levels = c(&quot;Freshman&quot;,&quot;Sophmore&quot;,&quot;Junior&quot;,&quot;Senior&quot;)) year &lt;- year[1:3] year [1] Freshman Freshman Junior Levels: Freshman Sophmore Junior Senior Passing the year vector to as.factor will not change anything in the vector’s structure: as.factor(year) [1] Freshman Freshman Junior Levels: Freshman Sophmore Junior Senior However, using factor() constructor on an existing factor vector is a convenient way to drop unused levels (when it’s desirable): factor(year) [1] Freshman Freshman Junior Levels: Freshman Junior The performance of as.factor() tends to be quicker when numeric or character vectors are passed to it. The two commands also treat NA levels slightly differently. You can read more about it in this Stack Overflow post. Finally, some R functions such as the data.frame constructor treat all read all character vectors as factors by default. This can be noticed by examining the dev_data data frame we created earlier: str(dev_data) &#39;data.frame&#39;: 6 obs. of 7 variables: $ country: chr &quot;Argentina&quot; &quot;Georgia&quot; &quot;Mexico&quot; &quot;Philippines&quot; ... $ eys : num 17.6 15.4 14.3 12.7 16.4 15.1 $ mys : num 10.6 12.8 8.6 9.4 7.7 11.3 $ lexp : num 76.5 73.6 75 71.1 77.4 72 $ gni : num 17611 9570 17628 9540 24905 ... $ log_gni: num 9.78 9.17 9.78 9.16 10.12 ... $ edu_ind: num 0.842 0.854 0.684 0.666 0.712 ... As you can see country is a factor with 6 levels - each for one country name. This doesn’t make too much sense, as the column is unlikely to have any repeating values. To avoid this behaviour, we can set the stringsAsFactors optional argument in the data.frame function explicitly to FALSE. This way, all the character vectors remain character variables in the data frame. dev_data &lt;- data.frame(country, eys, mys, lexp, gni, stringsAsFactors = FALSE) str(dev_data) &#39;data.frame&#39;: 6 obs. of 5 variables: $ country: chr &quot;Argentina&quot; &quot;Georgia&quot; &quot;Mexico&quot; &quot;Philippines&quot; ... $ eys : num 17.6 15.4 14.3 12.7 16.4 15.1 $ mys : num 10.6 12.8 8.6 9.4 7.7 11.3 $ lexp : num 76.5 73.6 75 71.1 77.4 72 $ gni : num 17611 9570 17628 9540 24905 ... Reading and writing the data Reading from CSV While so far, we’ve created small and simple datasets by manually typing them into the scripts, the usual way of loading data into R is through external files. The most common format used to store data for R analysis is a CSV file, which stands for Comma Separated Values. This essentially means, that the data is represented as a text file, in which values are separeted by columns to indicate their relative positions - for example, a csv file with 5 columns will have 4 commas to separate them in each row. In the example below, we read in data on Human Development Indicators for 209 countries for 2018 obtained from the UN Human Development Reports. Yuo can download the file used in the example from here. dev &lt;- read.csv(&quot;data/un_data/dev2018.csv&quot;, stringsAsFactors = FALSE) In the above example, the first argument specifies the path to the file read as a string, i.e. enclosed in quotation marks. The file can be read: 1. using absolute path - for example dev &lt;- read.csv(\"C:/Users/yourusername/Documents/dev2018.csv\") in Windows or dev &lt;- read.csv(\"/Users/yourusername/Documents/dev2018.csv\") in MacOS. In this case, you need to provide the full path to where the file is located in the computer. using relative path, as in the above example. In this case, R will search for the directory in your current working directory. Working directory is simply the specific folder in your computer in which R looks for the data. R Studio usually sets one default working directory (this can be changed under Tools -&gt; Global Options -&gt; Set Default Working Directory). This means that every time you open RStudio or restart your R session (as described in Chapter 1, the working directory is set to this default. You can also change working directory manually by executing the setwd() function from your script or the console. setwd(&quot;C:/Users/yourusername/folder&quot;) You can also get your current working directory by using the getwd() function: getwd() While some users tend to include setwd(path/to/project) in the beginnings of their scripts, this is potentially problematic, as whenever you move your data or script to another folder, errors are likely to occur. Therefore, it is a good practice to always set working directory to the location of your R Source script and keep the data in the same folder as your source script. This can be done by choosing the Session tab Note that in this case, it is assumed that you have selected “Set Working Directory” &gt; “To Source File” location from the “Session” tab in Rstudio, as discussed in the Introduction and that the directory of the source file has a folder called “data” in which the dev2018.csv file is stored. Alternatively, dev &lt;- read.csv(\"dev2018.csv\") would read the file directly from your working directory. You could also use dev &lt;- read.csv(\"C:/Users/yourusername/Documents/dev2018.csv\") in Windows or dev &lt;- read.csv(\"/Users/yourusername/Documents/dev2018.csv\") in MacOS to read the data file from an arbitary folder using its absolute path. Similarily to the data.frame constructor, we can also use the stringsAsFactors argument to ensure all character variables are read as strings. You can also save data to .csv files by using the write.csv, which takes the data frame as its first argument and the string specifying the path to which you want to save the file as the second argument. For example, suppose we want to keep only the first 40 rows of the data and store it in a separate file. dev_new &lt;- dev[1:40, ] write.csv(dev_new, &quot;data/un_data/dev_new.csv&quot;) Reading from other formats While csv is the most common format, the data is often likely to come in many other variants - common examples include Stata’s .dta files or SPSS’ .sav, as well as .xlsx Excel format. Some of the R packages offer functionalities percisely to deal with such files. So far, we have only used the built-in functionalities offered by R. While their range is pretty extensive and the ones covered in this course are only the tip of the iceberg, much more than that is offered by user-made packages, which offer new functions useful for specific tasks. The official R packages are available through CRAN. To use a package it needs to be installed first and then loaded. For example, to use an example package named foo, you should first run install.packages(\"foo\") to download the package files from CRAN and install it and then put library(foo) in your R Script to load it into R. Note that while installation has to be done only once, you have to load the library every time you use it - that’s why, you should always put the library calls at the top of your R script. If you use a function from that package without loading it first, your R script execution will fail! Please also note, that you pass the package name as a string (i.e. in quotation marks) to the install.packages, but without them to library. install.packages(&quot;haven&quot;) Coming back to our example, we can use the R package haven to load Stata, SPSS and SAS files. You can see an example below: library(haven) dev_stata &lt;- read_dta(&quot;data/un_data/dev2018.dta&quot;) Similarily, the data can be written using: write_dta(dev, &quot;data/un_data/dev2018.dta&quot;) Other alternatives offered by the haven package inlcude read_sav or read_xpt. Other packages useful for reading unusual data types include readxl for reading Excel files and foreign for a broader choice of file types. Missing values As mentioned earlier, the NA missing value constant is particularly important in R. Real-life data that you are likely to deal with most of the time when using R in practice is often imperfect and missingness should be addressed as one of the first steps of the analysis process. is.na() command can be used to determine whether a value of an R object is missing. It returns true for each value of the index which is missing. numbers &lt;- c(1, 4, NA, 6, NA) is.na(numbers) [1] FALSE FALSE TRUE FALSE TRUE To count NAs in an R object we can levarage the fact that TRUE values are also interpreted as 1 and use the sum function: sum(is.na(numbers)) [1] 2 You can also verify whether an object contains NAs using the anyNA function. Let’s check if the HDI data we have loaded contains any missing values: dev &lt;- read.csv(&quot;data/un_data/dev2018.csv&quot;) anyNA(dev) [1] TRUE The column returns TRUE. Therefore there is some missingness in the data. Another useful function for missing data analysis is complete.cases. As the name suggests, given a data frame it returns a logical vector with TRUE for each row which doesn’t contain missing values. We can verify which observations are the cause of the data missingness: dev[!complete.cases(dev), ] country eys gni lexp mys 91 Korea (Democratic People&#39;s Rep. of) 10.8 NA 72.1 NA 122 Nauru 11.3 17313 NA NA 150 San Marino 15.1 NA NA NA 180 Tuvalu 12.3 5409 NA NA 195 Somalia NA NA 57.1 NA Lists The final key R data structure covered in this section are lists. Similarily to data frames, lists can be thought of containers to store other data structures.2 However, unlike data frames, they are less strict in terms of their contents - a list can store vectors of different length, data frames and even other lists. Lists are created with the list() constructor. my_list &lt;- list(names = c(&quot;Tom&quot;,&quot;James&quot;,&quot;Tim&quot;), values = 1:20) my_list $names [1] &quot;Tom&quot; &quot;James&quot; &quot;Tim&quot; $values [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 You can extract elements from a list using their names or their numeric index. To index a list, double square brackets [[ are used, as opposed to vectors. my_list[[1]] [1] &quot;Tom&quot; &quot;James&quot; &quot;Tim&quot; my_list[[&quot;names&quot;]] [1] &quot;Tom&quot; &quot;James&quot; &quot;Tim&quot; If a list is indexed with single brackets, it returns a one-element list, rather than the object stored in it: values &lt;- my_list[[&quot;values&quot;]] str(values) int [1:20] 1 2 3 4 5 6 7 8 9 10 ... values &lt;- my_list[&quot;values&quot;] str(values) List of 1 $ values: int [1:20] 1 2 3 4 5 6 7 8 9 10 ... You can also extract elements from a list using the $ operator, similarily to data.frames. Finally, you can assign values to lists similarily as in the case of vectors or data.frames: my_list[[&quot;new&quot;]] &lt;- c(&quot;new&quot;,&quot;values&quot;) str(my_list) List of 3 $ names : chr [1:3] &quot;Tom&quot; &quot;James&quot; &quot;Tim&quot; $ values: int [1:20] 1 2 3 4 5 6 7 8 9 10 ... $ new : chr [1:2] &quot;new&quot; &quot;values&quot; 3.2 Summary Data Frames are one of the most common data structures in R. You can think about them as an Excel spreadsheet with tables with rows and columns or as a list of variables (vectors of equal length), each with its unique name. Each value in a data frame has two indexes - one for the row number and one for the column number. For example, df[2,3] retrieves the value in the second row from the third column. Factors are a special kind of vectors that can only take a pre-specified set of values, determined when the factor is created. csv files are the most common way of storing data in R. You can load the data from them using read.csv and save the data using write.csv. working directory is the folder in your computer where R looks for files by default. You can check it using the getwd() function, change it using setwd()or set it to the location where your current script is located by selecting Session &gt; Set Working Directory &gt; To Source File Location. packages are sets of new functions developed for R by external developers, extending its functionalities. You can install them using install.packages and load them using the library function. missing values are marked in R by the NA token. There are many useful functions created to detect missing values, such as is.na, complete.cases or anyNA. lists are another type of R data structure. They can be thought of as containers, which can be used to store arbitrary elements at each position. Functions list 3.3 Exercises The following code returns an error. Why? Check what happends if we set b to 1:5 instead of 1:3. Explain this behaviour. df &lt;- data.frame(a = 1:10, b = 1:3) What is the difference between character and factor vectors in R? In what situation you might prefer one over the other and vice versa? To complete this exercise, load the dev2018.csv data into R. What proportion of the rows are complete? Store all the non-missing rows in a data.frame called dev_clean. For the dev_clean, compute the HDI following the method outlined in the previous chapter. Use indexing to retrieve: countries with HDI greater than 0.7 or GNI per capita greater than 10000 10 countries with the largest GNI 10 countries with shortest life expactancy at birth the development data for Poland countries with Education Index higher than Life Expectancy Index The UN categorizes the countries into 4 groups based on their HDI value - very high human development \\(HDI \\geq 0.8\\), high human development \\(0.8 &gt; HDI \\geq 0.7\\), medium human development \\(0.7 &gt; HDI \\geq 0.55\\) and low human development \\(0.55 &gt; HDI\\). Based on this thresholds, create a data frame called hdi_groups, with element names \"vhigh\", \"high\", \"med\", \"low\", with each containing a dataframe only with observations corresponding to its respective HDI group. How many rows (as a fraction of total data.frame size) does each of these levels consist of? The following operation returns a warning error and the result is not quite as you would expect. Why? How would you replace the first element of the list with the 1:5 sequence so that the error doesn’t appear? Name the two ways this could be done. my_list &lt;- list(vals = 1:10, names = c(&quot;Jane&quot;,&quot;Kate&quot;)) my_list[1] &lt;- 1:5 Warning in my_list[1] &lt;- 1:5: number of items to replace is not a multiple of replacement length The solutions for the exercises will be available here on 2020-11-12. Chapter solutions More specifically, the data.frame class is a special type of list - you can verify that by running the typeof function with a data frame as input.↩︎ "],["exploratory.html", "4 Exploratory Data Analysis 4.1 Content 4.2 Summary 4.3 Exercises", " 4 Exploratory Data Analysis Chapter script Chapter markdown Chapter Data 4.1 Content What is exploratory data analysis? Next to data cleaning, exploratory data analysis is one of the first steps taken in the process of analysing quantitative data of any kind. Essentially, it refers to getting an overview of the data through looking at simple summary statistics and plots to understand the distribution of each variable, as well as look for particularly obvious and pronounced relationships between the variables. If one is taking a deductive approach, with a clear-cut, falsifiable hypothesis about the data defined upfront (such as higher per capita income is related to higher levels of democracy or income equality increases levels of subjective well-being), exploratory data analysis helps you verify whether the hypothetized relationship is in the data at all - for example by applying the so-called Inter-Ocular Trauma Test (if it hits you between the eyes, it’s there!) to a plot. This informs further formal statistical analyses. It also allows you to identify factors that may be important for the hypothesized relationship and should be included in the formal statistical model. In case of an inductive approach, exploratory data analysis allows you to find patterns and form hypothesis to be furhter tested using formal statistical methods. In this chapter, we will cover some basic exploratory methods that can be applied to examine numeric and categorical data. For this purpose we will use the data from UCI Machine Learning Repository, which covers math grades achieved in three years of education by a sample of students, along with some demographic variables 3. The data download link is available at the top of this course page. To load it, we can use the familiar read.csv function. Note that in this case, the sep optional argument is specified to \";\". You can find out why by reading about this argument in the function documentation (?read.csv) and by examining the dataset using your computer’s notepad app. math &lt;- read.csv(&quot;data/student/student-mat-data.csv&quot;, sep = &quot;;&quot;) First look at the data After loading the data into R, it’s useful to get an overview of it. The first thing it’s worth to look at is how many variables and how many observations are there in the dataset. This can be seen in the environment browser next to the name of the data frame. We can also access the dimensions of our data (i.e. how many rows and columns/observations and variables it has) through the dim function. dim(math) [1] 395 33 We can see that the math dataset consists of 395 observations of 33 variables. The next useful step is to look at the dataset’s structure using the str variable: str(math) &#39;data.frame&#39;: 395 obs. of 33 variables: $ school : chr &quot;GP&quot; &quot;GP&quot; &quot;GP&quot; &quot;GP&quot; ... $ sex : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... $ age : int 18 17 15 15 16 16 16 17 15 15 ... $ address : chr &quot;U&quot; &quot;U&quot; &quot;U&quot; &quot;U&quot; ... $ famsize : chr &quot;GT3&quot; &quot;GT3&quot; &quot;LE3&quot; &quot;GT3&quot; ... $ Pstatus : chr &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ... $ Medu : int 4 1 1 4 3 4 2 4 3 3 ... $ Fedu : int 4 1 1 2 3 3 2 4 2 4 ... $ Mjob : chr &quot;at_home&quot; &quot;at_home&quot; &quot;at_home&quot; &quot;health&quot; ... $ Fjob : chr &quot;teacher&quot; &quot;other&quot; &quot;other&quot; &quot;services&quot; ... $ reason : chr &quot;course&quot; &quot;course&quot; &quot;other&quot; &quot;home&quot; ... $ guardian : chr &quot;mother&quot; &quot;father&quot; &quot;mother&quot; &quot;mother&quot; ... $ traveltime: int 2 1 1 1 1 1 1 2 1 1 ... $ studytime : int 2 2 2 3 2 2 2 2 2 2 ... $ failures : int 0 0 3 0 0 0 0 0 0 0 ... $ schoolsup : chr &quot;yes&quot; &quot;no&quot; &quot;yes&quot; &quot;no&quot; ... $ famsup : chr &quot;no&quot; &quot;yes&quot; &quot;no&quot; &quot;yes&quot; ... $ paid : chr &quot;no&quot; &quot;no&quot; &quot;yes&quot; &quot;yes&quot; ... $ activities: chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... $ nursery : chr &quot;yes&quot; &quot;no&quot; &quot;yes&quot; &quot;yes&quot; ... $ higher : chr &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; ... $ internet : chr &quot;no&quot; &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; ... $ romantic : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... $ famrel : int 4 5 4 3 4 5 4 4 4 5 ... $ freetime : int 3 3 3 2 3 4 4 1 2 5 ... $ goout : int 4 3 2 2 2 2 4 4 2 1 ... $ Dalc : int 1 1 2 1 1 1 1 1 1 1 ... $ Walc : int 1 1 3 1 2 2 1 1 1 1 ... $ health : int 3 3 3 5 5 5 3 1 1 5 ... $ absences : int 6 4 10 2 4 10 0 6 0 0 ... $ G1 : int 5 5 7 15 6 15 12 6 16 14 ... $ G2 : int 6 5 8 14 10 15 12 5 18 15 ... $ G3 : int 6 6 10 15 10 15 11 6 19 15 ... This lists all the variables in the dataset, with their names and types. This way, we can scope the dataset for the variables that are interesting for our analysis and start thinking about possible relationships we might want to investigate. Finally, we can get some basic summary statistics using the summary function: summary(math) school sex age address famsize Length:395 Length:395 Min. :15.0 Length:395 Length:395 Class :character Class :character 1st Qu.:16.0 Class :character Class :character Mode :character Mode :character Median :17.0 Mode :character Mode :character Mean :16.7 3rd Qu.:18.0 Max. :22.0 Pstatus Medu Fedu Mjob Fjob Length:395 Min. :0.000 Min. :0.000 Length:395 Length:395 Class :character 1st Qu.:2.000 1st Qu.:2.000 Class :character Class :character Mode :character Median :3.000 Median :2.000 Mode :character Mode :character Mean :2.749 Mean :2.522 3rd Qu.:4.000 3rd Qu.:3.000 Max. :4.000 Max. :4.000 reason guardian traveltime studytime failures Length:395 Length:395 Min. :1.000 Min. :1.000 Min. :0.0000 Class :character Class :character 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:0.0000 Mode :character Mode :character Median :1.000 Median :2.000 Median :0.0000 Mean :1.448 Mean :2.036 Mean :0.3342 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:0.0000 Max. :4.000 Max. :4.000 Max. :3.0000 NA&#39;s :1 schoolsup famsup paid activities nursery Length:395 Length:395 Length:395 Length:395 Length:395 Class :character Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character Mode :character higher internet romantic famrel freetime Length:395 Length:395 Length:395 Min. :1.000 Min. :1.000 Class :character Class :character Class :character 1st Qu.:4.000 1st Qu.:3.000 Mode :character Mode :character Mode :character Median :4.000 Median :3.000 Mean :3.944 Mean :3.235 3rd Qu.:5.000 3rd Qu.:4.000 Max. :5.000 Max. :5.000 goout Dalc Walc health absences G1 Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 Min. : 0.000 Min. : 3.00 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:3.000 1st Qu.: 0.000 1st Qu.: 8.00 Median :3.000 Median :1.000 Median :2.000 Median :4.000 Median : 4.000 Median :11.00 Mean :3.112 Mean :1.482 Mean :2.291 Mean :3.554 Mean : 5.709 Mean :10.91 3rd Qu.:4.000 3rd Qu.:2.000 3rd Qu.:3.000 3rd Qu.:5.000 3rd Qu.: 8.000 3rd Qu.:13.00 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :75.000 Max. :19.00 NA&#39;s :1 NA&#39;s :1 G2 G3 Min. : 0.00 Min. : 0.00 1st Qu.: 9.00 1st Qu.: 8.00 Median :11.00 Median :11.00 Mean :10.72 Mean :10.42 3rd Qu.:13.00 3rd Qu.:14.00 Max. :19.00 Max. :20.00 NA&#39;s :1 This lists the minimum, maximum, mean and quartiles for each of the numeric variables, along with the count of missing values in it. These concepts will be discussed in more detail in the next sections of this chapter. In case of factor variables, it provides a count of each level of the variable. It also mentions the number of missing variable - in this case, we can see that there are couple of NAs in there. Before starting the analysis, we need to address that - in this case, we simply drop them. math &lt;- math[complete.cases(math), ] Numeric vs categorical variables Numeric variables Histograms The simplest and often most powerful way to examine a single numeric variable is through the use of histogram. Histogram divides a variable in ranges of equal size called bins. Each bin is then represented as a bar, the height of which corresponds with the count/proportion of the observations falling in that range. The main difference between a histogram and a bar chart is that a histogram does not have breaks between the bars, because the variable it describes is assumed to be continuous, not discrete. Let’s examine two numeric variables from the math dataset - age and absences (total absent hours recorded by the teacher for each student) using histograms. A histogram is created using the hist function: hist(math$age, breaks = length(unique(na.omit(math$age)))) hist(math$absences) In both cases, we can see that the lowest values are the most frequent. For example, from the second histogram we can read that 250 of the 395 students in the samples were absent between 0-5 hours during the school year. Mean Mean (aka average) is the simplest statistic describing any numeric variable - it is simply the sum of a variable/vector divided by its length. In R, we can calculate the mean of any variable using the mean function. For example, let’s examine the average number of absences in the sample: sum(math$absences)/nrow(math) [1] 5.750649 mean(math$absences) [1] 5.750649 The same done for age returns this value: mean(math$age) [1] 16.68312 Had we not removed the missing values (NA's) at the start, this command would not have worked and would have returned NA. Should you ever come across this issue, then this is a reminder that you need to do something with your missing data. If you just wish to circumvent the problem for the time being, you could call: mean(math$age, na.rm = TRUE) [1] 16.68312 where we specify the argument na.rm(remove NA values) to TRUE. We can also use the trim argument from to specify the fraction of observations to be removed from each end of the sorted variables before calculating the mean. This makes our estimate of the mean more robust to potentially large and unrepresentative values affecting the calculated value - so-called outliers, which will be discussed more extensively in the section on quantiles. Note that specifying the trim argument to 0.1 doesn’t seem to change the mean of age significantly: mean(math$age, na.rm = TRUE, trim = 0.1) [1] 16.62136 However, doing the same in case of absences changes the value of average absences quite a lot. Can you think of the reason why? Take a look at the histograms of both variables. mean(math$absences, trim = 0.1) [1] 4.262136 Variance and standard deviation While a mean offers a good description of the central tendency of a variable (i.e. a value that we would expect to see most often), describing a variable just by its mean can be very misleading. For example, consider the values -10000, 20, 10000 and 15, 20, 25. In both cases the mean is the same: x &lt;- c(15, 20, 25) y &lt;- c(-985, 20, 1025) mean(x) == mean(y) [1] TRUE However, it would be very misleading to say that these variables are similar. We could try to describe this difference by computing the average distance between each value and the mean: mean(x - mean(x)) [1] 0 mean(y - mean(y)) [1] 0 However, this results in a 0, since the negative and positive values in our example cancel each other out. To avoid this, we can measure the variance, which calculates the mean of the sum of the squared distances between each value of a variable and its mean. Since the distance is squared (always positive), the positive and negative values will not cancel out. mean((x - mean(x))^2) [1] 16.66667 mean((y - mean(y))^2) [1] 673350 We can see that this captures the difference between our two vectors. You can calculate the variance with a simple shortcut in R, with the var function: var(x) [1] 25 var(y) [1] 1010025 Note that this gives us different results than our variance computed by hand. This is because we calculate the population wariance in which we divide by the number of observations in the population (or length of the vector), N. So, the variance we calculated “manually” above is equivalent to: sum((x - mean(x))^2)/length(x) [1] 16.66667 mean((x - mean(x))^2) [1] 16.66667 Instead, the var() function calculates the sample variance, for which we divide the sum of squared distances from the mean by \\(N-1\\). This is because dividing the sample by N tends to underestimate the variance of the population. The mathematical reasons behind it are clearly outlined in this article. So, we can “manually” arrive at equivalent estiamte to the one obtained using the var function by: sum((x - mean(x))^2)/(length(x) - 1) == var(x) [1] TRUE We can apply the variance function to absence and age, to see their spread: var(math$age, na.rm = TRUE) [1] 1.59724 var(math$absences) [1] 65.06787 One problem arising when using variance to describe the data is that its units aren’t interpretable, since they are squared. Therefore, saying that the variance of the absence time is 64 squared hours doesn’t sound too intuitive. To avoid this, we usually use use the standard deviation in practice, which is simply the square root of the variance. Through taking the square root we return the variable to its original units. The standard deviation of a variable is calculated using the sd function: sd(math$age, na.rm = TRUE) [1] 1.26382 sd(math$age, na.rm = TRUE) == sqrt(var(math$age, na.rm = TRUE)) [1] TRUE Not ethat I am including the na.rm = TRUE option for illsutrative purposes only, since we did remove missing values, earlier. We can know compare the standard deviation and mean of both variables: with(math, c(mean = mean(age, na.rm = TRUE), sd = sd(age, na.rm = TRUE))) mean sd 16.68312 1.26382 with(math, c(mean = mean(absences, na.rm = TRUE), sd = sd(absences, na.rm = TRUE))) mean sd 5.750649 8.066466 It can be clearly seen that the hours of students’ absence have more variability than the students’ ages. This makes intuitive sense, since the sample consists of students from roughly the same age group (the easiest way you can see it is by running unique(math$age)). At the same time, students differ match more in the total hours of absence. This explains why the trimmed mean was that different from overall mean in case of absences, yet quite similar for age. You can use the widget below to see how varying the standard deviation and the mean affects the distribution of a variable (in this case a normally distributed random variable). Note that you need an active internet connection for the app to load. Quantiles The final statistic we are going to discuss is quantile. Quantiles allow us to get a better grasp of the distribution of the data. Essentially, quantiles are cut points that divide the variable into intervals of equal sizes. For example, deciles are 10-quantiles, dividing the variable into 10 ranges. For example the 8th decile of a variable is the value greater than 80% of the values in this variable. In R we can obtain an arbitrary quantile using the quantile function, specifying the proportion below each of the cutpoints through the probs argument. quantile(math$absences, probs = 0.9) 90% 14 In the above example, we can see that 90% of the values of the variable absences are lower than 14. The probs argument can be either a scalar or a vector, so we can obtain multiple quantiles at once. For example in the example below we obtain so-called quartiles (4-quantiles). quantile(math$absences, probs = c(0, .25, .5, .75, 1)) #quartiles 0% 25% 50% 75% 100% 0 0 4 8 75 We could get deciles by: quantile(math$absences, probs = seq(0, 1, by = 0.1)) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 0 0 0 2 2 4 4 6 10 14 75 We can visualize this using a histogram: Median The median is a specific quantile - the 50th percentile of a variable, i.e. the midpoint of the variable’s distribution. As opposed to the mean, it’s not affected by outlying values. Large differences between mean and meadian are often evidence of a skew in the variable’s distribution. Outliers Finally, as mentioned earlier, quantiles are particularly useful when it comes to identifying outliers. Outliers are observations with extreme values, lying far from majority of values in a dataset. In some cases they may be results of data collection error, while in others they are simply rare examples of our variable of interest taking a very high or low value. Outliers can often extert high levarage on a given statistic we are measuring (such as the mean), and removing them may sometimes change the results of our analysis significantly. Thus, it is often worth removing them and re-running the analysis to make sure that it’s not affected too severly by a small number of observations with extreme values. Note that this is not to say that outliers should always be removed or disregarded - contrary to that, observations with outlying values should be treated with extra care and it is the role of the analyst to examine why are these values extreme and what are the possible implications for the analysis. Quantiles can be used to find the outlying observations - for example, by looking at the 0.001 and 0.999 cutpoints, and considering all values below or above to be outiers. Box plots Box plots are commonly used to visualize the distribution of a variable. Below, we use the boxplot function to plot a boxplot of the age variable from the math dataset. boxplot(math$age) The box in the middle of the plot corresponds with the inter-quartile range of the variable (IQR) - this is the range between the 1st and the 3rd quartile of the variable (which is equivalent to the value range between 25th and 75th percentile). The thick line in the middle corrsponds to the variable’s median (the 2nd quartile/50th percentile). The ‘whiskers’ (i.e. the horizontal lines connected by the dashed line with each end of the box) correspond to the minimum and the maximum values of the variable. The maximum is defined as the largest value in the variable that is smaller than the number 1.5 IQR above the third quartile and the minimum is the lowest value in the variable that is larger than the number 1.5 IQR below the first quartile. Anything above/below the whiskers numbers is considered an outlier and marked with a dot. In the above example we can see that one observation is an outlier, lying significantly above the upper whisker of the boxplot. We can identify this value by plugging in the above formula for the upper whisker (#3rd quartile + \\(1.5IQR\\)) and finding the value that lies above it. maximum &lt;- quantile(math$age, 0.75, na.rm = TRUE, names = FALSE) + 1.5 * IQR(math$age, na.rm = TRUE) math$age[which(math$age &gt; maximum)] [1] 22 While useful under many circumstances, box plots can be deceiving, as two similarily looking box plots can represent very different disttributions. That’s why it is always useful to look at the variable’s histogram as well. This can be seen in the example below: Scatter plots Finally, a good way to explore the relationship between two numeric variables visually are scatter plots. Scatter plots represent each observation as a marker, with x-axis represnting value of one variable and y-axis of another. Scatter plots are simply created using the plot function. plot(math$G1, math$G2) In the example above, we can see that there’s a positive relationship between student’s grade in first year and the grade in the second year. While such plot would not be sufficient to make any strong empirical claims, it is usually a valuable first step in finding statistical regularities in the dataset. More formal ways of measuring association between variables will be discussed in sections on statisical association and linear regression. Categorical variables Cross-tabulation Categorical variables are often best described by frequency tables, which provide the counts of the number of occurrences of each level of the categorical variable. table(math$sex) F M 203 182 Additionally, we can transform this into a table of proportions, rather than frequencies, by using the prop.table function to transform the output of table. prop.table(table(math$sex)) F M 0.5272727 0.4727273 We can also convert such table into a bar plot, by using the barplot function. barplot(table(math$Mjob)) barplot(prop.table(table(math$Mjob)), names.arg = c(&quot;At home&quot;, &quot;Health&quot;, &quot;Other&quot;, &quot;Services&quot;, &quot;Teacher&quot;)) The table function can also be used for cross-tabulation - creating a table summarizing the count of observations in the overlap of two categories. In the example we look at the relationship between the reason for choosing the particular school and paid classes attendance. table(math$reason, math$paid) no yes course 91 49 home 51 56 other 16 20 reputation 51 51 It appears that students who chose the school because of their course preference were less likely to attend extra paid classes than students choosing the school for other reasons. This can be made more apparent if we substitute frequencies with proportions: prop.table(table(math$reason, math$paid)) no yes course 0.23636364 0.12727273 home 0.13246753 0.14545455 other 0.04155844 0.05194805 reputation 0.13246753 0.13246753 Note that in this case, the proportions are calculated with respect to the total count of participants (i.e. they add up to 1). For comparison purposes, it might be useful to look at the proportion with respect to the total of each of the categories. This can be specified by the margin argument. By setting it to 1, we calculate the proportions with respect to the row margins, i.e. divide the counts of individuals in the paid variable by the total of each category of the reason variable. prop.table(table(math$reason, math$paid), margin = 1) no yes course 0.6500000 0.3500000 home 0.4766355 0.5233645 other 0.4444444 0.5555556 reputation 0.5000000 0.5000000 By analogy, margin = 2 leads to the division by the column margins, i.e. sums of both categories of the paid variable. prop.table(table(math$reason, math$paid), margin = 2) no yes course 0.43540670 0.27840909 home 0.24401914 0.31818182 other 0.07655502 0.11363636 reputation 0.24401914 0.28977273 Customizing visualizations In the previous sections, we discussed some basic tools for data visualizations in R, such as histograms, scatter plots, box plots or bar charts. R base graphics allows the user to create powerful and great-looking visualizations. However achieving can be quite complicated. Because of that, a dedicated called ggplot2 was created to enable creating good-looking and informative visualziations with much simpler user interface. The data visualization chapter covers this in more detail. However, in case you wanted to start preparing visualizations for other purposes than exploratory data analysis, you might find some of the tips below useful: Changing axis labels In case of every plot in R you can change the axis labels by using the xlab and ylab arguments: plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;) You can also add the title by specifying the main argument: plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;, main = &quot;Student grades&quot;) The color of the objects in the plot can be altered using the col argument: plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;, main = &quot;Student grades&quot;, col = &quot;red&quot;) It can also be specified as character vector, with a different color for each point: col_gender &lt;- rep(&quot;red&quot;, nrow(math)) col_gender[which(math$sex == &quot;F&quot;)] &lt;- &quot;blue&quot; plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;, main = &quot;Student grades&quot;, col = col_gender) To make it more informative, you can also add a legend using the legend function: col_gender &lt;- rep(&quot;red&quot;, nrow(math)) col_gender[which(math$sex == &quot;F&quot;)] &lt;- &quot;blue&quot; plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;, main = &quot;Student grades&quot;, col = col_gender) legend(&#39;bottomright&#39;, legend = c(&#39;Female&#39;, &#39;Male&#39;), col = c(&#39;blue&#39;, &#39;red&#39;), pch = 1) You can also change the limits of each of the axes, by specifying the xlim and ylim arguments col_gender &lt;- rep(&quot;red&quot;, nrow(math)) col_gender[which(math$sex == &quot;F&quot;)] &lt;- &quot;blue&quot; plot(math$G1, math$G2, xlab = &quot;Grade in term 1&quot;, ylab = &quot;Grade in term 2&quot;, main = &quot;Student grades&quot;, xlim = c(0, 30), ylim = c(0, 30)) 4.2 Summary Exploratory data analysis is the essential first step of any quantitative data analysis. It provides you with an overview of the data and allows to select variables of interest, verify your first intuitions about the data and explore possible relationships. Functions useful for the first overview include str() and summary. Histograms are a useful way of summarizing a variable’s distribution by representing it by bars of equal width (i.e. equal value ranges), the height of which corresponds to the number/proportion of values within a given range. Mean, variance and standard deviation are commonly used measures of numeric data. The first refers to the most likely value of a variable, the later two - to the variable’s spread. The advantage of standard deviation over variance is that it’s expressed in the same units as the data. We can obtain them in R by using the mean, var and sd functions. Quantiles are cutpoints that divide a numeric variable into ranges of equal proportions. We can calculate arbitrary quantiles by using the quantile function. Box plots summarize the distribution of a variable by presenting the interquartile range as a box, with median as a tick line in the middle, whiskers extending to the maximum and minimum and dots marking outliers. Scatter plots are useful to explore the relationship between two numerical variables. You can create them using the plot function. Categorical variables describe data that can be classified into a number of discrete categories. You can summarize one categorical variable or relationship between two categorical variables by using frequency tables, available in R through the table function. The prop.table function can normalize the frequencies in table cells into proportions. You can also wrap table into a barplot. Functions list function package description names() c(“.GlobalEnv”, “base”) retrieve names of a list/vector plot() c(“graphics”, “package:base”) Generic function from base R to produce a plot var() c(“pROC”, “package:stats”) NA c() base Combine values/vectors into a vector dim() base get dimensions of a data frame length() base get number of elements in a vector or list library() base load an R package mean() base get mean of a vector nrow() base get number of rows of a data frame prop.table() base transform frequency table into table of proportions rep() base repeat a value or a vector N times seq() base create a sequence sqrt() base square root sum() base get sum of numeric values or a vector summary() base Obtain summary statistics or detailed regression output table() base obtain frequency table of a variable/cross-tabulation of two variables unique() base get unique elements which() base return indexes of TRUE entries of a logical vector with() base evaluate expression in the context of a data frame (without using the “$” operator) rgnorm() gnorm NA barplot() graphics plot a simple bar plot boxplot() graphics plot a simple box plot hist() graphics plot a simple histogram legend() graphics NA par() graphics set parameters of the plotting device segments() graphics NA text() graphics NA complete.cases() stats retrieve indices of complete observations IQR() stats obtain the inter-quartile range of a vector median() stats NA na.omit() stats NA quantile() stats obtain empirical quantiles of a vector rnorm() stats NA sd() stats Get standard deviation of a vector read.csv() utils read a csv file to data frame. Specify stringsAsFactors = FALSE to keep all string columns as characters str() utils get the structure of an R object 4.3 Exercises Match the histograms and boxplots below: The variable x contains the numbers [1, 20, 4, 50, 30, 40]. Compute the IQR. Which values would be the whiskers in the box plot? Which (if any) would be the outliers? Confirm your calculations by plotting the box plot Load the student-por.csv into your R session. The data describes the grades obtained by students from a Portugeese class and consists of variables similar to those in the math dataset. Conduct an EDA by preforming the following steps: por &lt;- read.csv(&quot;data/student/student-por.csv&quot;, sep = &quot;;&quot;) Examine the variables in the dataset, their types, their distribution and the first 5 observations. Get the proportion of missing observations (overall and in each variable) and remove the observations with missing values. Use tools of your choice to get analyze the distribution of the age, absence and grade variables. Identify the outliers in both cases. How do these variables compare with their couterparts from the math dataset analyzed in the chapter? How many girls and boys are there in the por dataset? Visualize it using a bar plot. The solutions for the exercises will be available here on 2020-11-12. Chapter solutions note that the data was slightly modified to include missing values for demonstration purposes↩︎ "],["programming.html", "5 Key Programming Concepts 5.1 Content 5.2 Summary 5.3 Exercises", " 5 Key Programming Concepts Chapter script Chapter markdown Chapter exercises Chapter data 5.1 Content To make the data analysis more efficient, it is crucial to understand some of the crucial programming concepts. In the first part of this section we discuss for loops and if statements. These are so-called “control flow statements”, which are common to almost all programming languages. The second part will discuss the creation and basic usage of functions. Finally, the third part will go through the sapply() function family, a common tool used in R to apply functions over objects multiple times. Control flow statements For loops For loops are essentially a way of telling the programming language “perform the operations I ask you to do N times”. A for loop in R beginns with an for() statement, which is followed by an opening curly brace { in the same line - this is esentially opening the for-loop. After this, usually in a new line, you place the code which you want to execute. Then, in the last line you close the for loop by another curly brace }. You can execute the for loop by placing the cursor either on the for statement (first line) or the closing brace (last line) and executing it as any other code. Below, you can see the for loop printing the string \"Hello world!\" 5 times for(i in 1:5) { print(&quot;Hello world&quot;) } [1] &quot;Hello world&quot; [1] &quot;Hello world&quot; [1] &quot;Hello world&quot; [1] &quot;Hello world&quot; [1] &quot;Hello world&quot; The i in the for statements is the variable that will sequentially take all the values of the object (usually a vector) specified on the right hand side of the in keyword. In majority of the cases, the object is a sequence of integers, as in the example below, where i takes the values of each element of the vector 1:5 and prints it. for(i in 1:5) { print(i) } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 A for loop could be used to add a constant to each element of a vector: x &lt;- c(4, 5, 1, 2, 9, 8, 0, 5, 3) x [1] 4 5 1 2 9 8 0 5 3 #for all integers between 1 and length of vector x: for(i in 1:length(x)) { x[i] &lt;- x[i] + 5 } x [1] 9 10 6 7 14 13 5 10 8 However, in R this is redundant, because of vectorization (see the section on vectors from chapter 2). The above statement os equivalent to: x &lt;- c(4, 5, 1, 2, 9, 8, 0, 5, 3) x + 5 [1] 9 10 6 7 14 13 5 10 8 This is not only simpler, but also more efficient. Another, more practical aplication of the for loop could examine all columns of a data frame for missing values, so that: dev &lt;- read.csv(&quot;data/un_data/dev2018.csv&quot;, stringsAsFactors = FALSE) missing &lt;- numeric() #create empty numeric vector for (i in 1:length(dev)){ missing[i] &lt;- sum(is.na(dev[,i])) #get sum of missing for ith column names(missing)[i] &lt;- names(dev)[i] #name it with ith column name } missing country eys gni lexp mys 0 1 3 3 5 From this, we can see that there are 0 misisng values in the country name, 1 missing value in the expected years of schooling variable and, 3 missing values in gni and life expectancy and 5 missing values in mean years of schooling. While this is a bit more useful than the previous example, R still offers a shorthand method for such problems, which is discussed in more detail in the last part of this chapter. In general, due to the phenomena of vectorization, for loops are rarely used in simple data analysis in R. However, they are a core element of programming as such, therefore it’s important to understand them. In fact, vectorization is made possible only because of for loops being used by R in the background - simply their faster and more efficient versions. If statements If statements are another crucial programming concept. They essentially allow performing computation conditionally on a logical statement. In other words, depending on a logical expression an operation is performed or not. If loops in R are constructed in the following way: if (logical_expression) { operations } Where logical_expression must an expression that evaluates to a logical value, for example X &gt; 5, country == \"France\" or is.na(x). operations are performed if and only if the logical_expression evaluates to TRUE. The simples possible example would be x &lt;- 2 if (x &gt; 0) { print(&quot;the value is greater than 0&quot;) } [1] &quot;the value is greater than 0&quot; x &lt;- -2 if (x &gt; 0) { print(&quot;the value is greater than 0&quot;) } If is naturally complemented by the else clause, i.e. the operations that should be performed otherwise. The general form of such statement is: if (logical_expression) { operations } else { other_operations } In this case, R first checks if the logical_expression evaluates to TRUE, and if it doesn’t, performs the other_operations. For example: x &lt;- -2 if (x &gt; 0) { print(&quot;the value is greater than 0&quot;) } else { print(&quot;the value is less or equal than 0&quot;) } [1] &quot;the value is less or equal than 0&quot; Finally, else if allows to provide another statement to be evaluated. The general form of such statement would be: if (logical_statement) { operation } else if (other_logical_statement) { other_operation } else { yet_another_operation } Here, R first checks the logical_statement, if it’s FALSE then it proceeds to check the other_logical_statement. If the second one is TRUE if performs the other_operation and if it’s FALSE it proceeds to perform the yet_another_operation. An extension of the previous example: x &lt;- 2 if (x &gt; 0) { print(&quot;The value is positive&quot;) } else if (x &lt; 0) { print(&quot;The value is negative&quot;) } else { print(&quot;The value is 0&quot;) } [1] &quot;The value is positive&quot; IF-ELSE statments can be used to conditionally replace values. For example, suppose that we want to create a variable that is 1 when country is France and 0 otherwise. We could do that by: dev$france &lt;- 0 for (i in 1:nrow(dev)) { if (dev$country[i] == &quot;France&quot;) { dev$france[i] &lt;- 1 } } dev$france[dev$country == &quot;France&quot;] [1] 1 Again, because of vectorization, R offers a shorthand for this, through the ifelse() function: dev$france &lt;- ifelse(dev$country == &quot;France&quot;, 1, 0) dev$france[dev$country == &quot;France&quot;] [1] 1 When you look at the documentation ?ifelse, you can see that it takes three arguments - test, yes and no. The test argument is the logical condition - same as logical_statement in the if, with the small subtle difference that it can evaluate to a logical vector rather than one single logical value. The yes argument is the value returned by the function if the test is TRUE and the no argument is returned when test is FALSE. You can fully see this in the example below: ifelse(c(TRUE, FALSE, FALSE, TRUE), &quot;yes&quot;, &quot;no&quot;) [1] &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ifelse(c(TRUE, FALSE, FALSE, TRUE), 1, 0) [1] 1 0 0 1 Functions R is known as a functional programming language - as you have already seen, almost all of the operations performed are done using functions. It is also possible to create our own, custom functions by combining other functions and data structures. This is done using the function() keyword. The general syntax of a function looks as follows: function_name &lt;- function(arg1, arg2) { output &lt;- operations(arg1, arg2) output } As with any R object, you can use almost any name instead of function_name. Arguments are separeted by commas (in the above example arg1, arg2) - these are the objects you pass to your function on which you perform some arbitrary operations. Again, the arguments can have arbitrary names, but you need to use them within the function consistently. Finally, most of the functions return a value - this is the last object called within the function (output in the above example). After creating the function we can run it, exactly the same way as we would with any of R’s built-in functions. A simple example could return the number of missing values in an object: count_na &lt;- function(x) { sum(is.na(x)) } count_na(dev$mys) [1] 5 We could also implement our own summary statistics function, similar to describe() discussed in the previous chapter: summary_stats &lt;- function(x) { if (is.numeric(x)) { list(Mean = mean(x, na.rm = TRUE), SD = sd(x, na.rm = TRUE), IQR = IQR(x, na.rm = TRUE)) } else if (is.character(x)) { list(Length = length(x), Mean_Nchar = mean(nchar(x))) } else if (is.factor(x)) { list(Length = length(x), Nlevels = length(levels(x))) } } Let’s walk through the above function Given a vector x, the function : 1. Checks whether x is a numeric vector. If so, returns a list of it’s mean, standard deviation and interquartile range. 2. Else, checks if x is a character vector. If so, returns a list containng its length and average number of characters. 3. Else, checks if x is a factor. If so returns a list containing its length and average number of character. We can see how it works below: summary_stats(c(1, 2, 3, 10)) $Mean [1] 4 $SD [1] 4.082483 $IQR [1] 3 summary_stats(dev$country) $Length [1] 195 $Mean_Nchar [1] 9.902564 summary_stats(as.factor(dev$country)) $Length [1] 195 $Nlevels [1] 195 Keyword arguments Many of the functions used in R come with so-called default arguments - this was already mentioned in sorting. When defining our own functions, we can make use of that functionality as well. For example, the count_na example can be modified in the following way: count_na &lt;- function(x, proportion = TRUE) { num_na &lt;- sum(is.na(x)) if (proportion == TRUE) { num_na/length(x) } else { num_na } } The proportion argument controls whether the function returns the number of NAs as value or as proportion of the entire vector: count_na(dev$gni) [1] 0.01538462 count_na(dev$gni, proportion = TRUE) #same as above [1] 0.01538462 count_na(dev$gni, proportion = FALSE) [1] 3 There are couple of reasons why functions are frequently applied when analyzing data: 1. To avoid repetition - often, you need to perform the same operation repeatedly - sometimes on a dataframe with tens or hunderds of columns or even multiple data frames. To avoid re-writing the same code over and over again (which always increases the chance of an error occuring). 2. To enhance clarity - when you perform a long and complicated series of operations on a dataset, it’s often much easier to break it down into functions. Then when you need to come back to your code after a long time, it is often much easier to see recode_missing_values(data) appear in your code, with the record_missing_values function defined somewhere else, as you don’t need to go through your code step by step, but only understand what particular functions return. 3 To improve performance - while most of the operations we’ve seen in R take fractions of seconds, larger data can often lead to longer computation times. Functions can be combined with other tools to make computation more elegant and quicker - some of these methods are discussed in the next section. Sapply Recall the code we used to check each column of our data frame for missingness in the for loops section: missing &lt;- numeric() #create empty numeric vector for (i in 1:length(dev)){ missing[i] &lt;- sum(is.na(dev[,i])) #get sum of missing for ith column names(missing)[i] &lt;- names(dev)[i] #name it with ith column name } We could re-write it using our new knowledge of functions, such that: count_na &lt;- function(x) { sum(is.na(x)) } missing &lt;- numeric() for (i in 1:length(dev)) { missing[i] &lt;- count_na(dev[,i]) names(missing)[i] &lt;- names(dev)[i] } missing country eys gni lexp mys france 0 1 3 3 5 0 While this may look a bit more fancy, in fact more code was used to perform this operation and it doesn’t differ too much in terms of clarity. The exact same result can be achieved using the sapply() function. sapply() takes two arguments - an R object, such as a vector and a data frame and a function. Then, it applies the function to each element of this object (i.e. value in case of vectors, column/variable in case of data frames). sapply(dev, count_na) country eys gni lexp mys france 0 1 3 3 5 0 The result is exactly the same as in the previous case. sapply() used the count_na function on each columns of the dev dataset. When using short, simple functions, sapply() can be even more concise, as we can defined our function without giving it a name. In the example below, instead of defining count_na separately, we define it directly within the sapply() call (i.e. inside the parentheses). This yields the same result. sapply(dev, function(x) sum(is.na(x))) country eys gni lexp mys france 0 1 3 3 5 0 Consider the function below. What do you expect it to return? Try going through each element of the code separately. You can check how the rowSums command works by typing ?rowSums into the R console. quartile &lt;- function(x) { quantiles &lt;- quantile(x, c(0.25, 0.5, 0.75), na.rm = TRUE) comparisons &lt;- sapply(quantiles, function(y) y &lt;= x) rowSums(comparisons) + 1 } The function takes a vector as input and computes three quantiles of its values - 25%, 50%, 75%. You may recall from the previous chapter that quantiles are cut points that divide a variable into ranges of equal proportions in the data set. The resulting quantiles vector consists of three values, corresponding with thre three quantiles. We then use sapply on these three values to compare each of them with the value of the x vector. As a result, we obtain a 3 x n array, where n is length of x. For each of the values of x we get three logical values. Each of them is TRUE when the corresponding value of x was larger than the quantile and FALSE if the corresponding value of x was lower than the quantile. We can then sum the results by row, using rowSums. Our final result is a vector with values of 0, 1 and 2. Its value is 0 if the corresponding value of x was less than all quartiles, 1 if it was greater or equal than the .25, 2 if it was greater or equal than 0.5 and 3 if it was greater or equal than all of them. We then finally add 1 to each, so that they correspond to true quartile numbers (1st quartile, rather than 0th quartile, etc). We can then use the split function, which takes a data frame and a vector as input and splits the data frame into several parts, each with the same value of the splitting variable. As a result, we obtain dev_split dataset, which stores 4 data frames, each only with countries in the respective quantile of expected years of schooling. dev_split &lt;- split(dev, quartile(dev$eys)) head(dev_split[[1]]) country eys gni lexp mys france 1 Afghanistan 10.1 1746 64.5 3.9 0 14 Bangladesh 11.2 4057 72.3 6.1 0 27 Burkina Faso 8.9 1705 61.2 1.6 0 33 Central African Republic 7.6 777 52.8 4.3 0 34 Chad 7.5 1716 54.0 2.4 0 38 Comoros 11.2 2426 64.1 4.9 0 You can then look at descriptive statistics of each of the quartiles using: sapply(dev_split, summary) $`1` country eys gni lexp mys france Length:47 Min. : 5.000 Min. : 777 Min. :52.80 Min. :1.600 Min. :0 Class :character 1st Qu.: 8.700 1st Qu.: 1611 1st Qu.:60.80 1st Qu.:3.700 1st Qu.:0 Mode :character Median : 9.700 Median : 2318 Median :64.30 Median :4.850 Median :0 Mean : 9.415 Mean : 3579 Mean :63.89 Mean :4.861 Mean :0 3rd Qu.:10.550 3rd Qu.: 3731 3rd Qu.:67.00 3rd Qu.:6.075 3rd Qu.:0 Max. :11.200 Max. :17796 Max. :75.10 Max. :9.800 Max. :0 NA&#39;s :1 NA&#39;s :1 $`2` country eys gni lexp mys france Length:50 Min. :11.30 Min. : 660 Min. :58.90 Min. : 3.100 Min. :0 Class :character 1st Qu.:11.80 1st Qu.: 4232 1st Qu.:68.03 1st Qu.: 6.500 1st Qu.:0 Mode :character Median :12.30 Median : 6903 Median :71.50 Median : 7.850 Median :0 Mean :12.22 Mean : 10788 Mean :70.39 Mean : 7.869 Mean :0 3rd Qu.:12.70 3rd Qu.: 11578 3rd Qu.:73.83 3rd Qu.: 9.475 3rd Qu.:0 Max. :13.00 Max. :110489 Max. :80.10 Max. :11.600 Max. :0 NA&#39;s :2 NA&#39;s :2 $`3` country eys gni lexp mys france Length:47 Min. :13.10 Min. : 3317 Min. :63.90 Min. : 5.500 Min. :0 Class :character 1st Qu.:13.65 1st Qu.:10694 1st Qu.:74.53 1st Qu.: 8.600 1st Qu.:0 Mode :character Median :14.30 Median :14356 Median :76.05 Median : 9.900 Median :0 Mean :14.19 Mean :22644 Mean :75.45 Mean : 9.883 Mean :0 3rd Qu.:14.70 3rd Qu.:26054 3rd Qu.:76.88 3rd Qu.:11.200 3rd Qu.:0 Max. :15.10 Max. :99732 Max. :82.10 Max. :12.600 Max. :0 NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 $`4` country eys gni lexp mys Length:50 Min. :15.20 Min. : 9570 Min. :72.40 Min. : 7.70 Class :character 1st Qu.:15.68 1st Qu.:24906 1st Qu.:77.25 1st Qu.:10.43 Mode :character Median :16.35 Median :34918 Median :81.20 Median :12.25 Mean :16.79 Mean :35322 Mean :79.66 Mean :11.55 3rd Qu.:17.40 3rd Qu.:45698 3rd Qu.:82.38 3rd Qu.:12.70 Max. :22.10 Max. :83793 Max. :84.70 Max. :14.10 france Min. :0.00 1st Qu.:0.00 Median :0.00 Mean :0.02 3rd Qu.:0.00 Max. :1.00 While working an R and looking for help online, you may stumble upon other variants of the sapply() functions. Essentially, all R functions with apply in their name serve the same purpose - applying a function to each element of an object. lapply() is a less user friendy version of sapply(), which always returns a list, not a vector. vapply() forces the user to determine the type of the output, which makes its behaviour more predictible and slightly faster. tapply() applies the function to data frame by group determined by another variable - a similar procedure to what we did using split() and sapply(), but in less steps. 5.2 Summary -For loops allow to perform the same operation multiple times over a range of values of one variable. They are constructed using for(i in vector). Their use in R is relatively rare due to vectorization. -If statements control whether na operation is performed depending on the value of a logical condition. To conditionally modify values of vectors, use ifelse(test, yes, no) -Functions can be created by user to combine multiple operations into shorter pieces of code, which allows to avoid repetition. They take one or many arguments and return one value. -sapply() is a function used for applying a function to each element of a vector or each column of a data frame. You may find other versions of it, with apply in their name, which perform the same task, but with slight alteration. Functions list function package description count_na() .GlobalEnv NA quartile() .GlobalEnv NA summary_stats() .GlobalEnv NA names() c(“.GlobalEnv”, “base”) retrieve names of a list/vector as.factor() base coerce a vector to factor c() base Combine values/vectors into a vector ifelse() base return a or b depending on the value of test is.character() base check if vector is character is.factor() base check if a vector is of class ‘factor’ is.na() base check if a value is NA/elements of vector are NA is.numeric() base check if vector is numeric length() base get number of elements in a vector or list levels() base get levels of a factor list() base create a list mean() base get mean of a vector nchar() base get number of characters in a string nrow() base get number of rows of a data frame numeric() base initialize a numeric vector print() base print object to the console rowSums() base get sums of a data frame by rows sapply() base apply function to each element of a list split() base split list based on a function/vector sum() base get sum of numeric values or a vector IQR() stats obtain the inter-quartile range of a vector quantile() stats obtain empirical quantiles of a vector sd() stats Get standard deviation of a vector head() utils show first 5 rows of a data frame head() utils print first n (default 5) rows of the data read.csv() utils read a csv file to data frame. Specify stringsAsFactors = FALSE to keep all string columns as characters 5.3 Exercises Suppose we pass a data frame to the summary_stats function. What would the function return? Why? Use the summary_stats function to summarize each variable from the iris dataset. You can load it using data(iris). Use a for loop to create a scatter plot of Sepal.Width and Sepal.length attributes from the iris dataset with each flower species (specified by the Species variable) having a different marker and color. To do that, use the skeleton code below. data(iris) plot(-99, -99, xlim = c(min(iris$Sepal.Width), max(iris$Sepal.Width)), ylim = c(min(iris$Sepal.Length), max(iris$Petal.Length))) for (...) { points(x = , y = , col = , pch = ) } Create function called detect_outliers that will take a vector and a quantile threshold as an argument and will return the indices of the values that can be considered outliers given this threshold (i.e. lie above the nth quantile or below 100 - nth quantile). Extend this function so that it returns rows of the data frame that contain outliers in any of the (numerical) variables. Apply the function from the above exercise to the dev dataset. Which countries can be considered outliers? Recall the quartile function from the examples above. Can you extend it so that: quartile &lt;- function(x) { quantiles &lt;- quantile(x, c(0.25, 0.5, 0.75), na.rm = TRUE) comparisons &lt;- sapply(quantiles, function(y) y &lt;= x) rowSums(comparisons) + 1 } it splits a variable into an arbitary number of ranges with equal proportions (for example into deciles). it returns a sensible default when a value of the vector is missing. What could such “sensible default” be? Make it the default value when specifying function arguments. Try applying your new function to the dev dataset and splitting it into parts using split. You may then compare the descriptive statistics of each part using the lapply function. The solutions for the exercises will be available here on 2021-01-07. Chapter solutions "],["dplyr.html", "6 Data Manipulation 6.1 Content 6.2 Summary 6.3 Exercises", " 6 Data Manipulation Chapter script Chapter markdown Chapter exercises Chapter Data - Math Chapter Data - GDP Chapter Data - Population Density 6.1 Content While we have already discussed some methods for data manipulation, such as indexing, subsetting and modifying data frames, the majority of R users approach this task using a dedicated collection of packages called tidyverse, introduced by Hadley Wickham, a statistician from New Zealand. While it may seem like this chapter covers tools for performing task that you are already familiar with, tidyverse follows a different philosophy than traditional R, and has a lot of advantages including better code readability and efficiency. As before, to install and load the package, simply run the code below. Remember, that you only need to call install.packages once, to download all the required files from CRAN. library needs to be called every time you start a new R session to attach the library functions to your working environment. install.packages(&quot;tidyverse&quot;) library(tidyverse) Reading the data In this chapter, we will use the same dataset we’ve used in the exploratory analysis chapter, which presents individual-level information about a sample of students. However, tidyverse offers an improved set of functions for reading in the data, as part of the readr subpackage - they work fairly similar to read.csv introduced before, however have some advantages (for example they read character columns to character vectors, rather than factors without having to include the stringsAsFactors argument, which was discussed in chapter 3. All the readr reading functions start with read_ and are used for different file types. The most usual one is read_csv (which you can use in exactly the same way as read.csv), however in this case we use read_delim, which allows us to read file with any delimiter. In case of the math dataset each row is separated by a semicolon (which you can check by opening the file via a notebook app). Because of that we specify for the second delim argument as in the example below: math &lt;- read_delim(&quot;data/student/student-mat-data.csv&quot;, delim = &quot;;&quot;) Rows: 395 Columns: 33 ── Column specification ────────────────────────────────────────────────────────────────────────── Delimiter: &quot;;&quot; chr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardian, schoolsup, fam... dbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime, goout, Dalc, Wal... ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. As you can see, running the function returns a message, which shows the specification of each column that was read - col_character refers to character columns, while col_double() means numeric columns. You could force each column to be read as a specific type. For example, you may want a character column to be read as factors in some of the cases - for example sex. As mentioned above, the default setting of read_ functions is to read all numbers as numeric variables and all text as character variables. class(math$sex) [1] &quot;character&quot; To read the sex column as factor, we can read the data again, this time specifying the col_types argument. The col_types argument takes a list as input, in which we specify the type of selected columns, for example sex = col_factor() to tell: math &lt;- read_delim(&quot;data/student/student-mat-data.csv&quot;, delim = &quot;;&quot;, col_types = list(sex = col_factor(levels = c(&quot;M&quot;,&quot;F&quot;)))) class(math$sex) [1] &quot;factor&quot; You may wonder why not simply use the as.factor or factor function: math$sex &lt;- factor(math$sex, levels = c(&quot;M&quot;,&quot;F&quot;)) This is exactly equivalent, however the previous way of doing this is much more explicit and concise. Anyone (including you) who reads your analysis will immedietaly know which columns have you specified to be which type. The final facet of the readr package functions is that they load the data as a slightly different data type than the normal read.csv math &lt;- read.csv(&quot;data/student/student-mat-data.csv&quot;, sep = &quot;;&quot;) class(math) [1] &quot;data.frame&quot; math &lt;- read_delim(&quot;data/student/student-mat-data.csv&quot;, delim = &quot;;&quot;, col_types = list(sex = col_factor(levels = c(&quot;M&quot;,&quot;F&quot;)))) class(math) [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; is_tibble(math) [1] TRUE is.data.frame(math) [1] TRUE The data comes loaded as a tibble (tbl for short). Tibbles are a special kind of data frames implemented by the tidyverse package - the only thing you need to know about them for know, is that everything that you learned about data frames so far applies to tibbles. They offer some improvements, which will not be discussed here. The pipe operator Perhaps the most imporant innovation offered by the tidyverse package is the so-called pipe operator %&gt;%. Its use may feel a bit quirky at first, but it is extremely useful and is widely used by most of modern R users. As you have learned so-far, R evaluates each function from inside out. For example, we can get the sum of missing values in a data frame by running: sum(is.na(math)) [1] 10 This essentially performs two steps - first, runs is.na on the math data frame, which returns a table filled with logical values, FALSE when a given entry is not missing and TRUE when it is. Then sum takes this table as input and adds up the values in it (treating FALSE as 0 and TRUE as 1). In many cases, such statements can get long, difficult to read and error-prone, especially when keyword arguments are specified. The same operation may be be done using the pipe operator. In this case, rather than evaluating the sequence of functions from within, they are evaluated left to right. The %&gt;% operator can be understood as a way of _passing the output of the thing on the left to the thing on the right as the first argument: math %&gt;% is.na() %&gt;% sum() [1] 10 In this example, the math data frame is passed to the is.na function, and then the output is passed to the sum function, which returns exactly the same result. As in the case of the regular call, you may store the output in a variable: number_missing &lt;- math %&gt;% is.na() %&gt;% sum() number_missing [1] 10 Before continuing we drop the missing observations: math &lt;- math[complete.cases(math), ] While this may feel slightly unintuitive in this case, it comes in very handy when performing long sequences of operations on data, as we will see in the following sections. Dataset manipulation Three key functions are most commonly used for dataset manipulation in the tidyverse package: mutate, select and filter, coming from the dplyr sub-package. They are used as follows: - mutate is used to modify and create columns in data frames - select is used to select columns by name - filter is used to select rows given a set of logical values All three functions take the data frame as the first argument. For example, we can create a new column, grade_average by adding together grades for all three years and dividing them by 3: math &lt;- mutate(math, average = (G1 + G2 + G3)/3) Again, this is equivalent to: math$average &lt;- (math$G1 + math$G2 + math$G3)/3 As both operations create a variable called average by adding G1, G2 and G3 toegether, and dividing them by three. Not however, that in case of mutate there’s no need to specify the $ opearator, as you pass the math data frame as the first arguments, so the function knows that the G1, G2 and G3 names refer to this particular data frame. The mutate function returns the same data frame, but with the additional column average. Most commonly, it is used with the pipe operator: math &lt;- math %&gt;% mutate(average = (G1 + G2 + G3)/3) In this case, through the %&gt;% we pass the math data frame to mutate, which creates a new column and returns the updated data.frame, which is stored in math. filter allows you to filter rows of the data frame, which is an operation similar to indexing. For example, we can get students with average grade higher than 10 by: math %&gt;% filter(average &gt; 18) # A tibble: 6 × 34 school sex age address famsize Pstatus Medu Fedu Mjob Fjob reason guardian traveltime &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 GP M 15 U GT3 T 4 4 services teac… course father 1 2 GP M 16 U GT3 T 4 3 health serv… reput… mother 1 3 GP M 15 U LE3 A 4 4 teacher teac… course mother 1 4 GP M 15 U LE3 T 4 2 teacher other course mother 1 5 GP F 18 U GT3 T 2 2 at_home at_h… other mother 1 6 MS F 18 R LE3 T 4 4 other other reput… mother 2 # ℹ 21 more variables: studytime &lt;dbl&gt;, failures &lt;dbl&gt;, schoolsup &lt;chr&gt;, famsup &lt;chr&gt;, # paid &lt;chr&gt;, activities &lt;chr&gt;, nursery &lt;chr&gt;, higher &lt;chr&gt;, internet &lt;chr&gt;, romantic &lt;chr&gt;, # famrel &lt;dbl&gt;, freetime &lt;dbl&gt;, goout &lt;dbl&gt;, Dalc &lt;dbl&gt;, Walc &lt;dbl&gt;, health &lt;dbl&gt;, # absences &lt;dbl&gt;, G1 &lt;dbl&gt;, G2 &lt;dbl&gt;, G3 &lt;dbl&gt;, average &lt;dbl&gt; This entire operation could be done in one step, by: math %&gt;% mutate(average = (G1 + G2 + G3)/3) %&gt;% filter(average &gt; 18) # A tibble: 6 × 34 school sex age address famsize Pstatus Medu Fedu Mjob Fjob reason guardian traveltime &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 GP M 15 U GT3 T 4 4 services teac… course father 1 2 GP M 16 U GT3 T 4 3 health serv… reput… mother 1 3 GP M 15 U LE3 A 4 4 teacher teac… course mother 1 4 GP M 15 U LE3 T 4 2 teacher other course mother 1 5 GP F 18 U GT3 T 2 2 at_home at_h… other mother 1 6 MS F 18 R LE3 T 4 4 other other reput… mother 2 # ℹ 21 more variables: studytime &lt;dbl&gt;, failures &lt;dbl&gt;, schoolsup &lt;chr&gt;, famsup &lt;chr&gt;, # paid &lt;chr&gt;, activities &lt;chr&gt;, nursery &lt;chr&gt;, higher &lt;chr&gt;, internet &lt;chr&gt;, romantic &lt;chr&gt;, # famrel &lt;dbl&gt;, freetime &lt;dbl&gt;, goout &lt;dbl&gt;, Dalc &lt;dbl&gt;, Walc &lt;dbl&gt;, health &lt;dbl&gt;, # absences &lt;dbl&gt;, G1 &lt;dbl&gt;, G2 &lt;dbl&gt;, G3 &lt;dbl&gt;, average &lt;dbl&gt; Note that this time we haven’t modified the data frame - the variable average is created only temporarily, so that we can use it as a filter. This query shows a bit too many columns. Suppose we wanted to narrow our search and only see the guardian of the students with average higher than 18. We could use the select function, which simply selects the column of the data frame by name: math %&gt;% mutate(average = (G1 + G2 + G3)/3) %&gt;% filter(average &gt; 18) %&gt;% select(guardian) # A tibble: 6 × 1 guardian &lt;chr&gt; 1 father 2 mother 3 mother 4 mother 5 mother 6 mother This leaves us with a data frame of one column, showing guardians of the students with the best marks. Reshaping data It’s not common for social scientific to be longitudinal in nature. This means, that data in a given unit of observation (for example country, household or an individual) is observed on multiple variable (for example GDP, income, well-being) over a period of time. Such data can come in two formats - long and wide. Wide data format - in the wide data format, each column represents a variable - for example, the table presented below presents grades of three students over three academic years in the wide format. Each column represents a separate year. student 1 2 3 Thomas 2.34 3.87 2.03 Mary 3.87 4.58 2.70 David 3.83 3.92 4.00 Long data format - in the long data format, a separate column represents the name of the variable and a separate one - value of the corresponding variable. This format is sometimes more useful for particular types of analysis such as panel data models and for visualization. You can see the student scores in the long format below: student year grade Thomas 1 2.34 Thomas 2 3.87 Thomas 3 2.03 Mary 1 3.87 Mary 2 4.58 Mary 3 2.70 David 1 3.83 David 2 3.92 David 3 4.00 We can see an example of such data by loading the dataset gdp.csv, which contains GDP per capita over several years for couple of European countries: gdp &lt;- read_csv(&quot;data/world_bank/gdp.csv&quot;) head(gdp) # A tibble: 6 × 23 country ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010` &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany DEU 27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250. 37802. 36851. 38979. 2 Denmark DNK 28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966. 41278. 40370. 43032. 3 Spain ESP 21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436. 33263. 32123. 31704. 4 France FRA 26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086. 35095. 34711. 35927. 5 United Kingd… GBR 26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597. 36660. 35030. 36368. 6 Greece GRC 19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290. 30856. 30388. 28169. # ℹ 10 more variables: `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, # `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, X65 &lt;lgl&gt; We can see that the data contains country names, as well as GDP values in years between 2000 and 2015 in the wide format. To reshape the data into the long format, we can use the pivot_longer function, which comes with the tidyr package, another element of the tidyverse suite. In pivot longer, we specify the dataset name as the first argument (which is usually piped to the function), followed by the column names that contain the wide-format variables (assuming that they are in order, this can be specified with a names of the left-most and right-most variable, separated by a colon). Note that in this example, we also use the inverse quotation marks, since the variable are named using numbers. The names_to argument specifies tha name of the variable which will be used to store the names of the re-formatted variables (in our example - years) and the value_to argument specifies the name of the variable which will be used to store values (GDP per capita). gdp_long &lt;- gdp %&gt;% pivot_longer(`2000`:`2019`, names_to = &quot;year&quot;, values_to = &quot;gdp_pc&quot;) head(gdp_long) # A tibble: 6 × 5 country ccode X65 year gdp_pc &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Germany DEU NA 2000 27209. 2 Germany DEU NA 2001 28381. 3 Germany DEU NA 2002 29179. 4 Germany DEU NA 2003 29875. 5 Germany DEU NA 2004 31305. 6 Germany DEU NA 2005 31794. As you can see, the function produces data in a long format, with only 4 columns, but 140 rows, as opposed to the wide data which consists of only 7 rows, but 22 columns. In some cases, your data might come in a long format, yet you might want to reshape it into long. This can be done using the pivot_wider function. This works exactly opposite to pivot_longer. We first specify the data by piping it to the function and then use the names_from argument to specify the name of the variable containing the variable names and value_from to specify the variable containing the values. We end up obtaining the same data frame that we started with. gdp_wide &lt;- gdp_long %&gt;% pivot_wider(names_from = &quot;year&quot;, values_from = &quot;gdp_pc&quot;) head(gdp_wide) # A tibble: 6 × 23 country ccode X65 `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010` &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany DEU NA 27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250. 37802. 36851. 38979. 2 Denmark DNK NA 28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966. 41278. 40370. 43032. 3 Spain ESP NA 21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436. 33263. 32123. 31704. 4 France FRA NA 26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086. 35095. 34711. 35927. 5 United… GBR NA 26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597. 36660. 35030. 36368. 6 Greece GRC NA 19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290. 30856. 30388. 28169. # ℹ 9 more variables: `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, # `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt; all.equal(gdp_wide, gdp) [1] &quot;Names: 21 string mismatches&quot; [2] &quot;Attributes: &lt; Names: 1 string mismatch &gt;&quot; [3] &quot;Attributes: &lt; Length mismatch: comparison on first 2 components &gt;&quot; [4] &quot;Attributes: &lt; Component \\&quot;class\\&quot;: Lengths (3, 4) differ (string compare on first 3) &gt;&quot; [5] &quot;Attributes: &lt; Component \\&quot;class\\&quot;: 3 string mismatches &gt;&quot; [6] &quot;Attributes: &lt; Component 2: Modes: numeric, externalptr &gt;&quot; [7] &quot;Attributes: &lt; Component 2: Lengths: 7, 1 &gt;&quot; [8] &quot;Attributes: &lt; Component 2: target is numeric, current is externalptr &gt;&quot; [9] &quot;Component 3: Modes: logical, numeric&quot; [10] &quot;Component 3: target is logical, current is numeric&quot; [11] &quot;Component 4: Mean relative difference: 0.04964384&quot; [12] &quot;Component 5: Mean relative difference: 0.04797772&quot; [13] &quot;Component 6: Mean relative difference: 0.02722602&quot; [14] &quot;Component 7: Mean relative difference: 0.05492863&quot; [15] &quot;Component 8: Mean relative difference: 0.03197982&quot; [16] &quot;Component 9: Mean relative difference: 0.08535064&quot; [17] &quot;Component 10: Mean relative difference: 0.04919092&quot; [18] &quot;Component 11: Mean relative difference: 0.04411082&quot; [19] &quot;Component 12: Mean relative difference: 0.02757141&quot; [20] &quot;Component 13: Mean relative difference: 0.05151733&quot; [21] &quot;Component 14: Mean relative difference: 0.04769779&quot; [22] &quot;Component 15: Mean relative difference: 0.01895478&quot; [23] &quot;Component 16: Mean relative difference: 0.03867191&quot; [24] &quot;Component 17: Mean relative difference: 0.03078083&quot; [25] &quot;Component 18: Mean relative difference: 0.02475497&quot; [26] &quot;Component 19: Mean relative difference: 0.05288349&quot; [27] &quot;Component 20: Mean relative difference: 0.05189977&quot; [28] &quot;Component 21: Mean relative difference: 0.03430557&quot; [29] &quot;Component 22: Mean relative difference: 0.04530826&quot; [30] &quot;Component 23: Modes: numeric, logical&quot; [31] &quot;Component 23: target is numeric, current is logical&quot; Joining The final data manipulation technique that we will discuss in this chapter is joining. In many cases we will have the dataset coming in two or more separate file, each containing different variables for the same unit for observations. This is the case with all the data coming from World Bank Open Data, where information about each indicator comes in a separate csv file. For example, suppose we have data on GDP and population density of some countries: gdp &lt;- read_csv(&quot;data/world_bank/gdp.csv&quot;) pop &lt;- read_csv(&quot;data/world_bank/pop_dens.csv&quot;) head(gdp) # A tibble: 6 × 23 country ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010` &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany DEU 27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250. 37802. 36851. 38979. 2 Denmark DNK 28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966. 41278. 40370. 43032. 3 Spain ESP 21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436. 33263. 32123. 31704. 4 France FRA 26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086. 35095. 34711. 35927. 5 United Kingd… GBR 26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597. 36660. 35030. 36368. 6 Greece GRC 19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290. 30856. 30388. 28169. # ℹ 10 more variables: `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, # `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, X65 &lt;lgl&gt; head(pop) # A tibble: 6 × 23 country ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010` &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 France FRA 111. 112. 113. 114. 115. 115. 116. 117. 118. 118. 119. 2 United Kingd… GBR 243. 244. 245. 247. 248. 250. 252. 253. 255. 257. 259. 3 Ireland IRL 55.2 56.1 57.1 58.0 59.1 60.4 62.0 63.9 65.2 65.8 66.2 4 Italy ITA 194. 194. 194. 195. 196. 197. 198. 199. 200. 201. 202. 5 Poland POL 125. 125. 125. 125. 125. 125. 125. 124. 124. 125. 124. 6 Portugal PRT 112. 113. 114. 114. 115. 115. 115. 115. 115. 115. 115. # ℹ 10 more variables: `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, # `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;lgl&gt;, X65 &lt;lgl&gt; Suppose we want to analyze the relationship between population density and gdp per capita. To do that, it would be convenient to merge these two datasets into one, containing the variables gdp and pop_dens. We can achieve this by using joins. First, we pivot the data into the long format: gdp_long &lt;- gdp %&gt;% pivot_longer(`2000`:`2019`, names_to = &quot;year&quot;, values_to = &quot;gdp_pc&quot;) pop_long &lt;- pop %&gt;% pivot_longer(`2000`:`2019`, names_to = &quot;year&quot;, values_to = &quot;pop_dens&quot;) Note that the countries in the two datasets are different: unique(gdp_long$country) [1] &quot;Germany&quot; &quot;Denmark&quot; &quot;Spain&quot; &quot;France&quot; &quot;United Kingdom&quot; [6] &quot;Greece&quot; &quot;Poland&quot; unique(pop_long$country) [1] &quot;France&quot; &quot;United Kingdom&quot; &quot;Ireland&quot; &quot;Italy&quot; &quot;Poland&quot; [6] &quot;Portugal&quot; identical(unique(gdp_long$country), unique(pop_long$country)) [1] FALSE To join two data frames, we need an ID variable (or a set of variables) that will identify observations and allow us to join them. In the example, the country and year variables are a perfect candidate, since each corresponds to one observation from a given country in a given period. We would then say that we join the two datasets on country and year. s There are three fundamental ways in which we can approach this: Inner join is used to join only the observations where the variables we are joining on which appear in both datasets. The rows where the identifying variables don’t match any observations in the other dataset are dropped from the resulting dataset. This join is used when we care primarily about the completeness of our data. The order of the dataframes does not matter when performing inner join. dat &lt;- inner_join(gdp_long, pop_long, by = c(&quot;country&quot;, &quot;ccode&quot;, &quot;year&quot;)) head(dat) # A tibble: 6 × 7 country ccode X65.x year gdp_pc X65.y pop_dens &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; 1 France FRA NA 2000 26100. NA 111. 2 France FRA NA 2001 27502. NA 112. 3 France FRA NA 2002 28524. NA 113. 4 France FRA NA 2003 28142. NA 114. 5 France FRA NA 2004 29034. NA 115. 6 France FRA NA 2005 30499. NA 115. We can see that the new dataframe dat contains both gdp_pc and pop_dens variables. Furthermore, only the countries present in both datasets were kept: unique(dat$country) [1] &quot;France&quot; &quot;United Kingdom&quot; &quot;Poland&quot; Left join is used to join only the observations where the variables we are joining appear in the first dataset (the one on the left of the joining function). This is done primarily when we care about keeping all the observations from the first (left) dataset. The observations where no corresponding identifying values were found are turned into missing values: dat &lt;- left_join(gdp_long, pop_long, by = c(&quot;country&quot;, &quot;ccode&quot;, &quot;year&quot;)) head(dat) # A tibble: 6 × 7 country ccode X65.x year gdp_pc X65.y pop_dens &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; 1 Germany DEU NA 2000 27209. NA NA 2 Germany DEU NA 2001 28381. NA NA 3 Germany DEU NA 2002 29179. NA NA 4 Germany DEU NA 2003 29875. NA NA 5 Germany DEU NA 2004 31305. NA NA 6 Germany DEU NA 2005 31794. NA NA As we can see in this example, the resulting dataset contains missing values for the countries that were not present in the pop_long dataset. All the countries from the gdp_long dataset were kept: all.equal(unique(gdp_long$country), unique(dat$country)) [1] TRUE Full join - joining observations from both data frames and producing missing values whenever there’s an observation missing in one of them. dat &lt;- full_join(gdp_long, pop_long, by = c(&quot;country&quot;, &quot;ccode&quot;, &quot;year&quot;)) head(dat) # A tibble: 6 × 7 country ccode X65.x year gdp_pc X65.y pop_dens &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; 1 Germany DEU NA 2000 27209. NA NA 2 Germany DEU NA 2001 28381. NA NA 3 Germany DEU NA 2002 29179. NA NA 4 Germany DEU NA 2003 29875. NA NA 5 Germany DEU NA 2004 31305. NA NA 6 Germany DEU NA 2005 31794. NA NA As a result of a full_join, all countries including in either of the datasets are kept: unique(c(dat$country)) [1] &quot;Germany&quot; &quot;Denmark&quot; &quot;Spain&quot; &quot;France&quot; &quot;United Kingdom&quot; [6] &quot;Greece&quot; &quot;Poland&quot; &quot;Ireland&quot; &quot;Italy&quot; &quot;Portugal&quot; unique(c(gdp_long$country, pop_long$country)) [1] &quot;Germany&quot; &quot;Denmark&quot; &quot;Spain&quot; &quot;France&quot; &quot;United Kingdom&quot; [6] &quot;Greece&quot; &quot;Poland&quot; &quot;Ireland&quot; &quot;Italy&quot; &quot;Portugal&quot; There are some other joining techniques, such as Filtering joins (semi_join and anti_join), as well as nest_join. You can read more about these in the documentation by typing ?join into the console. Aggregating data While we have discussed summary statistics that can be used to summarize data, it’s often very useful to compare their values across group, rather than only look at one number to describe an entire dataset. The tidyverse allows us to calculate summary statistics of variables through the summarise function. For example, to get the average GDP of countries in our data: gdp_long %&gt;% summarise(gdp_avg = mean(gdp_pc)) # A tibble: 1 × 1 gdp_avg &lt;dbl&gt; 1 33486. This is no different from using gdp_long$gdp_pc %&gt;% mean(), other than it returns a tibble rather than a scalar value. However, the summarize function is most powerful in conjunction with the group_by function. As the name suggests, group_by function divides the data frame into groups using one of the variables. On the surface, it doesn’t appear to alter much: gdp_long_groupped &lt;- gdp_long %&gt;% group_by(country) gdp_long_groupped # A tibble: 140 × 5 # Groups: country [7] country ccode X65 year gdp_pc &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; 1 Germany DEU NA 2000 27209. 2 Germany DEU NA 2001 28381. 3 Germany DEU NA 2002 29179. 4 Germany DEU NA 2003 29875. 5 Germany DEU NA 2004 31305. 6 Germany DEU NA 2005 31794. 7 Germany DEU NA 2006 34119. 8 Germany DEU NA 2007 36250. 9 Germany DEU NA 2008 37802. 10 Germany DEU NA 2009 36851. # ℹ 130 more rows pop_long %&gt;% group_by(country) %&gt;% summarise(avg_pop = mean(pop_dens, na.rm = TRUE), sd_pop = sd(pop_dens, na.rm = TRUE)) # A tibble: 6 × 3 country avg_pop sd_pop &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 France 118. 3.60 2 Ireland 63.9 4.89 3 Italy 200. 4.67 4 Poland 124. 0.301 5 Portugal 114. 1.13 6 United Kingdom 258. 10.3 Similarily, we could compare the country’s average, maximum and minimum GDP growth over the past years: gdp_long %&gt;% group_by(country) %&gt;% mutate(gdp_growth = (gdp_pc - lag(gdp_pc)) / lag(gdp_pc)) %&gt;% summarise(avg_growth = mean(gdp_growth, na.rm = TRUE), max_growth = max(gdp_growth, na.rm = TRUE), min_growth = min(gdp_growth, na.rm = TRUE)) %&gt;% arrange(-avg_growth) # A tibble: 7 × 4 country avg_growth max_growth min_growth &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Poland 0.0636 0.109 0.0361 2 Denmark 0.0398 0.0919 -0.0220 3 Germany 0.0391 0.0914 -0.0252 4 Spain 0.0364 0.111 -0.0343 5 France 0.0344 0.0633 -0.0134 6 United Kingdom 0.0330 0.0641 -0.0445 7 Greece 0.0264 0.115 -0.0730 In this case, we first group the data frame by country. We then use mutate to compute gdp_growth by subtracting GDP from one year before (calculated using the lag function) and dividing this difference by the lagged GDP. Note that the mutate function also applies the computation according to the grouping defined in the group_by - the lag() is computed within each country. We then use summarise to apply the functions. Finally, we use arrange to sort the output according to the negative value of the average GDP growth, i.e. in decreasing order. Since the behaviour of group_by affects many operations performed on a dataframe, it is important to call ungroup() at the end of our operations if we assign the data frame to a new name - performing operations on groupped tibbles can lead to suprising results. Coming back to our example, suppose we wanted to obtain the deviation of each country’s GDP growth from the global mean of this growth. First, we would obtain the growths: gdp_growths &lt;- gdp_long %&gt;% group_by(country) %&gt;% mutate(gdp_growth = (gdp_pc - lag(gdp_pc)) / lag(gdp_pc)) #store demeaned values: dem &lt;- gdp_growths %&gt;% mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE)) #stored demeaned values calculated after ungrouping dem_ungr &lt;- gdp_growths %&gt;% ungroup() %&gt;% mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE)) all.equal(dem$growth_dem, dem_ungr$growth_dem) [1] &quot;Mean relative difference: 0.3395083&quot; We can see using the all.equal function, that the resulting variables are different. This is because, if the tibble is still groupped, the mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE)) expression subtracts the group mean from each observation, wheres if it’s ungrouped, it calculates the global mean and subtracts it. While it may seem trivial in this example, forgetting to ungroup a tibble is a common error and it is crucial to remeber to ungroup the tibble after finishing performing operations on it. Also note that calling group_by() on an already groupped tibble discards the previous groupping and applies the new one instead. 6.2 Summary The tidyverse package is a set of tools for data manipulation commonly used in R. readr offers a convenient inferface for reading data into R and explicitly specifying column classes through functions such as read_csv, read_delim or read_tsv. a tibble is a special kind of data frame implemented in the tidyverse package. While it offers some new functionalities, working with tibbles is very similar to working with traditional R data frames. the pipe operator %&gt;% passess values into functions - instead of evaluating expressions from inside-out, they are evaluated from left to right. select is used to select columns from a data frame based on their names filter is used to filter rows from a data frame based on a logical statement mutate is used to modify the values of existing columns or to create new columns in a data frame Data can often come in a long or wide format. You can use pivot_longer and pivot_wider to switch between these data formats. You can merge two data frames by a common column (or a set of columns) using dplyr’s join functions. Depending on the way you want to treat non-matching records, you may use a left_join, inner_join or a full_join You can aggregate data using the summary function. It works best with group_by, which allows you to get statistics by group, following a group defined by a variable or a set of variables. You should call ungroup() on the tibble, to avoid errors. Functions list function package description filter() c(“dplyr”, “package:stats”) filter out rows of a data frame according to logical vector lag() c(“dplyr”, “package:stats”) lag observations by n periods all.equal() base NA c() base Combine values/vectors into a vector class() base retrieve class of an R object (e.g. ‘numeric’, ‘list’, ‘data.frame’) factor() base create a factor identical() base check if two R objects are identical is.data.frame() base check if an object is a data frame is.na() base check if a value is NA/elements of vector are NA list() base create a list max() base get maximum of a vector mean() base get mean of a vector min() base get minimum of a vector sum() base get sum of numeric values or a vector unique() base get unique elements arrange() dplyr sort values of data frame according to a variable/combination of varaibles full_join() dplyr join matching all observations from both data frames group_by() dplyr group tibble/data.frame by a factor variable. All further tidyverse operations are performed group-wise inner_join() dplyr join matching only observations with joining variable in both data frames left_join() dplyr join matching all observations from the left data frame mutate() dplyr modify/create a column in a data frame select() dplyr select columns from a tibble/data frame summarise() dplyr collapse the dataset to a summary statistic. Usually used with group_by() ungroup() dplyr NA col_factor() readr specify column type as factor. Used in readr’s functions. read_csv() readr read tibble from a csv file read_delim() readr read tibble from a delimited file complete.cases() stats retrieve indices of complete observations sd() stats Get standard deviation of a vector is_tibble() tibble NA pivot_longer() tidyr reshape data wide → long pivot_wider() tidyr reshape data long → wide head() utils show first 5 rows of a data frame head() utils print first n (default 5) rows of the data read.csv() utils read a csv file to data frame. Specify stringsAsFactors = FALSE to keep all string columns as characters 6.3 Exercises Select true statements about joins and correct the false statements: A. Full join will always produce a dataframe with more rows than inner join. B. Inner join will never produce any new missing values. C. Full join will always produce at least as many rows as left join. D. Left join is used when we want to ensure that the observations from the left data frame are always matched by a non-missing element from the right data frame. The remaining exercises will use the student data, which consists of three .csv files: math.csv, which contains grades achieved by each student in each of the years in a long format, port.csv, with the same data for portugeese classes and info.csv, which contains demographic variables. Load the math and port data to R. Specify the id variable to be a character column. Load the info data into R and join it with the grades datasets. Make sure to keep only the records of the students who have reported grades in both portugeese and math. Use the suffix argument of the join function to specify the names of the duplicated variable names (G1, G2 and G3) to be _por for portugeese grades and _mat for math grades. Create a vector called port_missing which contains student IDs that are present in the math dataset but missing from the port dataset. Compare the average performance in Portugeese and Math in third year between the schools: Did the student grades improve on average in each subject? Was the improvement better in one school than the other? (HINT: you might want your data in a long format and use the lag function used at the end of the chapter. Store the data in a new tibble called students_imp, as it will be useful for next exercises). Did the grade improvements vary across schools? Which form of support - famsup or schoolsup lead to higher average improvement in students’ grades in each subject? Does any of the two schools seem more effective in rising the average grade through school support? The solutions for the exercises will be available here on 2021-01-07. Chapter solutions "],["visualization.html", "7 Data Visualization 7.1 Content 7.2 Summary 7.3 Exercises", " 7 Data Visualization Chapter script Chapter markdown 7.1 Content The final chapter in this part focuses on data visualization. While some techniques for presenting data were already described in the chapter on Exploratory data analysis, this chapter will cover the topic in more depth and introduce you to the most commonly used tool for data visualization in R - the ggplot2 package. The main advantage of ggplot2 in comparison with some of the out-of-the-box plotting capabilities coming with R is the simplicity of creating relatively complex and nice looking plot, which would often take much longer to produce with R base graphics. The ggplot2 package is a part of tidyverse, which was introduced in the previous chapter, and it is often useful in conjunction with some of the data manipulation tools offered by other tidyverse packages. You can load the library directly: library(ggplot2) However you may also attach it along with other tidyverse packages: library(tidyverse) We will also load the salaries dataset, which describes the salaries of US professors. To do this, you will need to install the package using the carData package using the install.packages function: install.packages(&#39;carData&#39;) To load the data, simpy execute the below code. The dataframe will be attached to your global environment. data(&#39;Salaries&#39;, package = &#39;carData&#39;) glimpse(Salaries) Rows: 397 Columns: 6 $ rank &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof, Prof, Prof, Assoc… $ discipline &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A, A, A, A, A, A, A,… $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 12, 19, 38, 37, 39,… $ yrs.service &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3, 20, 34, 23, 36, 2… $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Female, Male, Male, … $ salary &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 147765, 119250, 1290… The data consists of 6 variables, describing academics rank, discipline (A for theoretical and B for applied), experience measured in years in service and years since PhD attainment, the subjects sex and salary. To see more details about the data, use ?carData::Salaries The plotting syntax Similarily to the other tidyverse tools explored in the previous chapter, ggplot uses its own, original approach to creating plots, which is quite different from what you have previously encouneterd in the exploratory data analysis lesson. To initialize a ggplot object, you always call the ggplot function, specifying the name of your data frame as the first argument. ggplot(Salaries) As you can see this initiates an empty plot. We then furhter specify the variables we want to associate with the object using the aes() function. The aes abbreviation stands for aesthetic mappings. It defines bindings between variables in the dataframe specified as the first argument of the ggplot object and aesthetic elements of the object - for example, the x and y axes of the plot, line/point colors and shapes or bar fills. For example, aes(x = absences) can be interpreted as “link the variable absences with the x-axis of this plot”. ggplot(Salaries, aes(x = salary)) Now the figure includes an x axis, with some automatically chosen tick locations. The final step is to create an actual plot. This is done by “adding” a so-called geom to the plot skeleton we have created so far. Geoms are simply functions used to create different plot types representing the data associated with a ggplot object through ggplot and aes - for example, geom_histogram is used to create a histogram and geom_boxplot to create a boxplot. Geoms are “added” to the ggplot object using the + operator. ggplot(Salaries, aes(x = salary)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can see now that the histogram of the variable appears on the plot. The plot also comes with a message prompting us to manually specify the bin width for the histogram, which can be done through the arguments of the geom function - either by specifying the number of bins through bins or the width of individual bins through binwidth. ggplot(Salaries, aes(x = salary)) + geom_histogram(bins = 40) ggplot(Salaries, aes(x = salary)) + geom_histogram(binwidth = 5000) We can also change to colors of the bins as well as their borders: ggplot(Salaries, aes(x = salary)) + geom_histogram(binwidth = 5000, fill = &#39;blue&#39;, color = &#39;black&#39;) As mentioned earlier, the aesthetic characteristics of the plot such as border color or fill can be treated as a representation of the data, not only the visual characteristics of the plot. For example, continuing with the absences example we may see the break ouf of the absences between male and female students, by specifying the fill argument in the aesthetic mapping: ggplot(Salaries, aes(x = salary, fill = sex)) + geom_histogram(binwidth = 5000, color = &#39;black&#39;) This also automatically adds a legend on the side of the plot. The blue color represents the proportion of each bin represented by male students and the red - by female students. An important thing to note is that the aesthetic mapping created by the aes() function can be defined globally i.e. for the entire ggplot figure, as in the case above (ggplot(Salaries, aes(x = salary))), as well as a locally, i.e. only for one specific geom (geom_histogram(aes(x = salary)). In the former case, the entire figure will use the set aesthetic mapping, while in the latter, it will apply only to the specific geom. ggplot(Salaries) + geom_histogram(aes(x = salary, fill = sex), binwidth = 5000,color = &#39;black&#39;) Note that that in this case, both global and local mappings produce exactly the same output. However, when adding multiple geom structures to one plot, as we will see later, using a global vs local aesthetic mapping can lead to very different results. All about geoms - plot types in ggplot2 In this section, we will explore some of the basic plot types (or geoms) offered by the ggplot2 package. The number of geoms available is vast, and this isn’t by any means an exhaustive discussion of the topic - you can find the full list in the ggplot2 cheatsheet. Even more can be found through external, ggplot-based packages. However, learning the basics of just couple of them should help you gain a sufficient understanding of ggplot’s plotting philosophy to be able to explore the rest by yourself and use them whenever you need to. Density estimate The density estimate is an alternative way to draw a histogram of the data. It represents the distribution of a continous variable by drawing a line. You can see that the shape of the density plot of absences below directly corresponds with the histogram of this variable from the previous example. ggplot(Salaries, aes(x = salary)) + geom_density() Kernel density estimates are particularly useful to compare the distribution of a variable between different values of a grouping variable. For example, we can examine the difference in the distributions of the salary depending on the position held by the individual: ggplot(Salaries, aes(x = salary, fill = rank)) + geom_density(alpha = 0.3) The differences are clear-cut - it appears that as expected, higher position held generally leads to higher median of earnings. However, each position also has higher variability in income, with the spread of Professors earnings more than twice as large as for Assistant Professors. Barplots Histograms and kernel density estimates are useful in representing continuous variables, such as temperature, GDP or hours of absence. In case of discrete variables, which only take a finite and unordered set of values, it’s often better to use bar plots. With ggplot2, these can be easily obtained using geom_bar. For example, we can use it to represent the of academic ranks within our dataset. ggplot(Salaries, aes(x = rank)) + geom_bar(width = 0.5, fill = &#39;forestgreen&#39;) You may also want to make the bar plots horizontal. There are two ways to to this: the old approach is simply to add coord_flip() to your ggplot object. You may still encounter this way of plotting horizontal bar plots in other users code. ggplot(Salaries, aes(x = rank)) + geom_bar(width = 0.5, fill = &#39;forestgreen&#39;) + coord_flip() However, since ggplot version 3.3.0, providing the y axis instead of x axis leads to the same result. Furthermore, it doesn’t cause unnecessary confusion caused by flipping the axis (for example, the y-axis label needs to be specified as the x-axis label when coord_flip() is applied). Thus it is recommended to use the second approach, provided that you have the most up-to-date version of tidyverse installed. ggplot(Salaries, aes(y = rank)) + geom_bar(width = 0.5, fill = &#39;forestgreen&#39;) Again, we can also fill the bars with another variable, simply by specifying the fill argument in the aesthetic mapping definition: ggplot(Salaries, aes(x = rank, fill = sex)) + geom_bar(width = 0.5) The bars can also be placed side-by-side to improve comparison. To do that, we simply specify a named argument position to the geom_bar function and set it to 'dodge', as seen in the example below. This means, that one bar should ‘dodge’ another, and be placed next to it. The default value of the position argument is 'stacked', which is pretty self-explanatory and can be seen in the previous example. ggplot(Salaries, aes(x = rank, fill = discipline)) + geom_bar(position = &#39;dodge&#39;) We can also use barplots to present summary statistics visually - for example, the average salary of a professor given his or her rank. To do that, we first construct a summary data frame, using tools covered in the previous chapter, and then plot the result. salary_by_rank &lt;- Salaries %&gt;% group_by(rank) %&gt;% summarise(avg_salary = mean(salary)) %&gt;% ungroup() ggplot(salary_by_rank, aes(x = rank, y = avg_salary)) + geom_bar(stat = &#39;identity&#39;, width = 0.5, fill = &#39;firebrick&#39;) Note that in this case we need to specify both x and y aesthetics, as x represents the name of the group (in this case the professors’ rank) and y - the associated value, indicated by the height of each bar. Moreover, we need to setthe stat argument of geom_bar to identity. This is because the default setting, stat = count implies that ggplot takes only one, discrete variable as arguments and counts the number of observations in each of the group defined by the levels of the variable - an operation very similar to the output of table(math$health). We can also use geom_col, which is a short-hand for geom_bar(stat = 'identity') and produces the same output: ggplot(salary_by_rank, aes(x = rank, y = avg_salary)) + geom_col(width = 0.5, fill = &#39;firebrick&#39;) Boxplots Another way to compare a variable across groups is through the use of boxplots. Boxplots were described in the chapter on exploratory data analysis as a way of evaluating the distribution of a continuous variable. They are also implemented in the ggplot2 package and can be used for comparisons of the continuous variables distributions across multiple groups defined by a discrete variable. This can be done by specifying the grouping variable to the x aesthetic and the continuous variable which we want to examine to the y aesthetic. ggplot(Salaries, aes(x = sex, y = salary)) + geom_boxplot(width = 0.5) We can see that while the medians of Male and Female professor’s wages in our sample are similar, the interquantile range is much higher for men. At the same time, the distribution of male wages is more spread out, as indicated by larger box, longer tails. It also includes some outliers. We can also draw an arbitrary summary statistic - such as the mean - on the plot, by using the stat_summary function - we specify the statistic we want to use with the fun keyword argument and the geom using the geom argument. In this case, we add a cross to indicate the mean salary for both sexes on the plot: ggplot(Salaries, aes(x = sex, y = salary)) + geom_boxplot(width = 0.5) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = 3, color = &quot;red&quot;) Violin plots Violin plots are closely related to box plots, however they depict the shape of the variables’ distribution. As such, they overcome the issue related to the use of boxplots decribed in the exploratory analysis chapter - namely that two distributions can have identical box plots, yet very different underlying shapes. The violin plots avoid it by connecting boxplots with kernel density estimates, giving a better approximation of the shape of a variable’s distribution. ggplot(Salaries, aes(x = sex, y = salary)) + geom_violin(width = 0.5) We can see the strength of that by coming back to the misleading example from the previous chapter. Suppose we have the The boxplots look very alike: ggplot(dt, aes(x = category, y = value)) + geom_boxplot() However, the violin plot give away the entire story - namely the fact that the first category has two peaks (i.e. comes from a bimodal distribution, using the proper statistical terms), and it is unlikely that we observe the median value. A good example of such distributions from the political realm are the self-identification on the left-right ideological scale, with majority of respondents reporting to be either slightly conservative or slightly liberal. ggplot(dt, aes(x = category, y = value)) + geom_violin() Scatter plots So far we have described multiple ways of visually representing a continous variable, as well as the relationship between a continous variable and some discrete grouping variables. Under many circumstances, we are interested in examining the relationship between two continuous variables. One of the most commonly employed data visualization techniques for problems like this are scatter plots, which simply describe each observation as a point marked on a two-dimensional graph, with x axis representing one variable and y axis - the other. This can be seen in the example below, in which we look at the relationship between the professor’s experience approximated by years since attaining PhD and his or her salary. ggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + geom_point() It is clear that we can see a some sort of relationship between the experience and earnings. In other words, there’s a degree of positive correlation between these two variables (we will examine this term more in depth in in the next chapter), i.e. professors with more experience tend to earn more on average. However, with experience the relationship becomes more noisy, as there’s more variation in earnings of the more experienced academics. As in the previous cases, we can specify additional aesthetics such as color or shape: ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank, shape = rank)) + geom_point() Line of best fit We can also add a line of best fit to our plot, by using the geom_smooth() geom, with the method argument specified to 'lm'. The line of best fit is simply the line that is closest to each of the points in the plot on average. The 'lm' methods stands for linear model - a method used for fitting such a line, described in more detail in the next part, in chapter on linear regression. ggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + geom_point() + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; Here, you should be able to fully understand the difference between local and global aesthetic mappings mentioned earlier in the chapter. Notice the difference between providing the aesthetic mapping at the global level (i.e. to the ggplot() function call)…: ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; … and just to the geom_point(). ggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + geom_point(aes(color = rank)) + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; In the former case, the color = sex argument is used by both geom_point and geom_line(), which leads to 3 lines of different colors being fitted for each of the groups separately. When the color aesthetic mapping is only provided to the geom_point at the local level, one line is fitted to the data, as specified by the global mapping. Both of these approaches may be useful, depending on our research question. The comparison of these two plots further reveals an interesting behaviour in our data: when all the observations are considered toegether, there seems to be a positive relationship between years since PhD and salary. However when we look at the group level, the relationship appears to disappear - this behaviour is known as the Simpson’s Paradox. In this case, it’s primarily the result of the relationship between the rank and years since PhD - professors with higher ranks have more experience. When the variation in the salaries within those groups is considered, the experience measured in years seems to hardly explain any of the variation in earnings. Line connecting points In some cases, we might want to connect points in our figure with a line to depict some sort of trend in the mean of a variable depending on a categorical group that has a natural ordering - for example, suppose we want to visually examining differences in the average earnings of professors in our dataset depending on their years in service. First we can obtain the average for each group using group_by and summarise. byage &lt;- Salaries %&gt;% mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;% group_by(yrs_range) %&gt;% summarise(avg = mean(salary, na.rm = TRUE)) %&gt;% ungroup() We can then plot the averages using geom_point: ggplot(byage, aes(x = yrs_range, y = avg)) + geom_point() While there seems to be some sort of trend in the data (even though the further points divert from it, which is likely the result of small sample size in a given group leading to rather unreliable sample estimates), the plot does not emphasize it enough. To make it more meanigful, we can add a line connecting the points with geom_line. Note that we have to set the group aesthetic to 1, which provides ggplot a hint on how to connect the point. ggplot(byage, aes(x = yrs_range, y = avg, group = 1)) + geom_point() + geom_line() We can extend the following example by comparing the same trend depending on the paid classes taken: byage &lt;- Salaries %&gt;% mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;% group_by(yrs_range, discipline) %&gt;% summarise(avg = mean(salary, na.rm = TRUE)) %&gt;% ungroup() `summarise()` has grouped output by &#39;yrs_range&#39;. You can override using the `.groups` argument. ggplot(byage, aes(x = yrs_range, y = avg, color = discipline, lty = discipline, group = discipline)) + geom_point() + geom_line() Multiple plots ggplot2 also makes it possible to put multiple plots in one figure - this is indeed very useful under many circumstances, and many data visualization professionals consider it a good practice, as it follows the rule of small multiples, making the data more digestible to the reader and avoiding informational overload in one plot. Arranging plots into a figure can be done in two ways - based on a value of a categorical variable (‘faceting’) and by arbitrarily putting plots into a rectangular grid. Faceting Faceting refers to simply splitting the data by the value of a categorical variable, similarily to the way group_by treats the data when performing regular manipulation. The syntax is relatively simple: we simply add another layer to our plot called facet_wrap and specify the faceting variable following the tilde symbol ~ - this is a commonly used notation in R, indicating some sort of dependence of one variable on another - you will encounter it in the next two chapters. ggplot(Salaries, aes(yrs.since.phd, salary)) + facet_wrap(~ rank) + geom_point() + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; As a result we can see the scatter plots and lines of best fit in three separate plots, each one with an appropriate heading indicating the group of the observations. However, the output is difficult to read, due to the fact that the x-axis in the plot is fixed to the same length for each of the plots - this improves comparability of the figures, but limits its clarity. This can be changed by specifying an additional argument to the facet_wrap layer: ggplot(Salaries, aes(yrs.since.phd, salary)) + facet_wrap(~ rank, scales = &#39;free_x&#39;) + geom_point(aes(color = sex)) + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; Again, we can see that the relationship seen in the overall dataset disappears within each of the groups. We can also facet using more than one variable and arrange the figures into a grid via a two dimensional grouping. For example, suppose we want to evaluate the distribution of the experience wihtin each combination of rank and discipline of the individuals. This can be achieved by using the facet_grid layer. ggplot(Salaries, aes(x = yrs.since.phd)) + facet_grid(discipline ~ rank) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In here, we can see that professors in theoretical disciplines seem to have worked longer on average, however the range of the values is comparable in both cases. There also appears to be more Assistant Professors and Associate Professors in the applied departments, with no siginificant differences in gender. Similar analysis applied to earnings, however, reveals that the earnings seem to be higher at each level in the applied departments. ggplot(Salaries, aes(x = salary)) + facet_grid(discipline ~ rank) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Plotting in a grid Sometimes, we want to make our figure arrangement more customized, not necessarily arranged in accordance with our variable. For example, we may want to depict a consince summary of the most important of our findings in one 2x2 plot. To do this, we can use the plot_grid function from the cowplot package (which, as always, needs to be installed first). library(cowplot) To do arrange the plots in an arbitrary rectangular grid, we first need to create each of our plots as a separate ggplot object. To avoid repetition, however, it’s best to first create a basic object and then add new layers to it and save it under separate names. plt &lt;- ggplot(Salaries, aes(fill = discipline, color = discipline)) plt_hist_s &lt;- plt + geom_density(aes(x = salary), alpha = 0.3) plt_hist_y &lt;- plt + geom_density(aes(x = yrs.since.phd), alpha = 0.3) plt_bar &lt;- plt + geom_bar(aes(y = rank)) plt_sct &lt;- plt + geom_point(aes(x = yrs.since.phd, y = salary)) + geom_smooth(aes(x = yrs.since.phd, y = salary), method = &#39;lm&#39;) plot_grid(plt_sct, plt_hist_s, plt_hist_y, plt_bar) `geom_smooth()` using formula = &#39;y ~ x&#39; Customizing plots Note that in a lot of the cases above, the plots were far from publication quality - while they allowed to get an overview of the data, they kept the original variable names on the axes and had issues with axis tick labels overlapping, amongst other things. All those issues can be fixed with ggplot’s customization capabilities. For startets, let’s consider one of the scatter plots we created earlier: ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; It would be useful to make the variable names more formal on the plot. This can be done using the labs layer, used to specify labels: ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + #add axes labels labs(x = &#39;Years since PhD&#39;, y = &#39;Salary in USD&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; Title of the plot can also be specified using the ggtitle function: ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + #add axes labels labs(x = &#39;Years since PhD&#39;, y = &#39;Salary in USD&#39;) + #add title ggtitle(&#39;Salaries of US college professors&#39;) `geom_smooth()` using formula = &#39;y ~ x&#39; Furthremore, we can customize the legend labels as well. Ths is done using the scale_color_discrete layer, which applies manual changes to the color aesthetic we specified when initializing the ggplot object. ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + #add axes labels: labs(x = &#39;Years since PhD&#39;, y = &#39;Salary in USD&#39;) + #add title ggtitle(&#39;Salaries of US college professors&#39;) + #rename legend elements: scale_color_discrete(name = &#39;Rank&#39;, labels = c(&#39;Asst. Prof.&#39;, &#39;Assoc. Prof.&#39;, &#39;Prof&#39;)) `geom_smooth()` using formula = &#39;y ~ x&#39; We can also customize the legend using the theme layer. This layer is used for customization of majority of plot elements, such as the legend, text and the axes. ggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + #add axes labels: labs(x = &#39;Years since PhD&#39;, y = &#39;Salary in USD&#39;) + #add title ggtitle(&#39;Salaries of US college professors&#39;) + #rename legend elements: scale_color_discrete(name = &#39;Rank&#39;, labels = c(&#39;Asst. Prof.&#39;,&#39;Assoc. Prof.&#39;,&#39;Prof&#39;)) + #customize the plot: theme(legend.position = &#39;bottom&#39;, #move legend to the bottom legend.background = element_rect(fill = &#39;lightblue&#39;), #legend background legend.text = element_text(size = 8, face = &#39;italic&#39;), #legend font legend.title = element_blank()) #remove title `geom_smooth()` using formula = &#39;y ~ x&#39; As seen above, the keyword arguments of theme usually take two forms - a string, such as legend.position = 'bottom', or element_* theme elements. You can read more about them in ?element_blank, however the basic logic is as follows - you use element_text() to specify text characteristics - such as the font, size, color or face of the text elements of the plot, such as legend labels or axis ticks, element_rect to specify the characteristics of rectangle elements, such as backgrounds and element_blank to make a given elements disappear from the plot entirely. In some cases our axis labels are two long, which can be problematic especially in the x axis, as it results in overlaps and makes impossible to read them. A common solution is to rotate the labels by a certain angle. This can be done using the axis.text.x argument with element_text(rotation = n), where n are the magnitude of counter-clockwise rotation. The vertical and horizontal justification can he adjusted using the vjust and hjust arguments respectively. byage &lt;- Salaries %&gt;% mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;% group_by(yrs_range, discipline) %&gt;% summarise(avg = mean(salary, na.rm = TRUE)) %&gt;% ungroup() `summarise()` has grouped output by &#39;yrs_range&#39;. You can override using the `.groups` argument. ggplot(byage, aes(x = yrs_range, y = avg, color = discipline, lty = discipline, group = discipline)) + geom_point() + geom_line() + labs(x = &#39;Years since PhD&#39;, y = &#39;Average earnings&#39;) + theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1.0)) Finally, we can make changes to the overall plot by applying themes to the entire plot. Some of them are built into the ggplot package; more come with cowplot or ggpubr. You can find couple examples below, but feel free to explore them online. plt &lt;- ggplot(byage, aes(x = yrs_range, y = avg, color = discipline, lty = discipline, group = discipline)) + geom_point() + geom_line() + labs(x = &#39;Years since PhD&#39;, y = &#39;Average earnings&#39;) plt + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) plt + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) plt + theme_classic() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) plt + theme_cowplot() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) plt + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) Note that the theme adjustments must be specified after a specific theme is specified, or otherwise if overrides them. You can also specify a theme upfront for your entire script by using theme_set(). theme_set(theme_grey(base_size = 10)) plt + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 1.0)) Saving plots Saving plots can be done using the ggsave function: ggsave(plot = plt, filename = &#39;files/test.png&#39;, width = 150, height = 100, units = &#39;mm&#39;) 7.2 Summary ggplot is a part of the tidyverse collection and the most popular plotting package in R you initialize a ggplot object using the ggplot() function, with the data frame you are planning to visualize as the first argument and the aesthetic mapping created by the aes() function as the second argument. aesthetic mappings are created by the aes() function, which defines connections between graphical elements of the plot and the variables from our data frame. The elements that can be mapped include the x and y axes, color, fill, shape or line type. The mapping can be created globally, provided to the ggplot() function and locally, by passing aes() to specific geoms. geoms are constituent elements of ggplot objects. They are different graphical structures, such as histograms, boxplots or scatter plots that can be added to an initialized ggplot object using the addition operator +. Each geom uses the aesthetic mapping specified globally, unless specified otherwise locally. facet_wrap and facet_grid functions can be used to split plots into small multiples. This is done using the ~ tilde operator - for example, facet_grid(sex ~ rank) will split our figure into a rectangular grid of plots of all combinations of sex and rank variables. we can customize elements of our plot, such as axes labels using the labs() command, legend names, using the scale_* functions, as well as other elements through the theme() layer, which most of the time works with element_* functions as arguments we can further apply pre-defined changes to our plots by using themes, such as theme_bw or theme_minimal. These can also be set for the entire session using theme_set(). plots can be saved via the ggsave function. Functions list function package description glimpse() c(“dplyr”, “package:tibble”) NA tibble() c(“dplyr”, “package:tidyr”, “package:tibble”) NA c() base Combine values/vectors into a vector length() base get number of elements in a vector or list library() base load an R package mean() base get mean of a vector rep() base repeat a value or a vector N times plot_grid() cowplot NA theme_cowplot() cowplot NA group_by() dplyr group tibble/data.frame by a factor variable. All further tidyverse operations are performed group-wise mutate() dplyr modify/create a column in a data frame summarise() dplyr collapse the dataset to a summary statistic. Usually used with group_by() ungroup() dplyr NA aes() ggplot2 NA coord_flip() ggplot2 NA cut_width() ggplot2 NA element_blank() ggplot2 NA element_rect() ggplot2 NA element_text() ggplot2 Customise text in GGPLOT2 facet_grid() ggplot2 NA facet_wrap() ggplot2 NA geom_bar() ggplot2 NA geom_boxplot() ggplot2 NA geom_col() ggplot2 NA geom_density() ggplot2 NA geom_histogram() ggplot2 NA geom_line() ggplot2 NA geom_point() ggplot2 NA geom_smooth() ggplot2 NA geom_violin() ggplot2 NA ggplot() ggplot2 Create a ggplot graph ggsave() ggplot2 NA ggtitle() ggplot2 NA labs() ggplot2 Customise labels in GGPLOT2 labs() ggplot2 Add a vertical line in GGPLOT2 scale_color_discrete() ggplot2 NA stat_summary() ggplot2 NA theme() ggplot2 Set theme for GGPLOT2 theme_bw() ggplot2 NA theme_classic() ggplot2 NA theme_grey() ggplot2 NA theme_minimal() ggplot2 NA theme_set() ggplot2 NA rgnorm() gnorm NA rnorm() stats NA data() utils NA 7.3 Exercises Consider the plot we have created earlier through the plot_grid function from the cowplot package. Use your knowledge about plot customization to make it more readable, for example by: keeping only one legend for the fill variable rotating the axis tick labels if necessary changing the axes labels making the points in the scatter plot smaller adding a theme Save the final figure to a png file. You can find the initial code below: plt &lt;- ggplot(Salaries, aes(fill = discipline, color = discipline)) plt_hist_s &lt;- plt + geom_density(aes(x = salary), alpha = 0.3) plt_hist_y &lt;- plt + geom_density(aes(x = yrs.since.phd), alpha = 0.3) plt_bar &lt;- plt + geom_bar(aes(y = rank)) plt_sct &lt;- plt + geom_point(aes(x = yrs.since.phd, y = salary)) + geom_smooth(aes(x = yrs.since.phd, y = salary), method = &#39;lm&#39;) plot_grid(plt_sct, plt_hist_s, plt_hist_y, plt_bar) `geom_smooth()` using formula = &#39;y ~ x&#39; For the remainder of next exericses, you will need to load the gapminder dataset which is a part of the gapminder package. data(gapminder, package = &#39;gapminder&#39;) Get an overview of the dataset. How many variables an observations are there? How many unique values does each variable have? Visualize the relationship between the per capita gdp and life expectancy in 2007 using a scatter plot. Include one line of best fit and color the markers by continent. Make sure the plot has appropriate axis labels. Does the line offer a good fit to the line? What kind of curve would better? Why? Examine the countries which appear to divert from the overall trend in the plot you created in the previous exercise. Visualize the difference in GDP per capita distribution between continents (in 2007). Use at least two different methods and compare their advantages and disadvantages. Remember about appropriate labels and title! Compare the GDP per capita of each continent in each year using a connected scatter plot. Be careful - taking the group average of each continent is incorrect, as the population sizes differ. You can try presenting the values on a log scale using scale_y_log10() to make the lines more visible. Prepare the same plot, but this time visualizing the average GDP growth rate for each of the continents. HINT: use the groupby() %&gt;% mutate() routine with the lag() function as in the previous chapter. The solutions for the exercises will be available here on 2021-01-07. Chapter solutions "],["association.html", "8 Statistical Inference and Association 8.1 Content 8.2 Summary 8.3 Exercises", " 8 Statistical Inference and Association Chapter script Chapter markdown Chapter exercises 8.1 Content library(tidyverse) So far, we have described variety of methods to wrangle and explore data in R. After performing cleaning and exploratory analysis, it is common to perform more formal tests on the data to verify whether the relationships we hypothesized hold under the scrutiny of tests supported by statistical theory. In this chapter we will present couple of testing such relationships. While some formal knowledge of statistical inference will be useful, the chapter does not rely on your background knowledge to heavily. Comparing means While a mean is a good measure of our data and comparing means across groups can help us determine the way a given variable varies depending on some conditions - for example, whether the average GDP growth over a certain period differs between two countries or whether average earnings are different depending on people’s gender and race. However, it may seem difficult to determine whether such difference is really meaningful, especially that we usually don’t have the entire population we are examining at hand, but only a (pressumably random) sample of the population. For example, suppose we randomly choose 5 men and 5 women and compute their average earnings. We obtain 35000 pounds for men and 40000 pounds for women. Should we claim that men on average earn less than women, based on this data? What difference is sufficient to claim that such difference exists in the overall population? Such questions are answered by statistical inference. Is mean different from 0? - one sample t-test We’ll start with a simple case. The data in the file wages.csv contains the information on the average salary of a sample of 100 individuals in 2005 and2 2010. We are interested in whether there was any growth in their salary. First, we read the data and obtain the percentege growth: wages &lt;- read_csv(&#39;data/wages.csv&#39;) Rows: 100 Columns: 3 ── Column specification ────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr (1): gender dbl (2): salary2005, salary2010 ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wages &lt;- wages %&gt;% mutate(change = (salary2010 - salary2005)/salary2005) We can then examine the average: mean(wages$change) [1] 0.01489452 We see that the mean is around 0.015. Is that sufficient to claim that the average salary of employees in our target population has grown between 2005 and 2010? To check that, we can perform the so-called t-test, which takes into account the variability of our sample and its size, and determines the probability that we falsely reject the hypothesis that the mean is equal to 0 - this probability is the p-value. Note that p-value is not the probability of the mean being equal to 0. For those familiar with frequentist statistical inference, recall that the p-value is the probability of obtaining a statistic more extreme than the sample statistic in under the assumption that the null hypothesis is true in repeated sampling. To conduct the t-test, we can simply use the t.test function: t.test(wages$change) One Sample t-test data: wages$change t = 2.7211, df = 99, p-value = 0.007688 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 0.004033511 0.025755527 sample estimates: mean of x 0.01489452 The p-value is usually the most important part of the output produced by the function. In this case, it is 0.0077. This means, that the mean is significantly different from 0 at the 95% confidence level. Confidence level is a way in which we can quantify our certainty in the difference of the parameter from some hypothesized value (in this case 0). The 95% confidence level is one of the most commonly used thresholds in the social sciences. Generally, the recipe for checking if a p-value satisfies a cerain confidence level is to subtract our confidence level from 1 and compare it with the p-value. If the p-value is smaller than the value obtained in the subtraction, the significance of the test statistic satisfies it. So, in our example, for 95% confidence \\(1 - 0.95 = 0.05\\) and 0.0077 $ &lt; 0.05$, thus we can say the the mean is significantly different from 0 at the 95% confidence level. Another important information reported by the output of the t-test is the confidence interval. The confidence interval provides us with a way of quantifying our uncertainty about the underlying value of a statistic (in our case the mean), given the variability of our sample (measured by the standard deviation, as described in chapter 4. Suppose we take 100 random samples from our original population. The 95% confidence interval describes the range into which we exepct the mean of 95 out of 100 samples to fall. In case of the mean, it is given by \\(\\bar{\\mu}\\pm 1.96SE(\\bar{\\mu})\\), where \\(\\bar\\mu\\) is the sample mean we obtained and \\(SE(\\bar{\\mu})\\) is the standard error of the mean given by \\(\\sigma/\\sqrt{n}\\), which is the sample standard deviation divided by the square root of the number of the observations in our sample. Note that 95% confidence interval not including 0 is equivalent with the statistic being significantly different from 0 at the 95% level. Error bars The confidence interval around the mean can be displayed in ggplot using the geom_errorbar function. The so-called errorbar is a good way to display the range of the likely values around the mean. In case of our data, to plot the errorbars, we first compute the standard error of the mean using the formula discussed above. wages_summary &lt;- wages %&gt;% summarise(change_mean = mean(change), change_err = sd(change)/sqrt(n())) We than create a bar plot and add the geom_errorbar on top of it. Note that in this case we specify the x aesthetic as a chraracter 'Wage change'. This is because it is unusual to plot just one bar in the plot, and geom_col and geom_errorbar require some x aesthetic to be passed. ggplot(wages_summary, aes(x = &#39;Wage change&#39;, y = change_mean)) + geom_col(width = 0.2) + geom_errorbar(aes(ymin = change_mean - 1.96 * change_err, ymax = change_mean + 1.96 * change_err), width = 0.1) + labs(x = &#39;&#39;, y = &#39;Mean change in wage&#39;) Are two means different? - two sample t-test By simple extension, we can apply the logic of the t-test to the difference between two means within one sample - this is so called a two sample t-test and is implemented in R by the same function. The grouping variable is specified using the formula notation, which we will encounter in the next chapter when dealing with linear regression. To test the difference in mean of variable x between groups defined by y, we specify t.test(x ~ y). So, coming back to our example, we can examine whether the average salary in 2005 is different for male and female employees. t.test(salary2005 ~ gender, data = wages) Welch Two Sample t-test data: salary2005 by gender t = -2.4595, df = 97.494, p-value = 0.01568 alternative hypothesis: true difference in means between group F and group M is not equal to 0 95 percent confidence interval: -19486.287 -2082.285 sample estimates: mean in group F mean in group M 31020.72 41805.01 The result seem to be significant, meaning that we have statistical evidence that there is a real difference between male and female earnings in the underlying populations (or more precisely, that if we took 100 samples from the underlying population, more than 95 of them would show a non-zero difference). Based on our sample, however, the magnitude of that difference is quite difficult to determine, as we expect it to range between 19000 and 2000 dollars. We can visualize this again using geom_errorbar: wages %&gt;% group_by(gender) %&gt;% summarise(wage_mean = mean(salary2005), wage_err = sd(salary2005)/sqrt(n())) %&gt;% ggplot(aes(x = gender, y = wage_mean)) + geom_col(width = 0.3) + geom_errorbar(aes(ymin = wage_mean - wage_err, ymax = wage_mean + wage_err), width = 0.2) + ylab(&#39;Average salary in 2005&#39;) Relationship between two continuous variables Covariance So far we have discussed some techniques for looking at statistical relationships between one continuous variable (such as wage) and one categoric variable (such as salary). Under many circumstances, however, we would be interested in measuring the association between two continuous variables. In this part we will consider data on a sample of students with free variables: average weekly time spent studying, days a student has been home ill and achieved GPA. scores &lt;- read_csv(&#39;data/scores.csv&#39;) Rows: 100 Columns: 3 ── Column specification ────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; dbl (3): study_time, days_ill, gpa ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We can examine pairwise relationships between the variables using the pairs() function, which is a useful tool when analyzing datasets with multiple continuous variables. pairs(scores,upper.panel = NULL) Let’s start by interpreting this plot. The rectangles with variable tell as the name of the axis. For example, the plot in the lower left corner shows the relationship between the GPA (x axis) and study time (y axis). We can see that students in our sample who spent more time studying seem to have had higher grades - points higher on one axis tend to also be higher on the other. On the other hand, students who were ill more had, on average, achieved lower grades. There seems to be no relationship between being ill and time spent studying. In statistical terms, this relationship can be measured with covariance between these two variables. Recall our discussion of variance in the exploratory analysis chapter. We calculated it for vector x using sum((x - mean(x))^2)/(length(x) - 1), or the var(x) function for short. Covariance is essentially the same, but calculated for two variables instead of one, using sum((x - mean(x)) * mean((y - mean(y))))/(length(x) - 1) or the cov function for short. You can see that demonstrated below: cov_fun &lt;- function(x, y) sum((x - mean(x)) * (y - mean(y)))/(length(x) - 1) cov_fun(scores$gpa, scores$study_time) [1] 1.337834 cov(scores$gpa, scores$study_time) [1] 1.337834 The variance is actually nothing else but covariance of a variable with itself! cov(scores$gpa, scores$gpa) == var(scores$gpa) [1] TRUE Essentially covariance measures the average of element-wise product of each observations deviation from its mean for two variables. This may sound complicated, but breaking it down makes it simple. In our two variables, gpa and days_ill, people who have grades higher than the average grade also tend to study longer than the average study time. So, if for all students we subtract the mean grade and mean study time from their respective grades and study times, the average of that will be positive. cov(scores$gpa, scores$study_time) [1] 1.337834 If, on the other hand people who have higher grades than average tend to have lower time spent ill at home than average, the average of the products of these demeaned (i.e. with the mean subtracted from each) variables will be negative, since the majority of the products will be negative. cov(scores$gpa, scores$days_ill) [1] -11.16068 Finally, if there’s no relationship between two variables - for example between days_ill and study_time, we expect the average of the products of the demeaned variables to be close to 0, since around the same number will be positive and negative. cov(scores$study_time, scores$days_ill) [1] 2.419399 The last example seems to be incorrect? How is that possible that the covariance between study_time and days_ill is larger than the covariance between study_time and gpa, even though the plot suggests a much stronger relationship between the latter? We will explore this in the next section on correlation. Correlation The downside of covariance as a measure of relationship is that it is completely dependent on the units of the two variables measures, or, more percisely, the range of values they take. If we take covariance average time spent studying and GPA and compare it with the covariance between average time spent studying and time ill, the latter will be greater, simply because time is expressed in hours and days, with values ranging between 0.8 and 15.1 and 1 and 110 respectively, while GPA ranges between 1 and 5: vapply(scores, range, numeric(2)) study_time days_ill gpa [1,] 1.42 4 1.33 [2,] 15.28 100 4.70 So, the values we get by multiplying study_time and days_ill will tend to be higher in general. This makes the use of covariance problematic in estimating the objective strength of the relationship between two variables. In order to avoid it, we use correlation instead. Correlation is simply covariance, but divided by the product of the standard deviation of each of the variables. Without going into the mathematical details, suffice to say that it standardizes the covariance, so that it ranges between -1 and 1, -1 meaning perfect negative relationship (i.e. when one variable increases, the other always decreases) and 1 meaning perfect positive relationship (when one variable increases, the other decreases). 0 means that there’s no relationship between these variables. We can see that again using R code. cor_fun is created for demonstration purposes, but the usual function used in R to obtain correlation is simply cor. cor_fun &lt;- function(x, y) cov_fun(x, y)/(sd(x) * sd(y)) cor_fun(scores$gpa, scores$study_time) [1] 0.5538811 cor(scores$gpa, scores$study_time) [1] 0.5538811 Now we can see, that the relationship between study_time and days_ill is indeed close to 0: cor(scores$study_time, scores$days_ill) [1] 0.03523657 We can also use cor on entire data frame to obtain the so-called correlation matrix, which shows pairwise relationships between two variables cor(scores) study_time days_ill gpa study_time 1.00000000 0.03523657 0.5538811 days_ill 0.03523657 1.00000000 -0.7001128 gpa 0.55388114 -0.70011285 1.0000000 Note that the diagonal is always 1, i.e. the correlation of a variable with itself is always 1. For obvious reasons, the function also won’t work if applied to a data frame containing any text data: cor(wages) To visualize pariswise relationships between two variables using a plot that is a bit more sophisticated that the one produced by pairs() we can use the ggpairs function from the GGally package, which produced a ggplot style plot, presenting correlations, density plots and scatter plots of all the numeric variables in our dataset: library(GGally) ggpairs(scores) Further details Here some more details on how the p-value is computed: err &lt;- sd(wages$change)/sqrt(nrow(wages)) tscore &lt;- mean(wages$change)/err df &lt;- length(wages$change) - 1 pt(tscore, df, lower.tail = FALSE) * 2 [1] 0.007687596 t.test(wages$change) One Sample t-test data: wages$change t = 2.7211, df = 99, p-value = 0.007688 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 0.004033511 0.025755527 sample estimates: mean of x 0.01489452 8.2 Summary Functions list function package description cor_fun() .GlobalEnv NA cov_fun() .GlobalEnv NA cov() c(“pROC”, “package:stats”) NA var() c(“pROC”, “package:stats”) NA length() base get number of elements in a vector or list library() base load an R package mean() base get mean of a vector nrow() base get number of rows of a data frame numeric() base initialize a numeric vector sqrt() base square root sum() base get sum of numeric values or a vector vapply() base NA group_by() dplyr group tibble/data.frame by a factor variable. All further tidyverse operations are performed group-wise mutate() dplyr modify/create a column in a data frame n() dplyr NA summarise() dplyr collapse the dataset to a summary statistic. Usually used with group_by() ggpairs() GGally NA aes() ggplot2 NA geom_col() ggplot2 NA geom_errorbar() ggplot2 NA ggplot() ggplot2 Create a ggplot graph labs() ggplot2 Customise labels in GGPLOT2 labs() ggplot2 Add a vertical line in GGPLOT2 ylab() ggplot2 NA pairs() graphics NA read_csv() readr read tibble from a csv file cor() stats NA pt() stats NA sd() stats Get standard deviation of a vector t.test() stats NA 8.3 Exercises Write a function to compute correlation for just the numeric variables in any data frame. Test it on the wages data. Why is the correlation of a variable with itself always equal to 1? Write R code to demonstrate this. For the remainder of the exercises, come back to the Salaries data we used in the previous chapter: Using correlation, examine which relationship is stronger - the one between years since PhD and salary or the one between years in service and salary. #ANSWER cor(Salaries$salary, Salaries$yrs.since.phd) [1] 0.4192311 cor(Salaries$salary, Salaries$yrs.service) [1] 0.3347447 #the relationship between years since PhD and salary appears to be stronger Use t-tests to check whether we can confirm or reject the following hypothesis, based on the sample in the Salaries dataset. There’s no difference between the salary of Male and Female professors. There’s no difference between the salary of professors from the applied and theoretical department. The average salary is not different from 100 000 USD. Recall the plot produced towards the end of the previous chapter depicting the average salary by age group and academic discipline. Produce the same plot, but this time include error bars depicting the confidence intervals around each of the means. The code to produce the table used in the last exercises is below. Note that you need to modify it (by computing the standard errors) before plotting the data. Remeber about adding appropriate labels to the plot. byage &lt;- Salaries %&gt;% mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;% group_by(yrs_range, discipline) %&gt;% summarise(avg = mean(salary, na.rm = TRUE)) %&gt;% ungroup() `summarise()` has grouped output by &#39;yrs_range&#39;. You can override using the `.groups` argument. Examine the correlation between years since phd and average salary in each of the groups defined in the data. The solutions for the exercises will be available here on 2021-02-18. Chapter solutions "],["ols.html", "9 Linear Regression 9.1 Content 9.2 Summary 9.3 Exercises 9.4 References", " 9 Linear Regression Chapter script Chapter markdown 9.1 Content Site still under construction! Introduction Regression is the power house of the social sciences. It is widely applied and takes many different forms. In this Chapter we are going to explore the linear variant, also called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In the following Chapter we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression. Bivariate Linear Regression The Theory Regression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the effect. Let us choose Y as our dependent variable, and X as our independent variable. We have some data which we are displaying in a scatter plot: With a little goodwill we can already see that there is a positive relationship: as X increases, Y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line. The position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much Y increases (or decreases is the slope is negative) if we add an additional unit of X. In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients. You can see this depicted graphically here: Population We will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows: \\[\\begin{equation} E(Y|X_{i}) = \\beta_{0} + \\beta_{1} X_{i} \\tag{9.1} \\end{equation}\\] Because wer are dealing with the population, the line is the geometric locus of all the expected values of the dependent variable Y, given the values of the independent variables X. This has to do with the approach to statistics that underpins this module: frequentist statsctics (as opposed to Bayesian statistics). We are understanding all values to be “in the long run”, and if we sampled repeatedly from a population, then the expected value is the value we would, well, expect to see most often in the long run. The regression line is not intercepting with all observations. Only two are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(Y|X_{i})\\) and \\(Y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\). To describe the observations \\(Y_{i}\\) we therefore need to add the error terms to equation (9.1): \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i} \\tag{9.2} \\end{equation}\\] Sample In reality we hardly ever have the population in the social sciences, and we generally have to contend with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive fromt he sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF. In order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this: \\[\\begin{equation} \\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} \\tag{9.3} \\end{equation}\\] Analogously, we would would describe the observations \\(Y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation. \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} + \\hat{\\epsilon}_{i} \\end{equation}\\] The following graph visdualised the relationship between an observation, the PRF, the SRF and the respective error terms. Ordinary Least Squares (OLS) When you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error term \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero. But similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS). We are now ready to apply this stuff to a real-world example! The Application In the applied part of this Chapter, we are going to model the feelings towards Donald Trump in the lead-up to the presidential election 2020. Data for this investigation are taken from https://electionstudies.org/data-center/2020-exploratory-testing-survey/ Please follow this link and download the “2020 Exploratory Testing Survey” and pop it into a working directory. We can then load the ANES data set: anes &lt;- read.csv(&quot;data/anes.csv&quot;) summary(anes$fttrump1) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00 0.00 40.00 44.59 80.00 999.00 This is no good, as the variable is bounded between 0 and 100. In fact 999 is a placeholder for missing data throughout the data set. We need to replace this with NAs. anes[anes == 999] &lt;- NA If we look at the summary again, everything looks fine now: summary(anes$fttrump1) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 0.00 0.00 40.00 42.42 80.00 100.00 7 By the way, had you just wanted to replace this in one variable, for example only in fttrump1, you could have called: anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) Norris and Inglehart (2016) have argued that: populist support in Europe is generally stronger among the older generation, men, the less educated, the religious, and ethnic majorities, patterns confirming previous research. Let’s see if this also applies to presidential elections in the US. We first look at the question: “Do older people rate Trump higher than younger people?”. Our independent variables is age. summary(anes$age) Min. 1st Qu. Median Mean 3rd Qu. Max. 18.00 34.75 49.00 48.87 63.00 110.00 Let’s evaluate the relationship through a scatter plot with a line of best fit: library(tidyverse) ggplot(anes, aes(x = age, y = fttrump1)) + geom_point() + geom_smooth(method = lm) There is a positive relationship. We can calculate the exact numerical nature of that relationship as follows: model1 &lt;- lm(fttrump1 ~ age, data = anes) We start by specifying an object into which we store the results. Then we call lm which means linear model. Our dependent variable fttrump1 is listed first, and then after a tilde the independent variable, age. Finally, we tell R which data set to use. We can then print the result, by calling model1. model1 Call: lm(formula = fttrump1 ~ age, data = anes) Coefficients: (Intercept) age 31.6837 0.2197 How would we interpret these results? At an age of zero, a person would rate Trump at 31.68 on average. This of course makes little sense in anything but a theoretical / mathematical consideration. With every additional year of age, a person would rate Trump 0.22 points higher on average. But are these findings significant at an acceptable significance level? Let’s find out, by getting a more detailed output: summary(model1) Call: lm(formula = fttrump1 ~ age, data = anes) Residuals: Min 1Q Median 3Q Max -55.847 -39.152 -0.523 39.258 64.143 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 31.68372 2.14801 14.750 &lt; 2e-16 *** age 0.21967 0.04157 5.284 1.35e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 38.67 on 3071 degrees of freedom (7 observations deleted due to missingness) Multiple R-squared: 0.009011, Adjusted R-squared: 0.008688 F-statistic: 27.92 on 1 and 3071 DF, p-value: 1.35e-07 OK, there is a lot more here, and it is worth pausing to go through this step by step. First, R reminds us of the actual formula we have used to estimate the model: I am ignoring the section on residuals, as we don’t need to make our life more difficult than it needs to be. Now come the coefficients: The size and direction is of course the same as in our previous output, but this output now contains some additional information about the standard error, the resulting t-value, and the p-value. R is very helpful here, in that it offers us a varying amount of asterisks according to different, commonly accepted levels of significance. 0.05 is standard practice in the social sciences, so we will accept anything with one, or more asterisks. Both our intercept and the slope coefficient are significant at a 95% confidence level, so we have shown that there is a statistical relationship between age and ratings for Trump. I am omitting the residual standard error for the same reason as before, but let us look at the model fit indicators. Multiple R-Squared (aka \\(R^{2}\\)) tells us how much variation in the dependent variable fttrump1 is explained through the independent variable age. \\(R^{2}\\) runs between 0 and 1, where 1 is equal to 100% of the variation. In our case, we have explained a mere 0.09% of the Trump rating. This is lousy, and we can do a lot better than that. Never expect anything near 100% unless you work with a toy data set from a text book. If you get 60-70% you can be very happy. I will return to Adjusted \\(R^{2}\\) in the Section on Multiple Linear Regression. The F-statistic at the end: is a test with the null hypothesis that all coefficients of the model are jointly zero. In our case, we can reject this null hypothesis very soundly, as the p-value is far below the commonly accepted maximum of 5%. Categorical Independent Variables (aka ‘Dummies’) Often variables are categorical. One such example is the variable sex which has two categories: male and female. summary(anes$sex) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 1.000 2.000 1.522 2.000 2.000 table(anes$sex) 1 2 1473 1607 Turn this into a factor variable and assign telling labels anes$sex &lt;- factor(anes$sex, labels = c(&quot;Male&quot;, &quot;Female&quot;)) Check if this has worked: table(anes$sex) Male Female 1473 1607 Let’s estimate the model: model2 &lt;- lm(fttrump1 ~ sex, data = anes) summary(model2) Call: lm(formula = fttrump1 ~ sex, data = anes) Residuals: Min 1Q Median 3Q Max -46.156 -38.992 -1.156 38.844 61.008 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 46.156 1.009 45.749 &lt; 2e-16 *** sexFemale -7.165 1.397 -5.129 3.09e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 38.68 on 3071 degrees of freedom (7 observations deleted due to missingness) Multiple R-squared: 0.008493, Adjusted R-squared: 0.00817 F-statistic: 26.31 on 1 and 3071 DF, p-value: 3.095e-07 How do we interpret this? Let’s do the slope coefficient first: a women would rate Trump at 7.16 points less than a man on average. The interpretation of a dummy variable coefficient is done with regards to the reference category. In our case this is “male”. So the effect we observe here is equivalent of moving from “male” to “female” and that effect adds 7.16 points. This gives you an indication of how to interpret the intercept in this case: The value displayed is how men would rate Trump on average, namely at 46.16 points. All of this is significant at a 95% confidence level. This effect corroborates the hypothesis advanced by Inglehart and Norris, but the results are not displayed in the most elegant way. The sex these authors made a statement about were men. So we need to change the reference category to “female”. anes &lt;- anes %&gt;% mutate(sex = relevel(sex, ref = &quot;Female&quot;)) When we re-estimate the model, we get the effect displayed directly: model2 &lt;- lm(fttrump1 ~ sex, data = anes) summary(model2) Call: lm(formula = fttrump1 ~ sex, data = anes) Residuals: Min 1Q Median 3Q Max -46.156 -38.992 -1.156 38.844 61.008 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 38.9919 0.9661 40.358 &lt; 2e-16 *** sexMale 7.1646 1.3969 5.129 3.09e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 38.68 on 3071 degrees of freedom (7 observations deleted due to missingness) Multiple R-squared: 0.008493, Adjusted R-squared: 0.00817 F-statistic: 26.31 on 1 and 3071 DF, p-value: 3.095e-07 Whilst many categorical variables are binary, of course not all of them are. So how does this work with a categorical variable with 3 or more levels? The next determinant mentioned in Inglehart and Norris’ paper is education. We can obtain information about a respondent’s level of education with the variable educ. summary(anes$educ) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 3.000 4.000 4.012 5.000 8.000 table(anes$educ) 1 2 3 4 5 6 7 8 100 656 622 326 761 424 102 89 This is not terribly telling in itself, yet, so let’s have a look at the codebook: The first step is, as before to recode this variable into a factor variable: anes$educ &lt;- factor(anes$educ) Eight levels are too many here to do any meaningful analysis, and two would be too reductionist. For sake of simplicity, let’s go with three: low, medium and high education. We recode into an ordered factor as follows: anes &lt;- anes %&gt;% mutate(educ_fac = recode(educ, &#39;1&#39;=&quot;low&quot;, &#39;2&#39;= &quot;low&quot;, &#39;3&#39;= &quot;low&quot;, &#39;4&#39; = &quot;medium&quot;, &#39;5&#39; = &quot;medium&quot;, &#39;6&#39; = &quot;high&quot;, &#39;7&#39; = &quot;high&quot;, &#39;8&#39; = &quot;high&quot;)) Check the results: table(anes$educ_fac) low medium high 1378 1087 615 And we are ready to go: model3 &lt;- lm(fttrump1 ~ educ_fac, data = anes) summary(model3) Call: lm(formula = fttrump1 ~ educ_fac, data = anes) Residuals: Min 1Q Median 3Q Max -42.943 -42.019 -2.388 37.981 57.981 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 42.0189 1.0481 40.090 &lt;2e-16 *** educ_facmedium 0.9240 1.5775 0.586 0.558 educ_fachigh 0.3693 1.8870 0.196 0.845 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 38.85 on 3070 degrees of freedom (7 observations deleted due to missingness) Multiple R-squared: 0.0001119, Adjusted R-squared: -0.0005395 F-statistic: 0.1718 on 2 and 3070 DF, p-value: 0.8422 Whilst the intercept is statistically significant, the slope coefficients are not. Therefore, we can conclude that education has no statistical influence on Trump’s approval ratings. Note, that as in the sex example before, R has chose the first level of the independent variable as the reference category. If you wish to change this, you can do so in the same manner as before. You can also check which level R has used as the reference category with the contrasts() command. Here: contrasts(anes$educ_fac) medium high low 0 0 medium 1 0 high 0 1 Summary for Bivariate Regression And that’s it! You have made the first big step to understanding regression output and producing such output yourself. But explanations in the real world are never mono-causal. There are always multiple influences working at the same time, and we need to set up our statistical model to take this complexity into account. Which brings us to the next step: multiple regression. Multiple Linear Regression The Theory We are simply extending Equation (9.3) by adding more independent variables. For two independent variables we would write: \\[\\begin{equation} \\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{1i} + \\hat{\\beta}_{2} X_{2i} \\end{equation}\\] Note that not only the betas have a subscript now, but also the independent variables. For example \\(X_{1i}\\) would denote the \\(i^{th}\\) observation of independent variable 1. We can extend this more generally to: \\[\\begin{equation} \\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{1i} + \\hat{\\beta}_{2} X_{2i} + ... + \\hat{\\beta}_{n} X_{ni} \\tag{9.4} \\end{equation}\\] where n is the number of independent variables in the model. The Application Just as we have extended Equation (9.3) to (9.4), we can extend our model in R - we simply need to connect the independent variables with +. If we wished to look at the joint influence of all independent variables we have included so far, we would type: model4 &lt;- lm(fttrump1 ~ age + sex + educ_fac, data = anes) summary(model4) Call: lm(formula = fttrump1 ~ age + sex + educ_fac, data = anes) Residuals: Min 1Q Median 3Q Max -55.109 -38.373 -1.385 37.536 68.242 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 27.33647 2.40776 11.353 &lt; 2e-16 *** age 0.23274 0.04156 5.599 2.34e-08 *** sexMale 7.66785 1.40848 5.444 5.62e-08 *** educ_facmedium 0.32209 1.56889 0.205 0.837 educ_fachigh -0.36738 1.89537 -0.194 0.846 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 38.5 on 3068 degrees of freedom (7 observations deleted due to missingness) Multiple R-squared: 0.01868, Adjusted R-squared: 0.0174 F-statistic: 14.6 on 4 and 3068 DF, p-value: 8.15e-12 The Interpretation As we have included multiple independent variables now, the interpretation of coefficients changes slightly. The principle here is called “Ceteris Paribus” which means “all other things being equal”. What exactly does that mean? Take the coefficient for age, for example. If this was a bivariate regression we would interpret it as follows: “on average, for every additional year of age, the support for Trump would increase by 0.23274 units”. The first thing you will note here is that the size of this coefficient has changed. When we ran the bivariate model it was 0.21967. The reason is the inclusion of other variables. By doing this, we are isolating, or purifying the influence our variable age has, holding sex and educ_fac constant. You could also say “sex and educ_fac being equal, on average, for every additional year of age, the support for Trump would increase by 0.23274 units”. This is our new interpretation of coefficients in multiple regression. Model Fit (again) If I added “shoe size of interviewee” into the model, this would make absolutely no sense from a theoretical point of view. Yet, our R-Squared would either stay the same, or even increase. R-Squared can never decrease from the addition of more variables. This is of course no good. We need a measure that takes into account the number of independent variables, and penalises us for the inclusion of irrelevant variables. This measure is called “Adjusted R-Squared” and can be found at the bottom of the model summary. If you run a bivariate model, always use “Multiple R-Squared”, when running multiple regression, always use “Adjusted R-Squared”. This will allow you to compare the model fit between models and to determine whether a variable adds explanatory power (“Adjusted R-Squared” increases), is pointless (“Adjusted R-Squared” stays the same), or is detrimental to the model (“Adjusted R-Squared” decreases). Model Specification You will have noticed that I quoted Norris and Inglehart’s determinants for populist support and I have done so intentionally. The selection of your independent variables always needs to be guided by theory. Theory provides a logic and a structure to our enquiry which makes it scientific (which is why what we do is called “Political Science” and not “Politics”, and let nobody tell you otherwise. Politics takes place in Westminster and is defined as the negotiation between different groups according to their respective power - don’t say you didn’t learn anything on this module). Otherwise, we would only be stabbing into the dark, randomly checking factors we deem influential. The composition of a regression model therefore also needs to be guided by theory. If a theory has multiple components, or has been extended over time, you can test the propositions of each stage separately. Classical Modernisation for example, posits that wealthier countries are more democratic. Later on, influencing factors were extended to health, education, urbanisation, etc. So it would make sense to run a model with just GDP first, and then to add the other three variables. Occasionally, the inclusion of new variables takes the significance away from previously included ones, and you are able to show that these new variables explain the variation in the dependent variable better. So, it always makes sense to test different combinations of independent variables to look deeper into which explains the outcome better / best. One last word on selecting independent variables: you can’t just throw a random number of variables into the model. you need to observe the principle of parsimony which asks you to use as many variables as necessary, but as few as possible. 9.2 Summary OLS Acronym for “Ordinary Least Squares”. CLM Acronym for “Classical Linear Model”. BLUE Acronym for “Best Linear Unbiased Estimator”. Parsimony Use as many variables as necessary, but as few as possible. Functions list function package description c() base Combine values/vectors into a vector factor() base create a factor library() base load an R package summary() base Obtain summary statistics or detailed regression output table() base obtain frequency table of a variable/cross-tabulation of two variables mutate() dplyr modify/create a column in a data frame recode() dplyr Recode a variable aes() ggplot2 NA geom_point() ggplot2 NA geom_smooth() ggplot2 NA ggplot() ggplot2 Create a ggplot graph contrasts() stats NA lm() stats NA relevel() stats NA read.csv() utils read a csv file to data frame. Specify stringsAsFactors = FALSE to keep all string columns as characters 9.3 Exercises The relationship proposed by Inglehart and Norris (2016) also includes religiosity and ethnic majorities as possible predictors. Using the variable att2, assess in a bivariate regression the influence of religiosity. Categorise those attending services almost every week, or every week as ‘religious’, and all others as ‘not religious’. Using the variable latin1, assess in a bivariate regression if being a member of Hispanic, Latino, or Spanish origin has a negative effect on approving of Trump. Another, frequently used variable to predict endorsements of politicians is income. In a bivariate regression, use the variable income to test this relationship. For this, create a new variable called income_fac with three levels: low income (0-44,999), medium income (45,000-84,999), and high income (&gt;85,000). The solutions for the exercises will be available here on 2021-02-18. Chapter solutions 9.4 References Inglehart, Ronald F. and Norris, Pippa (2016) Trump, Brexit, and the Rise of Populism: Economic Have-Nots and Cultural Backlash. HKS Working Paper No. RWP16-026, Available at SSRN: https://ssrn.com/abstract=2818659 or http://dx.doi.org/10.2139/ssrn.2818659 "],["logit.html", "10 Logistic Regression 10.1 Content 10.2 Summary 10.3 Exercises", " 10 Logistic Regression Chapter script Chapter markdown Chapter data 10.1 Content Site still under construction! In the previous Chapter we encountered linear regression analysis which allowed us to quantify the amount and direction of one or more independent variables on a continuous dependent variable. I already mentioned there, that there is also a type of a regression which can deal with a binary dependent variable. This is usually a yes/no scenario, such as democracy / autocracy, war / peace, trade agreement / no trade agreement, … You get the picture. Many problems or questions in political science have binary outcomes, and so you are about to learn a very important and useful method to answer research questions. As in the previous Chapter, I will take you some through some theory first, and then we are applying the theory to an empirical example. This time concerning the survival of passengers on the Titanic. Logit - The Intuition COVID-19 has put a bit of a damper on this, but a question we can all relate to is whether to go out tonight, or not. The “propensity to go out” is not directly observable, and so we call this a latent variable. You can imagine this running from minus infinity to plus infinity, and at some point on this continuum you are making the decision to go out. Let’s call this point tau (\\(\\tau\\)). Graphically, this would look like this: Your inclination to go out, is likely to be influenced by the amount of money you have in your wallet / bank. If you are broke, you will be less inclined (if you are sensible), and if you are swimming in it, you will be more inclined. So, if the “propensity to go out” (which remember is running from minus to plus infinity) is influenced by your budget, then let’s construct a graph, in which we pop the propensity to go out on the y-axis, and the budget on the x-axis. If we assume that this relationship is linear, we can fit a regression line into this coordinate system, just as we have in the previous Chapter: Whilst this visualises the influence of the budget on the latent variable, what we are aiming for is to make a prediction about the probability of you going out, or not. Now imagine, your budget is \\(x_{1}\\). This is way below the threshold \\(\\tau\\) and so the most likely scenario here is that you will stay home. But all of your friends are going out, the sun is shining, and you just scored a 74 in an essay. So there is some chance you will still go out. It is less likely than the budgetary constraint would suggest, but it is there. Where I am going with this is this: over every budgetary point, there is a probability distribution (for logit a bell-shaped curve not unlike the normal distribution, called a logistic distribution4). If we draw these probability distributions in, the graph looks like this: The probability of going out is coloured in in grey. You can see that even at \\(x_{1}\\) there is a teeny bit of probability that you will go out. As the budget increases, more and more probability slides over the threshold \\(\\tau\\), until we reach the magical point of \\(x_{5}\\) where the probability is 50%. From there on, the amount of probability sliding over \\(\\tau\\) is steadily decreasing, because of the shape of the logistic distribution. We can depict the amount of probability (or the size of the grey area) for each \\(x_{i}\\) in a separate graph which is called the Cumulative Probability (Density Function), or short CDF: This s-shaped curve now gives us the probability (of going out) for each \\(x_{i}\\) (budget). It is important to note that the relationship is not linear, as in linear regression. Because we have an s-shaped curve the increase in probability when going from \\(x_{2}\\) to \\(x_{3}\\) is not the same as going from \\(x_{3}\\) to \\(x_{4}\\). You can see that visualised here: We will therefore not be able to interpret the coefficients in the same way as for OLS. We will be using predicted probabilities instead. But one step at a time. Let’s first get our hands dirty with some data. Logit - The Estimation We are going to use data on the passengers of the Titanic in 1912 and will investigate why or why not they survived. Schematically, we can write the logit command as follows: logit &lt;- glm(depvar ~ indepvar, data = mydata, family = &quot;binomial&quot;) Before we can use it, however, we need to set the working directory, load the data, and prepare our dependent variable, turning it into a factor: library(readxl) library(tidyverse) titanic &lt;- read_excel(&quot;data/titanic.xlsx&quot;, sheet=&quot;titanic_full&quot;) titanic$survived &lt;- factor(titanic$survived, labels=c(&quot;no&quot;, &quot;yes&quot;)) For our first investigation, let’s see whether a passenger’s age has influenced the probability to survive the sinking. Our hypotheses are as follows: \\(H_{0}\\): Age had no impact on the probability to survive \\(H_{a}\\): Age had an impact on the probability to survive If we visualise the Null-Hypothesis, we would find the same probability (the average probability) of surviving for every \\(x_{i}\\): Let’s see how much evidence against this hypothesis we have by estimating the logit model: logit &lt;- glm(survived ~ age, data = titanic, na.action = na.exclude, family = &quot;binomial&quot;) As in OLS we can obtain the results by using the summary() function: summary(logit) Call: glm(formula = survived ~ age, family = &quot;binomial&quot;, data = titanic, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.136531 0.144715 -0.943 0.3455 age -0.007899 0.004407 -1.792 0.0731 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1414.6 on 1045 degrees of freedom Residual deviance: 1411.4 on 1044 degrees of freedom (263 observations deleted due to missingness) AIC: 1415.4 Number of Fisher Scoring iterations: 4 I will take you throught his output bit by bit now. Logit - The Output As always, R first shows you the formula for the estimated model. You will find the coefficients further down below – not unlike the OLS output. First we need to see whether our coefficients are statistically significant. If they are not, then the numbers have just occurred by chance, and we cannot take any message from them. We do this by looking at the p-value: In the social sciences we generally require a 95% confidence level, or a p-value \\(p\\le 0.05\\). Our p-value for the age coefficient is 0.0731 and is therefore larger than the required threshold. We can draw the conclusion that age had no significant impact on the probability to survive the sinking of the Titanic! R also gives you some information on missing values in the end. It is always worthwhile looking at this, so that you are aware how many observations have actually been used for the estimation. Logit - Interpreting the Coefficients So far, I have only said that in logistic regression we cannot interpret coefficients directly, because they are not linear. This is why we have to resort to something called predicted probabilities. But what are they? Before I can show you this, we need to estimate a new model, because we can only interpret coefficients if they are statistically significant - I know I am labouring the point, but you would be surprised how often this goes wrong in essays. Let’s look at whether age influenced the probability to travel in first-class. For this, we recode the class variable into a binary dummy and then estimate our model: titanic &lt;- titanic %&gt;% mutate(class = as.numeric( recode(pclass, &#39;1&#39;=&#39;1&#39;, &#39;2&#39;=&#39;0&#39;, &#39;3&#39;=&#39;0&#39;))) class_age &lt;- glm(class ~ age, data = titanic, na.action = na.exclude, family = &quot;binomial&quot;) summary(class_age) Call: glm(formula = class ~ age, family = &quot;binomial&quot;, data = titanic, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.187456 0.213561 -14.93 &lt;2e-16 *** age 0.067767 0.005825 11.63 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1223.3 on 1045 degrees of freedom Residual deviance: 1056.1 on 1044 degrees of freedom (263 observations deleted due to missingness) AIC: 1060.1 Number of Fisher Scoring iterations: 4 As you can see our coefficients are highly significant, and we can interpret them. As explained before, we cannot do this the same way as in OLS. At this point, what you can say from looking at this output is restricted to: Direction: does the independent variable have a positive or a negative influence on the dependent variable? Size: you can compare the size effect of different coefficients as in “this is larger than”, or “this is smaller than”. In effect, the size of coefficients determines how steep our s-shaped CDF is, and so the higher the coefficient, the steeper the curve. If we want to delve deeper into the interpretation, we can evaluate the probability on the y-axis for different values on the x-axis – or to stay with our example, what is the probability of travelling first class at different ages? Let us first get a basic overview of the variable . We can do this by calling summary(titanic$age) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000 263 Average age: 30 Minimum age: 0.17 (two months) Maximum age: 80 263 missing observations As a first step, it might be interesting to see what the probability of travelling first class was at the average age. When calculating the average age, we need to exclude missing values by setting na.rm=TRUE. meanage = data.frame(age = mean(titanic$age, na.rm=TRUE)) After defining this object, we can use it to predict the probability at this point: predict(class_age, meanage, type=&quot;response&quot;) 1 0.2382104 It is important to set the type to response here to obtain the probabilities, otherwise R will return to you the log-odds which is another way of interpreting the results of a logit. Log-odds are defined as the logarithm of probability of success over probability of failure. We are not going to look at these in this module. At mean age, the probability to travel First Class was 24%. If we set age to its maximum maxage = data.frame(age = max(titanic$age, na.rm=TRUE)) and calculate the predicted probability again, it changes very drastically: predict(class_age, maxage, type=&quot;response&quot;) 1 0.9032496 Now we can make statements such as: At average age, the probability to travel First Class on the Titanic was 24%. At maximum age of 80 years, this probability increases by 66% to 90%. You might wish to get an overview of what the probabilities to travel first class were for the entire range of the age variable. You can do this, by finding out the range creating a sequence ranging from minimum to maximum value, in steps of 1 year predicting the probabilities by using the previously created sequence as a list summary(titanic$age) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000 263 xage &lt;- seq(0, 80, 1) yage &lt;- predict(class_age, list(age = xage),type=&quot;response&quot;) If you are in a rush, you can simply plot this in a base R plot. plot(xage, yage, xlab = &quot;age&quot;, ylab = &quot;Probability to travel in First Class&quot;, type=&quot;l&quot;) If you want this to look jazzy, use GGPLOT2 (you need to turn the sequence and the predicted probabilities into a data frame first): predictions &lt;- data.frame(xage,yage) ggplot(data=predictions, aes(x=xage, y=yage)) + geom_line() + labs(x= &quot;Age&quot;, y=&quot;Predicted Probability to Travel in First Class&quot;) + theme(axis.text=element_text(size=12), axis.title=element_text(size=12,face=&quot;bold&quot;)) You can see from this graph that you to be just older than 47 to be likely to travel in First Class: firstage = data.frame(age = 47) predict(class_age, firstage, type=&quot;response&quot;) 1 0.4993925 We can also derive this in a more sophisticated way. We take the log-odd (defined as logarithm of the probability of success over probability of failure) and set it equal to our regression line5: \\[\\begin{equation*} log(0.5/(1-0.5)) = -3.187456 + 0.067767 \\times age \\end{equation*}\\] \\[\\begin{equation*} log(1) = 0 = -3.187456 + 0.067767 \\times age \\end{equation*}\\] Rearranging, we obtain \\[\\begin{equation*} age = 3.187456/0.067767 = 47.03586 \\end{equation*}\\] In general, to solve for age for any value of p: \\[\\begin{equation*} age = (log(p/(1-p)) - 3.187456)/0.067767 \\end{equation*}\\] Or, to make it more easily reproducible: \\[\\begin{equation} age = (log(p/(1-p)) - coef(class\\_age)[1])/coef(class\\_age)[2] \\end{equation}\\] If you just want the x-value associated with 50% probability, simply divide the intercept by the slope coefficient! Logit - Model Fit The question is now, how good does this model predict travelling in first class? The higher the proportion of correctly predicted cases, the better. But the proportion of correctly predicted cases is very much dependent on where we set the cut-off point \\(\\tau\\). So far, I have assumed this to be at 50%, because it is a sensible default. But we could choose different cut-off points to classify predictions. If, for example, we are incorrectly predicting 1s (here: travelled in first class) for a large range of x-values, we could just raise the cut-off point to turn those (incorrectly) predicted 1s into zeros (here: travelling in a class other than first) instead. It is therefore fairer to use a method which captures the trade-off between correctly predicting 1s and 0s. This method is called a ROC-curve (receiver operating ciriterion) and takes the form of a curve. This curve has two axes: y-axis: Probability of correctly predicting a 1. (Sensitivity) x-axis: (1-Specificity); Specificity: Probability of correctly predicting a 0. Usually, there is also a diagonal fitted into the coordinate system to indicate the model fit for a model without independent variables (AKA covariates). For our passenger class model, this curve looks like this: How do we interpret this? The further away from the diagonal, the better our model predicts 1s and 0s. The area called ‘auc’ would be 100% if the model correctly predicted everything. This area decreases as the model becomes worse. At 50% we might as well have tossed a coin to make predictions, because we are only correctly predicting 50% of cases. Here we predict 74.5% which isn’t great, but not a bad start. We could now estimate different models and use their area to compare fit between different models. But let me show you how I got this curve in the first place. We start by installing the package pROC install.packages(&quot;pROC&quot;) We can then load the package library(pROC) and calculate the curve: prob_trav &lt;- predict(class_age, type=&quot;response&quot;) titanic$prob_trav &lt;- unlist(prob_trav) roc &lt;- roc(titanic$class, titanic$prob_trav) auc(roc) Area under the curve: 0.745 What have I done here? I started by predicting the probabilities to travel first class. In a second step, I unlisted these probabilities and saved them as a new variable in our titanic data set. Finally, I fed the information of the values of the dependent variable and the respective predicted probabilities for each observation into the roc() function and saved the results in an object called roc. The auc() function returns to us the area, but we can also get the curve graphically: plot(roc, print.auc=TRUE) For this procedure to work, it is essential to use the option na.exclude when dealing with missing data in estimating the model. Only this way will there be a predicted probability for every observation. If we deleted the missing values with na.omit, then the number of observations would be longer than the vector containing predicted values and we would receive an error message. 10.2 Summary latent variable - a variable for which the values are not directly observable logit - Logistic regression Model ROC curve - receiver operating characteristic curve, captures the trade-off between correctly predicting 1s and 0s. Functions list function package description roc() c(“.GlobalEnv”, “pROC”) Estimate ROC Curve plot() c(“graphics”, “package:base”) Generic function from base R to produce a plot as.numeric() base coerce a vector to numeric c() base Combine values/vectors into a vector data.frame() base create a data.frame from vectors expression() base Used in plots to add symbols to axes factor() base create a factor library() base load an R package list() base create a list max() base get maximum of a vector mean() base get mean of a vector min() base get minimum of a vector seq() base create a sequence setwd() base Set Working Directory summary() base Obtain summary statistics or detailed regression output unlist() base NA mutate() dplyr modify/create a column in a data frame recode() dplyr Recode a variable aes() ggplot2 NA element_blank() ggplot2 NA element_line() ggplot2 NA element_text() ggplot2 Customise text in GGPLOT2 geom_hline() ggplot2 Add a horizontal line in GGPLOT2 geom_line() ggplot2 NA geom_vline() ggplot2 NA ggplot() ggplot2 Create a ggplot graph labs() ggplot2 Customise labels in GGPLOT2 labs() ggplot2 Add a vertical line in GGPLOT2 scale_x_continuous() ggplot2 Customise continuous x axis scale_y_continuous() ggplot2 Customise continuous y axis theme() ggplot2 Set theme for GGPLOT2 theme_bw() ggplot2 NA axis() graphics NA text() graphics NA auc() pROC Calculate area under ROC Curve read_excel() readxl NA glm() stats Estimate a General Linear Model pnorm() stats NA predict() stats Predict Probability 10.3 Exercises First Exercise The solutions for the exercises will be available here on 2021-02-18. Chapter solutions For probit, the normal distribution is used. This would lead to very similar results.↩︎ Source: https://stackoverflow.com/questions/32040504/regression-logistic-in-r-finding-x-value-predictor-for-a-particular-y-value↩︎ "],["rmarkdown.html", "11 R Markdown 11.1 Content 11.2 Summary 11.3 Exercises", " 11 R Markdown 11.1 Content One of the more persuasive arguments to use R (over say SPSS) is its ability to easily make work reproducible. This means that you could give your RScript to another person, and they would be able to replicate, step by step your data preparation and analysis, obtaining the same results. This is not only a fundamental part of any scientific enquiry, but also helpful to you, should you wish to replicate your own work at a later stage. Take it from me: after a few months you will have forgotten any data management procedure or steps used in an analysis. How do you achieve this? You can, of course, use annotations in your RScript to explain to others and yourself what you have done in each step. And in fact you should do exactly that as a matter of routine. But we can go a step further than that. You might have asked yourself when studying the previous Chapters how to get all the great output in the form of Tables and Figures into an essay, or your dissertation. The answer to this is Markdown. It lets you create reproducible essays / articles with great ease, and even has a feature to create your bibliography and take care of your referencing. Intrigued? Then read on! By the way, this very webpage is also created with R Markdown. Introduction So, what is R Markdown? As promised in the introduction, R Markdown provides a unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. (https://r4ds.had.co.nz/r-markdown.html) Usage For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis. For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code). As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking. (https://r4ds.had.co.nz/r-markdown.html) The Components of an R Markdown Document YAML Text Code Chunks The YAML Acronym for “Yet Another Markup Language” Contains the settings for the entire document Primarily parameters and bibliography Text Writing Basically, you can just write the text as you always would. It’s “only” the formatting that differs. Compiling Markdown is not WYSIWYG (What You See Is What You Get) Compiling is called “Knitting” Headings # Heading ## Sub-Heading ### Sub-Sub Heading Emphasis *italic* **bold** \\texttt(courier) \\underline(underline) Links Just insert the link Lists - - - Numbered Lists 1. a. b. 2. Line Breaks This will not produce a line break: line 1 line 2 But this will: line 1 line 2 Block Quotes Simply precede the quote with “\\(&gt;\\)” The block will turn green when you do so Equations Two ways to set equations Either wrapped in $ signs $ equation $ or \\begin{equation} equation \\end{equation} Example The command \\begin{equation} Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon \\end{equation} results in: \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon \\end{equation}\\] To suppress the numbering you type: \\begin{equation*} Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon \\end{equation*} \\[\\begin{equation*} Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon \\end{equation*}\\] List of Symbols You can find a good compilation of symbols here: https://latex.wikia.org/wiki/List_of_LaTeX_symbols Code Chunks In-Line R Code You can include R Code in Markdown by wrapping it in ```{r} ``` Shortcut: Mac: Option + Command + I Windows: Ctrl + Alt + I Example ```{r} 5+3 ``` results in 5+3 [1] 8 Chunk Options Types of Output Suppressed by: Examples Display, but not calculate: ```{r eval=F} 5+3 ``` Suppress Messages from Packages ```{r message=F} library(tidyverse) ``` Suppress Messages from Packages and only display ```{r message=F, eval=F} library(tidyverse) ``` Figures and Graphs You can also use this to include figures and graphs: ```{r echo=FALSE, out.width=&#39;75%&#39;} knitr::include_graphics(&#39;./filename.png&#39;) ``` Add a Caption! ```{r echo=FALSE, out.width=&#39;75%&#39;, fig.cap=&quot;\\\\label{fig:test}Test Caption&quot;} knitr::include_graphics(&#39;./filename.png&#39;) ``` And then refer to it in the text with \\ref{fig:test} Example ```{r echo=FALSE, out.width=&#39;75%&#39;, fig.cap=&quot;\\\\label{fig:spell}RStudio Task Bar&quot;} knitr::include_graphics(&#39;./spell.png&#39;) ``` As we can see in \\ref{fig:spell} turns into Figure 11.1: RStudio Task Bar As we can see in Figure 11.1 Useful Stuff Spell-Checker Useful Commands New page \\newpage Centering a Line \\begin{center} Text to be centred. \\end{center} Bibliography It is possible (and most highly recommended) to include an automatic bibliography in Markdown This requires more coding language and LaTeX Perks It only includes what you cite It sorts the references alphabetically Consistent citation Automatically conform to PAIS style Getting Started Download and install: Mac: MacTeX (http://www.tug.org/mactex/) Windows: MiKTeX (https://miktex.org/) These contain a complete TeX system with LaTeX itself and editors to write documents. More on: https://www.latex-project.org/get/ Instructions Place in the working directory the file “bibliography.bib” Add a heading at the very end of the document called Citations in Markdown Citations Text [@grolemund:2016, p. 361] Text [@grolemund:2016, pp. 33-35, 38-39 and *passim*]. Suppress Author Grolemund and Wickham write that ... [-@grolemund:2016] This can be useful when the author is already mentioned in the text. The .bib file Every citation needs to have a reference in the .bib file, such as: @book{grolemund:2016, author={Garrett Grolemund and Hadley Wickham}, title={R for Data Science}, publisher={O&#39;Reilly Media}, year={2016}} You can only edit this file in LaTeX If you want to learn more about , there is another Moodle Skills Module called Academic Writing in LaTeX available. Input # Introduction Text [@grolemund:2016, p. 361] # References turns into this output stargazer I have little to no patience for students (or academics!) who are fiddling around in MS Word and are using all sorts of fancy options to make their Tables and Figures as unreadable as possible. You can tell that I am quite particular when it comes to the tabular display of statistical output. When you study for a degree in Political Science, you are acquiring the skills to write professionally about a topic of the discipline, and to analyse a research question within its remit. James A. Stimson puts it very aptly in his article Writing in Political Science when he says: You are a professional author. Learn to use the tools of authorship or choose a profession for which you are better suited. (p. 10) Stimson says this in reference to Tables in particular, as there are some principles that any academic author needs to observe. Let me quote again from his article here: Table design is important, and often done badly. It requires you to think about what the reader knows and wants to know from your work and then very carefully lay out the table to tell the story. (…) Tables should always be composed so that a reader can pick one up and understand its content, without having read the text. That means it must be fully self-contained, depending on nothing that is explained only in the text. The opposite is also true; a reader should be able to skip the table and understand the analysis completely from the text. (p.10) To observe most of the principles set out in Stimson’s article (please read it, it’s worth its weight in gold), R has a snazzy package that helps you on the way. It is called stargazer and is pretty much the best invention since sliced bread: is an R package that creates code, HTML code and ASCII text for well-formatted regression tables, with multiple models side-by-side, as well as for summary statistics tables, data frames, vectors and matrices. (https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) Advantages Its ease of use The large number of models it supports Its beautiful aesthetics Estimating a Model, replicating the work by Fearon and Laitin (2003) library(haven) library(tidyverse) library(pROC) library(stargazer) Data Prep fearon &lt;- read_dta(&quot;data/fearon.dta&quot;) fearon$onset[fearon$onset==4] &lt;- NA fearon$onset &lt;- as.factor(fearon$onset) Model 1 model1 &lt;- glm(onset ~ gdpenl, family = binomial(link = logit), na.action = na.exclude, data = fearon) prob_model1 &lt;- predict(model1, type=&quot;response&quot;) fearon$prob_model1 &lt;- unlist(prob_model1) roc_model1 &lt;- roc(fearon$onset, fearon$prob_model1) Setting levels: control = 0, case = 1 Setting direction: controls &lt; cases Model 2 model2 &lt;- glm(onset ~ gdpenl + lpopl1, family = binomial(link = logit), na.action = na.exclude, data = fearon) prob_model2 &lt;- predict(model2, type=&quot;response&quot;) fearon$prob_model2 &lt;- unlist(prob_model2) roc_model2 &lt;- roc(fearon$onset, fearon$prob_model2) Setting levels: control = 0, case = 1 Setting direction: controls &lt; cases Model 3 model3 &lt;- glm(onset ~ gdpenl + lpopl1 + lmtnest, family = binomial(link = logit), na.action = na.exclude, data = fearon) prob_model3 &lt;- predict(model3, type=&quot;response&quot;) fearon$prob_model3 &lt;- unlist(prob_model3) roc_model3 &lt;- roc(fearon$onset, fearon$prob_model3) Setting levels: control = 0, case = 1 Setting direction: controls &lt; cases Now we use stargazer to put the results into a table. I am suppressing the annoying heager here, and am adjusting the font size, too. Output You will note that stargazer adds the asterisks that Stimson is so strongly (and correctly) opposed to. The rating of lower p-values with increasing numbers of asterisks suggests a certain goal (like wanting to stay in a five-star hotel), but this goal is nonesense. If you want to know more about this, I recommend an excellent module called “Introduction to Quantitative Political Analysis I” where you learn about the p-value and the Type I and Type II errors. stargazer’s practice to include these asterisks is therefore misguided, but since most journals insist on this nonsense, it is worthwhile to get used to it. Font Sizes In descending Order \\Huge \\huge \\LARGE \\Large \\large \\normalsize \\small \\footnotesize \\scriptsize \\tiny I will ignore the {r results='asis', echo=F, message=F, tab.cap = NULL} environment now, and only show you the actual stargazer script to be used. Adding Variable Names, again quoting Stimson: The usual problem is that the names are too brief to convey what the indicator is. (And remember the rule about being self-contained: if the reader needs to page back to find out what some ambiguous name stands for, you have violated the rule and caused reader impatience.) Abbreviate nothing. And never ever ever use computer variable names to stand for concepts. These are personal code words that convey no meaning to readers. (p.10) stargazer(model1, model2, model3, header=F, font.size = &quot;tiny&quot;, covariate.labels = c(&quot;GDP per capita (in logged 1985\\\\$ thousands)&quot;, &quot;Population (logged, in thousands)&quot;, &quot;Mountainous Terrain (logged \\\\% of total)&quot;), dep.var.labels = &quot;Onset of Civil War&quot;) Suppress Statistics stargazer(model1, model2, model3, header=F, font.size = &quot;tiny&quot;, covariate.labels = c(&quot;GDP per capita (in logged 1985\\\\$ thousands)&quot;, &quot;Population (logged, in thousands)&quot;, &quot;Mountainous Terrain (logged \\\\% of total)&quot;), dep.var.labels = &quot;Onset of Civil War&quot;, omit.stat = c(&quot;aic&quot;, &quot;ll&quot;)) For a full list of abbreviations for statistics, see page 22 of https://cran.r-project.org/web/packages/stargazer/stargazer.pdf Add ROC Curve stargazer(model1, model2, model3, header=F, font.size = &quot;tiny&quot;, covariate.labels = c(&quot;GDP per capita (in logged 1985\\\\$ thousands)&quot;, &quot;Population (logged, in thousands)&quot;, &quot;Mountainous Terrain (logged \\\\% of total)&quot;), dep.var.labels = &quot;Onset of Civil War&quot;, omit.stat = c(&quot;aic&quot;, &quot;ll&quot;), add.lines = list(c(&quot;ROC Curve&quot;, auc(roc_model1), auc(roc_model2), auc(roc_model3)))) Round ROC Curve stargazer(model1, model2, model3, header=F, font.size = &quot;tiny&quot;, covariate.labels = c(&quot;GDP per capita (in logged 1985\\\\$ thousands)&quot;, &quot;Population (logged, in thousands)&quot;, &quot;Mountainous Terrain (logged \\\\% of total)&quot;), dep.var.labels = &quot;Onset of Civil War&quot;, omit.stat = c(&quot;aic&quot;, &quot;ll&quot;), add.lines = list(c(&quot;ROC Curve&quot;, round(auc(roc_model1),2), round(auc(roc_model2),2), round(auc(roc_model3),2)))) You can also control the number of decimal places with the digits = option more generally. For example, if you want to round everything to two decimal places, you would set digits = 2. Lastly, let’s label the Table, so that a reader knows what is being shown. stargazer(model1, model2, model3, header=F, font.size = &quot;tiny&quot;, covariate.labels = c(&quot;GDP per capita (in logged 1985\\\\$ thousands)&quot;, &quot;Population (logged, in thousands)&quot;, &quot;Mountainous Terrain (logged \\\\% of total)&quot;), dep.var.labels = &quot;Onset of Civil War&quot;, omit.stat = c(&quot;aic&quot;, &quot;ll&quot;), add.lines = list(c(&quot;ROC Curve&quot;, round(auc(roc_model1),2), round(auc(roc_model2),2), round(auc(roc_model3),2))), title = &quot;Determinants of Civil War (Fearon and Laitin, 2003)&quot;) This is a beautiful and informative Table which should be the standard to which you work. 11.2 Summary Functions list 11.3 Exercises The solutions for the exercises will be available here on 2021-03-12. Chapter solutions "],["course-summary.html", "12 Course summary Index of functions used throughout the course", " 12 Course summary Index of functions used throughout the course "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
